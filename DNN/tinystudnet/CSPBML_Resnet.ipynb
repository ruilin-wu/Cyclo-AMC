{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 计算α，β\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scanning: 100%|███████████████████████| 20/20 [00:00<00:00, 24.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  mean=4295.14  std=14429.3  α=6.93034e-05  β=-0.297668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scanning: 100%|█████████████████████████| 4/4 [00:00<00:00, 24.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL  ]  mean=4258.27  std=14284.5  α=7.00059e-05  β=-0.298104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scanning: 100%|█████████████████████████| 4/4 [00:00<00:00, 25.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST ]  mean=4280.03  std=14425.2  α=6.93232e-05  β=-0.296705\n",
      "file,   mean,      std,        alpha,       beta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "per-file α/β:  11%|█         | 3/28 [00:00<00:01, 23.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_batch_01.pt, 4093.09, 14121.9, 7.08121e-05, -0.289841\n",
      "2022_batch_02.pt, 4677.42, 15457.6, 6.46929e-05, -0.302596\n",
      "2022_batch_03.pt, 4209.98, 14110.1, 7.08714e-05, -0.298367\n",
      "2022_batch_04.pt, 4099.58, 13298.3, 7.51978e-05, -0.308279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "per-file α/β:  21%|██▏       | 6/28 [00:00<00:00, 25.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_batch_05.pt, 4160.33, 14470.6, 6.91056e-05, -0.287502\n",
      "2022_batch_06.pt, 4662.43, 15452.6, 6.47141e-05, -0.301725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "per-file α/β:  32%|███▏      | 9/28 [00:00<00:00, 24.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_batch_07.pt, 4245.91, 14162.9, 7.06069e-05, -0.29979\n",
      "2022_batch_08.pt, 4036.47, 13126.1, 7.61838e-05, -0.307513\n",
      "2022_batch_09.pt, 4190.58, 14581.5, 6.85803e-05, -0.287391\n",
      "2022_batch_10.pt, 4632.37, 15371.2, 6.50567e-05, -0.301367\n",
      "2022_batch_11.pt, 4165.24, 13868.5, 7.21059e-05, -0.300338\n",
      "2022_batch_12.pt, 4072.68, 13271.6, 7.53486e-05, -0.306871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "per-file α/β:  54%|█████▎    | 15/28 [00:00<00:00, 25.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_batch_13.pt, 4184.38, 14623, 6.83853e-05, -0.286151\n",
      "2022_batch_14.pt, 4610.79, 15287.6, 6.54123e-05, -0.301602\n",
      "2022_batch_15.pt, 4151.6, 13854.3, 7.21796e-05, -0.299661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "per-file α/β:  64%|██████▍   | 18/28 [00:00<00:00, 25.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_batch_16.pt, 4180.08, 13622.2, 7.34093e-05, -0.306857\n",
      "2022_batch_17.pt, 4265.58, 15061.6, 6.63939e-05, -0.283209\n",
      "2022_batch_18.pt, 4522.87, 15096.3, 6.62413e-05, -0.299601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "per-file α/β:  75%|███████▌  | 21/28 [00:00<00:00, 25.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_batch_19.pt, 4192.76, 14169.8, 7.05728e-05, -0.295894\n",
      "2022_batch_20.pt, 4244.66, 13765.1, 7.26477e-05, -0.308365\n",
      "2022_batch_21.pt, 4234.7, 15009.7, 6.66235e-05, -0.282131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "per-file α/β:  86%|████████▌ | 24/28 [00:00<00:00, 25.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_batch_22.pt, 4562.91, 15208.6, 6.57524e-05, -0.300022\n",
      "2022_batch_23.pt, 4215.54, 14297.5, 6.99423e-05, -0.294845\n",
      "2022_batch_24.pt, 4231.87, 13633.1, 7.33508e-05, -0.310411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "per-file α/β:  96%|█████████▋| 27/28 [00:01<00:00, 25.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_batch_25.pt, 4235.05, 14939.4, 6.69372e-05, -0.283483\n",
      "2022_batch_26.pt, 4583.8, 15249.2, 6.55771e-05, -0.300592\n",
      "2022_batch_27.pt, 4221.13, 14273.7, 7.00588e-05, -0.295727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "per-file α/β: 100%|██████████| 28/28 [00:01<00:00, 25.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_batch_28.pt, 4172.21, 13507.8, 7.40314e-05, -0.308875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scanning: 100%|███████████████████████| 20/20 [00:00<00:00, 24.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  mean=4295.14  std=14429.3  α=6.93232e-05  β=-0.296705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch, math, os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPS = 1e-5\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 工具：统计一个 .pt 列表的全局 mean / std\n",
    "# ------------------------------------------------------------\n",
    "def calc_mean_std(pt_list):\n",
    "    total_sum, total_sq, total_n = 0.0, 0.0, 0\n",
    "    for f in tqdm(pt_list, desc=\"scanning\", ncols=70):\n",
    "        X, _ = torch.load(f, map_location='cpu', weights_only=True)  # X: (N,128,128)\n",
    "        X = X.float()\n",
    "        total_sum += X.sum().item()\n",
    "        total_sq  += (X ** 2).sum().item()\n",
    "        total_n   += X.numel()\n",
    "    mu  = total_sum / total_n\n",
    "    var = total_sq / total_n - mu ** 2\n",
    "    std = math.sqrt(max(var, 0.0))\n",
    "    return mu, std\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 读取三个列表\n",
    "# ------------------------------------------------------------\n",
    "root = \"../../cached_scd_tim/2022_c_64_512sample_hamming\"   # 你的数据根\n",
    "pt_files = sorted([f\"{root}/2022_batch_{i:02d}.pt\" for i in range(1, 29)])\n",
    "\n",
    "train_files = pt_files[:10] + pt_files[-10:]   # 01–10 & 19–28\n",
    "val_files   = pt_files[10:14]                  # 11–14\n",
    "test_files  = pt_files[14:18]                  # 15–18\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 逐集合计算 α β\n",
    "# ------------------------------------------------------------\n",
    "for tag, flist in [(\"TRAIN\", train_files),\n",
    "                   (\"VAL  \", val_files),\n",
    "                   (\"TEST \", test_files)]:\n",
    "    mu, std = calc_mean_std(flist)\n",
    "    alpha   = 1.0 / (std + EPS)\n",
    "    beta    = -mu * alpha\n",
    "    print(f\"[{tag}]  mean={mu:.6g}  std={std:.6g}  \"\n",
    "          f\"α={alpha:.6g}  β={beta:.6g}\")\n",
    "\n",
    "import torch, math, os, csv\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPS = 1e-5\n",
    "\n",
    "def alpha_beta_for_file(pt_path: str):\n",
    "    \"\"\"返回 (mu, std, alpha, beta)\"\"\"\n",
    "    X, _ = torch.load(pt_path, map_location=\"cpu\", weights_only=True)  # X:(N,128,128)\n",
    "    X = X.float()\n",
    "    mu  = X.mean().item()\n",
    "    std = X.std(unbiased=False).item()      # population std\n",
    "    alpha = 1.0 / (std + EPS)\n",
    "    beta  = -mu * alpha\n",
    "    return mu, std, alpha, beta\n",
    "\n",
    "root = \"../../cached_scd_tim/2022_c_64_512sample_hamming\"\n",
    "pt_files = sorted([f\"{root}/2022_batch_{i:02d}.pt\" for i in range(1, 29)])\n",
    "\n",
    "print(\"file,   mean,      std,        alpha,       beta\")\n",
    "for f in tqdm(pt_files, desc=\"per-file α/β\"):\n",
    "    mu, std, a, b = alpha_beta_for_file(f)\n",
    "    print(f\"{Path(f).name}, {mu:.6g}, {std:.6g}, {a:.6g}, {b:.6g}\")\n",
    "\n",
    "for tag, flist in [(\"TRAIN\", train_files)]:\n",
    "    mu, std = calc_mean_std(flist)\n",
    "    alpha_train   = 1.0 / (std + EPS)\n",
    "    beta_train    = -mu * alpha\n",
    "    print(f\"[{tag}]  mean={mu:.6g}  std={std:.6g}  \"\n",
    "          f\"α={alpha:.6g}  β={beta:.6g}\")\n",
    "\n",
    "\n",
    "ALPHA = alpha_train      # 1 / (std_train + 1e-5)\n",
    "BETA  = beta_train        # -mean_train * ALPHA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💻 device = cuda\n",
      "Total params: 6.40 M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31595/3861110918.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  n = torch.load(f, map_location=\"cpu\")[0].shape[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔀 Shuffled split → train 80,000 | val 16,000 | test 16,000\n",
      "E01  train 16.60% | loss 2.0664   val 17.54% | loss 2.0321   test 17.49% | loss 2.0328\n",
      "current lr = 0.028000000000000004\n",
      "E02  train 17.37% | loss 2.0356   val 17.81% | loss 2.0114   test 18.22% | loss 2.0119\n",
      "current lr = 0.046000000000000006\n",
      "E03  train 17.48% | loss 2.0247   val 16.76% | loss 2.0343   test 16.77% | loss 2.0346\n",
      "current lr = 0.064\n",
      "E04  train 17.51% | loss 2.0204   val 17.82% | loss 2.0098   test 17.78% | loss 2.0105\n",
      "current lr = 0.082\n",
      "E05  train 17.76% | loss 2.0069   val 17.61% | loss 2.0113   test 17.58% | loss 2.0111\n",
      "current lr = 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruilin/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E06  train 18.13% | loss 1.9905   val 18.57% | loss 1.9768   test 18.49% | loss 1.9773\n",
      "current lr = 0.09999899300374603\n",
      "E07  train 18.55% | loss 1.9795   val 18.31% | loss 1.9929   test 18.21% | loss 1.9948\n",
      "current lr = 0.09999597205554578\n",
      "E08  train 18.60% | loss 1.9745   val 18.84% | loss 1.9663   test 18.93% | loss 1.9671\n",
      "current lr = 0.09999093727708258\n",
      "E09  train 18.69% | loss 1.9694   val 18.73% | loss 1.9648   test 18.88% | loss 1.9660\n",
      "current lr = 0.09998388887115658\n",
      "E10  train 19.05% | loss 1.9658   val 19.14% | loss 1.9634   test 18.92% | loss 1.9648\n",
      "current lr = 0.09997482712167656\n",
      "E11  train 19.00% | loss 1.9638   val 18.51% | loss 1.9687   test 18.51% | loss 1.9697\n",
      "current lr = 0.09996375239364846\n",
      "E12  train 19.16% | loss 1.9627   val 19.49% | loss 1.9551   test 19.28% | loss 1.9566\n",
      "current lr = 0.09995066513316068\n",
      "E13  train 19.69% | loss 1.9549   val 19.94% | loss 1.9555   test 19.98% | loss 1.9555\n",
      "current lr = 0.09993556586736622\n",
      "E14  train 20.13% | loss 1.9496   val 21.09% | loss 1.9375   test 20.86% | loss 1.9367\n",
      "current lr = 0.09991845520446127\n",
      "E15  train 20.55% | loss 1.9429   val 20.51% | loss 1.9514   test 20.19% | loss 1.9518\n",
      "current lr = 0.09989933383366086\n",
      "E16  train 21.37% | loss 1.9303   val 22.04% | loss 1.9130   test 22.12% | loss 1.9136\n",
      "current lr = 0.09987820252517098\n",
      "E17  train 22.29% | loss 1.9136   val 23.49% | loss 1.8920   test 23.55% | loss 1.8925\n",
      "current lr = 0.09985506213015768\n",
      "E18  train 23.61% | loss 1.8930   val 23.92% | loss 1.8809   test 24.16% | loss 1.8800\n",
      "current lr = 0.09982991358071269\n",
      "E19  train 25.48% | loss 1.8638   val 26.88% | loss 1.8363   test 27.07% | loss 1.8356\n",
      "current lr = 0.09980275788981592\n",
      "E20  train 27.92% | loss 1.8240   val 29.04% | loss 1.8102   test 29.23% | loss 1.8058\n",
      "current lr = 0.09977359615129464\n",
      "E21  train 31.97% | loss 1.7522   val 34.62% | loss 1.7095   test 34.77% | loss 1.7085\n",
      "current lr = 0.09974242953977941\n",
      "E22  train 37.87% | loss 1.6470   val 41.50% | loss 1.5828   test 41.21% | loss 1.5855\n",
      "current lr = 0.09970925931065681\n",
      "E23  train 44.34% | loss 1.5233   val 45.57% | loss 1.5076   test 45.42% | loss 1.5076\n",
      "current lr = 0.09967408680001884\n",
      "E24  train 50.46% | loss 1.4015   val 52.91% | loss 1.3470   test 53.61% | loss 1.3455\n",
      "current lr = 0.0996369134246091\n",
      "E25  train 55.80% | loss 1.3011   val 57.41% | loss 1.2513   test 57.34% | loss 1.2536\n",
      "current lr = 0.09959774068176572\n",
      "E26  train 60.00% | loss 1.2121   val 58.83% | loss 1.2356   test 58.91% | loss 1.2347\n",
      "current lr = 0.09955657014936106\n",
      "E27  train 63.22% | loss 1.1467   val 61.22% | loss 1.1646   test 61.28% | loss 1.1701\n",
      "current lr = 0.09951340348573819\n",
      "E28  train 65.89% | loss 1.0891   val 62.76% | loss 1.1440   test 63.13% | loss 1.1429\n",
      "current lr = 0.09946824242964401\n",
      "E29  train 67.55% | loss 1.0521   val 64.76% | loss 1.1029   test 64.25% | loss 1.1093\n",
      "current lr = 0.09942108880015926\n",
      "E30  train 69.11% | loss 1.0204   val 64.64% | loss 1.1028   test 65.02% | loss 1.0977\n",
      "current lr = 0.0993719444966253\n",
      "E31  train 70.29% | loss 0.9929   val 66.39% | loss 1.0684   test 66.47% | loss 1.0657\n",
      "current lr = 0.09932081149856747\n",
      "E32  train 71.08% | loss 0.9743   val 65.46% | loss 1.0913   test 65.62% | loss 1.0936\n",
      "current lr = 0.09926769186561549\n",
      "E33  train 71.89% | loss 0.9565   val 68.44% | loss 1.0170   test 68.76% | loss 1.0120\n",
      "current lr = 0.09921258773742044\n",
      "E34  train 73.02% | loss 0.9321   val 68.52% | loss 1.0264   test 68.77% | loss 1.0273\n",
      "current lr = 0.09915550133356855\n",
      "E35  train 73.68% | loss 0.9180   val 61.56% | loss 1.2041   test 61.78% | loss 1.2024\n",
      "current lr = 0.09909643495349187\n",
      "E36  train 74.43% | loss 0.9047   val 69.59% | loss 1.0090   test 69.41% | loss 1.0078\n",
      "current lr = 0.09903539097637555\n",
      "E37  train 74.73% | loss 0.8970   val 72.97% | loss 0.9233   test 72.89% | loss 0.9298\n",
      "current lr = 0.09897237186106211\n",
      "E38  train 75.39% | loss 0.8828   val 70.33% | loss 0.9937   test 70.11% | loss 0.9964\n",
      "current lr = 0.09890738014595231\n",
      "E39  train 75.92% | loss 0.8733   val 72.07% | loss 0.9462   test 71.91% | loss 0.9489\n",
      "current lr = 0.09884041844890294\n",
      "E40  train 76.38% | loss 0.8594   val 72.16% | loss 0.9516   test 71.66% | loss 0.9518\n",
      "current lr = 0.09877148946712144\n",
      "E41  train 76.62% | loss 0.8553   val 70.88% | loss 0.9762   test 71.36% | loss 0.9678\n",
      "current lr = 0.09870059597705712\n",
      "E42  train 77.06% | loss 0.8476   val 73.27% | loss 0.9088   test 73.66% | loss 0.9071\n",
      "current lr = 0.0986277408342894\n",
      "E43  train 77.29% | loss 0.8359   val 72.36% | loss 0.9506   test 72.22% | loss 0.9516\n",
      "current lr = 0.09855292697341285\n",
      "E44  train 77.81% | loss 0.8271   val 73.02% | loss 0.9280   test 73.48% | loss 0.9216\n",
      "current lr = 0.09847615740791885\n",
      "E45  train 78.16% | loss 0.8215   val 73.79% | loss 0.9028   test 74.17% | loss 0.8975\n",
      "current lr = 0.09839743523007433\n",
      "E46  train 78.33% | loss 0.8183   val 74.05% | loss 0.9001   test 74.24% | loss 0.8995\n",
      "current lr = 0.0983167636107971\n",
      "E47  train 78.65% | loss 0.8109   val 72.64% | loss 0.9375   test 72.14% | loss 0.9386\n",
      "current lr = 0.09823414579952824\n",
      "E48  train 78.84% | loss 0.8035   val 71.41% | loss 0.9678   test 71.77% | loss 0.9678\n",
      "current lr = 0.09814958512410114\n",
      "E49  train 79.24% | loss 0.7974   val 76.24% | loss 0.8645   test 75.74% | loss 0.8633\n",
      "current lr = 0.09806308499060748\n",
      "E50  train 79.47% | loss 0.7940   val 75.37% | loss 0.8751   test 75.66% | loss 0.8693\n",
      "current lr = 0.09797464888326002\n",
      "E51  train 79.52% | loss 0.7878   val 74.49% | loss 0.8952   test 73.92% | loss 0.9020\n",
      "current lr = 0.09788428036425227\n",
      "E52  train 79.84% | loss 0.7807   val 76.69% | loss 0.8509   test 75.75% | loss 0.8680\n",
      "current lr = 0.09779198307361499\n",
      "E53  train 80.03% | loss 0.7781   val 75.56% | loss 0.8740   test 75.32% | loss 0.8760\n",
      "current lr = 0.0976977607290696\n",
      "E54  train 80.26% | loss 0.7752   val 75.52% | loss 0.8734   test 75.71% | loss 0.8756\n",
      "current lr = 0.0976016171258784\n",
      "E55  train 80.21% | loss 0.7755   val 76.42% | loss 0.8444   test 76.79% | loss 0.8458\n",
      "current lr = 0.09750355613669169\n",
      "E56  train 80.68% | loss 0.7665   val 73.28% | loss 0.9345   test 73.87% | loss 0.9276\n",
      "current lr = 0.09740358171139184\n",
      "E57  train 80.76% | loss 0.7634   val 76.12% | loss 0.8554   test 76.20% | loss 0.8592\n",
      "current lr = 0.0973016978769341\n",
      "E58  train 80.73% | loss 0.7628   val 71.54% | loss 0.9835   test 72.26% | loss 0.9801\n",
      "current lr = 0.0971979087371845\n",
      "E59  train 81.12% | loss 0.7520   val 76.46% | loss 0.8506   test 76.46% | loss 0.8582\n",
      "current lr = 0.09709221847275443\n",
      "E60  train 81.47% | loss 0.7509   val 77.04% | loss 0.8425   test 77.29% | loss 0.8290\n",
      "current lr = 0.09698463134083232\n",
      "E61  train 81.31% | loss 0.7503   val 78.28% | loss 0.7991   test 78.81% | loss 0.7960\n",
      "current lr = 0.09687515167501218\n",
      "E62  train 81.52% | loss 0.7467   val 75.24% | loss 0.8870   test 74.67% | loss 0.8858\n",
      "current lr = 0.09676378388511897\n",
      "E63  train 81.72% | loss 0.7413   val 78.21% | loss 0.8093   test 77.82% | loss 0.8171\n",
      "current lr = 0.09665053245703104\n",
      "E64  train 81.85% | loss 0.7394   val 77.54% | loss 0.8309   test 77.54% | loss 0.8265\n",
      "current lr = 0.09653540195249939\n",
      "E65  train 81.80% | loss 0.7355   val 70.28% | loss 1.0210   test 69.97% | loss 1.0244\n",
      "current lr = 0.09641839700896396\n",
      "E66  train 82.05% | loss 0.7314   val 76.60% | loss 0.8577   test 76.39% | loss 0.8596\n",
      "current lr = 0.09629952233936681\n",
      "E67  train 82.23% | loss 0.7293   val 78.51% | loss 0.8005   test 78.47% | loss 0.8081\n",
      "current lr = 0.09617878273196231\n",
      "E68  train 82.38% | loss 0.7260   val 77.78% | loss 0.8200   test 77.36% | loss 0.8284\n",
      "current lr = 0.09605618305012424\n",
      "E69  train 82.44% | loss 0.7240   val 77.64% | loss 0.8163   test 78.17% | loss 0.8164\n",
      "current lr = 0.0959317282321499\n",
      "E70  train 82.58% | loss 0.7214   val 77.32% | loss 0.8342   test 77.34% | loss 0.8376\n",
      "current lr = 0.0958054232910612\n",
      "E71  train 82.81% | loss 0.7170   val 74.86% | loss 0.8948   test 74.62% | loss 0.8929\n",
      "current lr = 0.09567727331440276\n",
      "E72  train 82.65% | loss 0.7167   val 78.76% | loss 0.7967   test 79.14% | loss 0.7913\n",
      "current lr = 0.09554728346403696\n",
      "E73  train 82.88% | loss 0.7119   val 76.10% | loss 0.8712   test 75.59% | loss 0.8880\n",
      "current lr = 0.09541545897593601\n",
      "E74  train 83.09% | loss 0.7073   val 76.66% | loss 0.8520   test 76.42% | loss 0.8562\n",
      "current lr = 0.09528180515997108\n",
      "E75  train 83.18% | loss 0.7100   val 78.26% | loss 0.8204   test 77.99% | loss 0.8233\n",
      "current lr = 0.09514632739969839\n",
      "E76  train 83.28% | loss 0.7055   val 77.81% | loss 0.8294   test 77.38% | loss 0.8348\n",
      "current lr = 0.09500903115214233\n",
      "E77  train 83.32% | loss 0.7039   val 79.10% | loss 0.7999   test 78.64% | loss 0.8104\n",
      "current lr = 0.09486992194757579\n",
      "E78  train 83.33% | loss 0.7016   val 77.34% | loss 0.8493   test 76.70% | loss 0.8544\n",
      "current lr = 0.09472900538929722\n",
      "E79  train 83.59% | loss 0.6983   val 78.02% | loss 0.8200   test 77.55% | loss 0.8351\n",
      "current lr = 0.09458628715340503\n",
      "E80  train 83.70% | loss 0.6964   val 78.59% | loss 0.8113   test 77.94% | loss 0.8224\n",
      "current lr = 0.09444177298856896\n",
      "E81  train 83.70% | loss 0.6958   val 79.99% | loss 0.7700   test 79.69% | loss 0.7780\n",
      "current lr = 0.0942954687157985\n",
      "E82  train 83.53% | loss 0.6931   val 78.29% | loss 0.8124   test 78.09% | loss 0.8187\n",
      "current lr = 0.09414738022820841\n",
      "E83  train 83.78% | loss 0.6915   val 77.72% | loss 0.8321   test 77.61% | loss 0.8279\n",
      "current lr = 0.09399751349078139\n",
      "E84  train 83.98% | loss 0.6878   val 80.11% | loss 0.7686   test 79.92% | loss 0.7718\n",
      "current lr = 0.09384587454012777\n",
      "E85  train 84.26% | loss 0.6829   val 78.06% | loss 0.8136   test 78.16% | loss 0.8218\n",
      "current lr = 0.09369246948424238\n",
      "E86  train 84.18% | loss 0.6817   val 79.56% | loss 0.7784   test 79.12% | loss 0.7888\n",
      "current lr = 0.0935373045022585\n",
      "E87  train 84.25% | loss 0.6807   val 79.83% | loss 0.7809   test 79.65% | loss 0.7821\n",
      "current lr = 0.09338038584419897\n",
      "E88  train 84.25% | loss 0.6808   val 79.05% | loss 0.7957   test 78.94% | loss 0.7982\n",
      "current lr = 0.09322171983072446\n",
      "E89  train 84.41% | loss 0.6776   val 80.12% | loss 0.7746   test 79.91% | loss 0.7774\n",
      "current lr = 0.09306131285287884\n",
      "E90  train 84.57% | loss 0.6767   val 79.91% | loss 0.7733   test 79.50% | loss 0.7845\n",
      "current lr = 0.09289917137183182\n",
      "E91  train 84.68% | loss 0.6725   val 78.49% | loss 0.8126   test 78.41% | loss 0.8192\n",
      "current lr = 0.09273530191861859\n",
      "E92  train 84.65% | loss 0.6729   val 78.29% | loss 0.8043   test 77.84% | loss 0.8144\n",
      "current lr = 0.09256971109387684\n",
      "E93  train 84.79% | loss 0.6689   val 78.94% | loss 0.8042   test 79.09% | loss 0.8032\n",
      "current lr = 0.09240240556758085\n",
      "E94  train 84.70% | loss 0.6690   val 78.70% | loss 0.8012   test 78.77% | loss 0.7966\n",
      "current lr = 0.09223339207877282\n",
      "E95  train 84.89% | loss 0.6650   val 79.86% | loss 0.7860   test 79.75% | loss 0.7871\n",
      "current lr = 0.09206267743529142\n",
      "E96  train 84.97% | loss 0.6633   val 79.90% | loss 0.7737   test 79.49% | loss 0.7791\n",
      "current lr = 0.09189026851349762\n",
      "E97  train 85.28% | loss 0.6601   val 80.69% | loss 0.7584   test 79.91% | loss 0.7788\n",
      "current lr = 0.09171617225799765\n",
      "E98  train 85.11% | loss 0.6608   val 79.42% | loss 0.7816   test 79.37% | loss 0.7913\n",
      "current lr = 0.09154039568136327\n",
      "E99  train 85.26% | loss 0.6584   val 79.76% | loss 0.7930   test 80.33% | loss 0.7823\n",
      "current lr = 0.09136294586384938\n",
      "E100  train 85.26% | loss 0.6595   val 79.70% | loss 0.7831   test 79.71% | loss 0.7903\n",
      "current lr = 0.09118382995310875\n",
      "E101  train 85.18% | loss 0.6570   val 80.98% | loss 0.7544   test 80.89% | loss 0.7555\n",
      "current lr = 0.09100305516390415\n",
      "E102  train 85.29% | loss 0.6562   val 79.31% | loss 0.7954   test 79.14% | loss 0.8016\n",
      "current lr = 0.09082062877781771\n",
      "E103  train 85.33% | loss 0.6566   val 80.67% | loss 0.7537   test 80.33% | loss 0.7589\n",
      "current lr = 0.0906365581429577\n",
      "E104  train 85.62% | loss 0.6492   val 79.11% | loss 0.8002   test 78.61% | loss 0.8107\n",
      "current lr = 0.09045085067366242\n",
      "E105  train 85.61% | loss 0.6513   val 79.92% | loss 0.7853   test 79.32% | loss 0.7848\n",
      "current lr = 0.09026351385020166\n",
      "E106  train 85.63% | loss 0.6478   val 81.90% | loss 0.7332   test 81.48% | loss 0.7385\n",
      "current lr = 0.09007455521847535\n",
      "E107  train 85.96% | loss 0.6434   val 80.65% | loss 0.7637   test 80.56% | loss 0.7642\n",
      "current lr = 0.08988398238970964\n",
      "E108  train 85.75% | loss 0.6429   val 79.46% | loss 0.7888   test 79.36% | loss 0.7905\n",
      "current lr = 0.08969180304015027\n",
      "E109  train 85.76% | loss 0.6435   val 80.30% | loss 0.7724   test 79.91% | loss 0.7783\n",
      "current lr = 0.08949802491075343\n",
      "E110  train 85.90% | loss 0.6388   val 80.95% | loss 0.7486   test 80.74% | loss 0.7567\n",
      "current lr = 0.0893026558068739\n",
      "E111  train 86.07% | loss 0.6389   val 79.85% | loss 0.7769   test 80.40% | loss 0.7744\n",
      "current lr = 0.08910570359795071\n",
      "E112  train 86.12% | loss 0.6350   val 79.58% | loss 0.7887   test 80.00% | loss 0.7864\n",
      "current lr = 0.08890717621719012\n",
      "E113  train 86.20% | loss 0.6331   val 79.67% | loss 0.7859   test 79.54% | loss 0.7908\n",
      "current lr = 0.08870708166124606\n",
      "E114  train 86.21% | loss 0.6348   val 82.26% | loss 0.7241   test 82.41% | loss 0.7188\n",
      "current lr = 0.08850542798989808\n",
      "E115  train 86.35% | loss 0.6303   val 80.16% | loss 0.7643   test 80.77% | loss 0.7583\n",
      "current lr = 0.08830222332572667\n",
      "E116  train 86.42% | loss 0.6278   val 80.56% | loss 0.7698   test 80.97% | loss 0.7634\n",
      "current lr = 0.08809747585378606\n",
      "E117  train 86.23% | loss 0.6335   val 80.19% | loss 0.7712   test 80.53% | loss 0.7660\n",
      "current lr = 0.08789119382127455\n",
      "E118  train 86.45% | loss 0.6291   val 80.53% | loss 0.7616   test 80.17% | loss 0.7661\n",
      "current lr = 0.08768338553720237\n",
      "E119  train 86.50% | loss 0.6262   val 77.22% | loss 0.8494   test 77.38% | loss 0.8468\n",
      "current lr = 0.08747405937205686\n",
      "E120  train 86.52% | loss 0.6243   val 81.66% | loss 0.7417   test 81.73% | loss 0.7397\n",
      "current lr = 0.08726322375746545\n",
      "E121  train 86.76% | loss 0.6213   val 82.11% | loss 0.7226   test 82.37% | loss 0.7174\n",
      "current lr = 0.08705088718585596\n",
      "E122  train 86.68% | loss 0.6242   val 81.26% | loss 0.7440   test 81.27% | loss 0.7419\n",
      "current lr = 0.08683705821011452\n",
      "E123  train 86.91% | loss 0.6170   val 80.29% | loss 0.7738   test 80.00% | loss 0.7718\n",
      "current lr = 0.0866217454432411\n",
      "E124  train 86.86% | loss 0.6173   val 82.50% | loss 0.7081   test 82.32% | loss 0.7159\n",
      "current lr = 0.08640495755800255\n",
      "E125  train 86.93% | loss 0.6164   val 80.68% | loss 0.7470   test 80.71% | loss 0.7484\n",
      "current lr = 0.08618670328658329\n",
      "E126  train 86.80% | loss 0.6171   val 81.55% | loss 0.7445   test 81.14% | loss 0.7586\n",
      "current lr = 0.08596699142023352\n",
      "E127  train 86.92% | loss 0.6155   val 80.79% | loss 0.7578   test 80.90% | loss 0.7653\n",
      "current lr = 0.0857458308089152\n",
      "E128  train 87.04% | loss 0.6119   val 79.58% | loss 0.7867   test 79.45% | loss 0.7914\n",
      "current lr = 0.08552323036094552\n",
      "E129  train 87.17% | loss 0.6112   val 82.11% | loss 0.7360   test 81.86% | loss 0.7327\n",
      "current lr = 0.08529919904263802\n",
      "E130  train 87.19% | loss 0.6100   val 82.60% | loss 0.7107   test 82.42% | loss 0.7189\n",
      "current lr = 0.08507374587794159\n",
      "E131  train 87.19% | loss 0.6106   val 81.06% | loss 0.7610   test 81.19% | loss 0.7521\n",
      "current lr = 0.08484687994807678\n",
      "E132  train 87.44% | loss 0.6026   val 82.54% | loss 0.7132   test 82.85% | loss 0.7137\n",
      "current lr = 0.0846186103911702\n",
      "E133  train 87.17% | loss 0.6067   val 81.67% | loss 0.7385   test 81.51% | loss 0.7409\n",
      "current lr = 0.08438894640188632\n",
      "E134  train 87.68% | loss 0.5979   val 82.24% | loss 0.7262   test 82.32% | loss 0.7274\n",
      "current lr = 0.08415789723105711\n",
      "E135  train 87.50% | loss 0.6006   val 82.57% | loss 0.7155   test 82.01% | loss 0.7245\n",
      "current lr = 0.0839254721853095\n",
      "E136  train 87.52% | loss 0.5998   val 81.73% | loss 0.7387   test 81.73% | loss 0.7396\n",
      "current lr = 0.08369168062669044\n",
      "E137  train 87.65% | loss 0.5992   val 83.08% | loss 0.6940   test 82.86% | loss 0.6989\n",
      "current lr = 0.08345653197228983\n",
      "E138  train 87.74% | loss 0.5937   val 81.86% | loss 0.7271   test 81.76% | loss 0.7377\n",
      "current lr = 0.08322003569386117\n",
      "E139  train 87.80% | loss 0.5935   val 81.39% | loss 0.7479   test 81.41% | loss 0.7462\n",
      "current lr = 0.08298220131744008\n",
      "E140  train 87.94% | loss 0.5893   val 78.03% | loss 0.8241   test 78.30% | loss 0.8301\n",
      "current lr = 0.08274303842296055\n",
      "E141  train 87.72% | loss 0.5969   val 82.12% | loss 0.7189   test 82.71% | loss 0.7119\n",
      "current lr = 0.08250255664386909\n",
      "E142  train 88.00% | loss 0.5912   val 82.10% | loss 0.7178   test 82.66% | loss 0.7207\n",
      "current lr = 0.08226076566673672\n",
      "E143  train 88.05% | loss 0.5900   val 82.23% | loss 0.7247   test 82.04% | loss 0.7292\n",
      "current lr = 0.08201767523086874\n",
      "E144  train 88.02% | loss 0.5894   val 82.52% | loss 0.7240   test 82.53% | loss 0.7155\n",
      "current lr = 0.08177329512791244\n",
      "E145  train 88.07% | loss 0.5856   val 81.85% | loss 0.7326   test 81.54% | loss 0.7353\n",
      "current lr = 0.08152763520146275\n",
      "E146  train 88.30% | loss 0.5845   val 81.94% | loss 0.7306   test 81.76% | loss 0.7370\n",
      "current lr = 0.08128070534666569\n",
      "E147  train 87.97% | loss 0.5909   val 82.67% | loss 0.7123   test 82.46% | loss 0.7133\n",
      "current lr = 0.08103251550981978\n",
      "E148  train 88.16% | loss 0.5852   val 82.42% | loss 0.7187   test 82.68% | loss 0.7143\n",
      "current lr = 0.08078307568797549\n",
      "E149  train 88.22% | loss 0.5832   val 81.84% | loss 0.7253   test 81.88% | loss 0.7331\n",
      "current lr = 0.08053239592853247\n",
      "E150  train 88.36% | loss 0.5779   val 83.22% | loss 0.6880   test 82.94% | loss 0.6999\n",
      "current lr = 0.08028048632883485\n",
      "E151  train 88.36% | loss 0.5774   val 80.15% | loss 0.7707   test 79.99% | loss 0.7756\n",
      "current lr = 0.0800273570357646\n",
      "E152  train 88.62% | loss 0.5766   val 81.89% | loss 0.7205   test 82.51% | loss 0.7157\n",
      "current lr = 0.07977301824533273\n",
      "E153  train 88.51% | loss 0.5781   val 83.01% | loss 0.7025   test 83.26% | loss 0.6986\n",
      "current lr = 0.07951748020226862\n",
      "E154  train 88.53% | loss 0.5751   val 83.41% | loss 0.6996   test 83.58% | loss 0.6988\n",
      "current lr = 0.07926075319960742\n",
      "E155  train 88.58% | loss 0.5755   val 83.23% | loss 0.6999   test 83.47% | loss 0.6889\n",
      "current lr = 0.07900284757827533\n",
      "E156  train 88.78% | loss 0.5701   val 82.98% | loss 0.6982   test 83.49% | loss 0.6886\n",
      "current lr = 0.07874377372667318\n",
      "E157  train 88.74% | loss 0.5710   val 83.02% | loss 0.7053   test 82.73% | loss 0.7168\n",
      "current lr = 0.07848354208025793\n",
      "E158  train 88.66% | loss 0.5706   val 82.61% | loss 0.7098   test 83.14% | loss 0.6941\n",
      "current lr = 0.07822216312112235\n",
      "E159  train 88.69% | loss 0.5692   val 82.56% | loss 0.7193   test 83.01% | loss 0.7117\n",
      "current lr = 0.0779596473775728\n",
      "E160  train 88.89% | loss 0.5687   val 83.09% | loss 0.7054   test 83.24% | loss 0.7013\n",
      "current lr = 0.07769600542370518\n",
      "E161  train 89.01% | loss 0.5637   val 84.26% | loss 0.6714   test 83.75% | loss 0.6742\n",
      "current lr = 0.07743124787897893\n",
      "E162  train 88.91% | loss 0.5659   val 82.96% | loss 0.7074   test 83.14% | loss 0.7014\n",
      "current lr = 0.07716538540778933\n",
      "E163  train 89.17% | loss 0.5635   val 81.77% | loss 0.7366   test 81.61% | loss 0.7455\n",
      "current lr = 0.07689842871903797\n",
      "E164  train 89.00% | loss 0.5626   val 83.13% | loss 0.7064   test 82.92% | loss 0.7109\n",
      "current lr = 0.07663038856570135\n",
      "E165  train 89.10% | loss 0.5602   val 71.66% | loss 1.0155   test 71.78% | loss 1.0162\n",
      "current lr = 0.07636127574439776\n",
      "E166  train 89.28% | loss 0.5581   val 83.99% | loss 0.6880   test 83.76% | loss 0.6872\n",
      "current lr = 0.07609110109495242\n",
      "E167  train 89.33% | loss 0.5549   val 82.53% | loss 0.7136   test 82.63% | loss 0.7154\n",
      "current lr = 0.07581987549996078\n",
      "E168  train 89.46% | loss 0.5544   val 83.71% | loss 0.6958   test 83.08% | loss 0.7023\n",
      "current lr = 0.07554760988435028\n",
      "E169  train 89.28% | loss 0.5550   val 83.89% | loss 0.6846   test 83.96% | loss 0.6867\n",
      "current lr = 0.0752743152149402\n",
      "E170  train 89.34% | loss 0.5548   val 83.26% | loss 0.6992   test 83.06% | loss 0.6988\n",
      "current lr = 0.0750000025\n",
      "E171  train 89.35% | loss 0.5547   val 83.45% | loss 0.7022   test 83.61% | loss 0.7003\n",
      "current lr = 0.07472468278880581\n",
      "E172  train 89.55% | loss 0.5519   val 82.05% | loss 0.7463   test 81.96% | loss 0.7440\n",
      "current lr = 0.07444836717119548\n",
      "E173  train 89.48% | loss 0.5524   val 83.30% | loss 0.7030   test 83.86% | loss 0.6948\n",
      "current lr = 0.07417106677712176\n",
      "E174  train 89.56% | loss 0.5499   val 82.62% | loss 0.7141   test 83.27% | loss 0.7006\n",
      "current lr = 0.07389279277620414\n",
      "E175  train 89.55% | loss 0.5507   val 82.99% | loss 0.7099   test 83.12% | loss 0.7050\n",
      "current lr = 0.07361355637727877\n",
      "E176  train 89.59% | loss 0.5480   val 82.62% | loss 0.7079   test 82.83% | loss 0.7074\n",
      "current lr = 0.07333336882794708\n",
      "E177  train 89.74% | loss 0.5455   val 81.91% | loss 0.7319   test 82.26% | loss 0.7353\n",
      "current lr = 0.07305224141412271\n",
      "E178  train 89.83% | loss 0.5430   val 84.06% | loss 0.6721   test 84.42% | loss 0.6702\n",
      "current lr = 0.0727701854595769\n",
      "E179  train 89.94% | loss 0.5435   val 83.47% | loss 0.6951   test 83.62% | loss 0.6956\n",
      "current lr = 0.07248721232548236\n",
      "E180  train 89.85% | loss 0.5427   val 83.05% | loss 0.6957   test 83.37% | loss 0.6905\n",
      "current lr = 0.07220333340995566\n",
      "E181  train 89.97% | loss 0.5413   val 82.69% | loss 0.7157   test 83.04% | loss 0.7123\n",
      "current lr = 0.07191856014759816\n",
      "E182  train 90.03% | loss 0.5380   val 82.06% | loss 0.7349   test 81.94% | loss 0.7369\n",
      "current lr = 0.07163290400903531\n",
      "E183  train 90.02% | loss 0.5379   val 84.71% | loss 0.6646   test 84.88% | loss 0.6612\n",
      "current lr = 0.07134637650045474\n",
      "E184  train 90.18% | loss 0.5341   val 82.88% | loss 0.7066   test 82.64% | loss 0.7106\n",
      "current lr = 0.07105898916314271\n",
      "E185  train 90.06% | loss 0.5364   val 84.18% | loss 0.6824   test 84.19% | loss 0.6807\n",
      "current lr = 0.07077075357301928\n",
      "E186  train 90.34% | loss 0.5305   val 83.89% | loss 0.6864   test 83.38% | loss 0.6960\n",
      "current lr = 0.07048168134017196\n",
      "E187  train 90.09% | loss 0.5354   val 83.53% | loss 0.6957   test 83.84% | loss 0.6927\n",
      "current lr = 0.07019178410838814\n",
      "E188  train 90.06% | loss 0.5351   val 83.81% | loss 0.6868   test 84.07% | loss 0.6873\n",
      "current lr = 0.06990107355468607\n",
      "E189  train 90.31% | loss 0.5327   val 83.19% | loss 0.7043   test 82.89% | loss 0.7111\n",
      "current lr = 0.06960956138884444\n",
      "E190  train 90.16% | loss 0.5309   val 83.33% | loss 0.7066   test 83.26% | loss 0.7036\n",
      "current lr = 0.06931725935293082\n",
      "E191  train 90.40% | loss 0.5265   val 81.23% | loss 0.7553   test 81.20% | loss 0.7479\n",
      "current lr = 0.06902417922082855\n",
      "E192  train 90.44% | loss 0.5275   val 82.50% | loss 0.7195   test 82.69% | loss 0.7172\n",
      "current lr = 0.06873033279776264\n",
      "E193  train 90.57% | loss 0.5251   val 84.58% | loss 0.6733   test 85.12% | loss 0.6639\n",
      "current lr = 0.06843573191982417\n",
      "E194  train 90.59% | loss 0.5240   val 84.33% | loss 0.6759   test 84.41% | loss 0.6705\n",
      "current lr = 0.06814038845349353\n",
      "E195  train 90.70% | loss 0.5209   val 84.63% | loss 0.6742   test 84.66% | loss 0.6775\n",
      "current lr = 0.0678443142951625\n",
      "E196  train 90.83% | loss 0.5194   val 85.48% | loss 0.6489   test 85.59% | loss 0.6454\n",
      "current lr = 0.06754752137065498\n",
      "E197  train 90.66% | loss 0.5196   val 85.21% | loss 0.6487   test 85.29% | loss 0.6581\n",
      "current lr = 0.0672500216347467\n",
      "E198  train 90.84% | loss 0.5185   val 84.46% | loss 0.6797   test 84.39% | loss 0.6766\n",
      "current lr = 0.06695182707068367\n",
      "E199  train 90.70% | loss 0.5207   val 84.44% | loss 0.6673   test 84.62% | loss 0.6633\n",
      "current lr = 0.06665294968969941\n",
      "E200  train 90.77% | loss 0.5158   val 83.03% | loss 0.7134   test 82.83% | loss 0.7167\n",
      "current lr = 0.06635340153053129\n",
      "E201  train 91.00% | loss 0.5136   val 83.83% | loss 0.6901   test 83.61% | loss 0.6872\n",
      "current lr = 0.06605319465893543\n",
      "E202  train 90.99% | loss 0.5121   val 83.45% | loss 0.6995   test 83.74% | loss 0.6861\n",
      "current lr = 0.0657523411672009\n",
      "E203  train 90.97% | loss 0.5113   val 84.26% | loss 0.6881   test 84.98% | loss 0.6735\n",
      "current lr = 0.06545085317366242\n",
      "E204  train 91.21% | loss 0.5064   val 84.00% | loss 0.6885   test 83.51% | loss 0.7003\n",
      "current lr = 0.06514874282221245\n",
      "E205  train 91.10% | loss 0.5084   val 83.53% | loss 0.7129   test 83.87% | loss 0.7013\n",
      "current lr = 0.0648460222818119\n",
      "E206  train 91.25% | loss 0.5074   val 83.88% | loss 0.6949   test 84.29% | loss 0.6841\n",
      "current lr = 0.06454270374599996\n",
      "E207  train 91.07% | loss 0.5103   val 75.52% | loss 0.8801   test 76.05% | loss 0.8627\n",
      "current lr = 0.06423879943240306\n",
      "E208  train 91.37% | loss 0.5012   val 83.56% | loss 0.6882   test 83.49% | loss 0.6914\n",
      "current lr = 0.06393432158224262\n",
      "E209  train 91.25% | loss 0.5060   val 85.80% | loss 0.6336   test 85.74% | loss 0.6429\n",
      "current lr = 0.06362928245984203\n",
      "E210  train 91.35% | loss 0.5037   val 85.12% | loss 0.6587   test 84.97% | loss 0.6541\n",
      "current lr = 0.06332369435213271\n",
      "E211  train 91.44% | loss 0.5020   val 81.22% | loss 0.7518   test 81.67% | loss 0.7407\n",
      "current lr = 0.06301756956815904\n",
      "E212  train 91.59% | loss 0.4986   val 84.84% | loss 0.6605   test 84.55% | loss 0.6690\n",
      "current lr = 0.06271092043858272\n",
      "E213  train 91.64% | loss 0.4987   val 84.93% | loss 0.6643   test 85.10% | loss 0.6603\n",
      "current lr = 0.062403759315185915\n",
      "E214  train 91.81% | loss 0.4937   val 85.14% | loss 0.6622   test 85.19% | loss 0.6609\n",
      "current lr = 0.06209609857037394\n",
      "E215  train 91.62% | loss 0.4965   val 84.54% | loss 0.6762   test 84.12% | loss 0.6789\n",
      "current lr = 0.06178795059667673\n",
      "E216  train 91.68% | loss 0.4982   val 85.22% | loss 0.6544   test 85.69% | loss 0.6451\n",
      "current lr = 0.06147932780624969\n",
      "E217  train 91.74% | loss 0.4950   val 85.48% | loss 0.6625   test 85.89% | loss 0.6472\n",
      "current lr = 0.06117024263037387\n",
      "E218  train 91.85% | loss 0.4898   val 84.83% | loss 0.6762   test 85.12% | loss 0.6673\n",
      "current lr = 0.06086070751895506\n",
      "E219  train 91.83% | loss 0.4900   val 84.06% | loss 0.6852   test 83.58% | loss 0.7029\n",
      "current lr = 0.06055073494002242\n",
      "E220  train 91.87% | loss 0.4900   val 85.52% | loss 0.6393   test 86.04% | loss 0.6326\n",
      "current lr = 0.060240337379226225\n",
      "E221  train 91.95% | loss 0.4896   val 85.19% | loss 0.6557   test 85.34% | loss 0.6512\n",
      "current lr = 0.059929527339334966\n",
      "E222  train 91.87% | loss 0.4908   val 84.97% | loss 0.6652   test 85.38% | loss 0.6543\n",
      "current lr = 0.059618317339731754\n",
      "E223  train 92.02% | loss 0.4864   val 85.43% | loss 0.6458   test 85.81% | loss 0.6440\n",
      "current lr = 0.059306719915909946\n",
      "E224  train 92.05% | loss 0.4855   val 85.45% | loss 0.6612   test 85.29% | loss 0.6632\n",
      "current lr = 0.05899474761896836\n",
      "E225  train 92.03% | loss 0.4869   val 85.81% | loss 0.6384   test 86.23% | loss 0.6271\n",
      "current lr = 0.058682413015105656\n",
      "E226  train 92.23% | loss 0.4813   val 83.67% | loss 0.7110   test 83.85% | loss 0.7053\n",
      "current lr = 0.0583697286851141\n",
      "E227  train 92.23% | loss 0.4814   val 85.73% | loss 0.6446   test 85.88% | loss 0.6340\n",
      "current lr = 0.058056707223872926\n",
      "E228  train 92.19% | loss 0.4810   val 85.70% | loss 0.6432   test 85.78% | loss 0.6449\n",
      "current lr = 0.05774336123984094\n",
      "E229  train 92.32% | loss 0.4785   val 85.56% | loss 0.6541   test 85.40% | loss 0.6575\n",
      "current lr = 0.0574297033545487\n",
      "E230  train 92.41% | loss 0.4791   val 85.24% | loss 0.6618   test 85.54% | loss 0.6506\n",
      "current lr = 0.057115746202090076\n",
      "E231  train 92.40% | loss 0.4788   val 85.38% | loss 0.6588   test 85.19% | loss 0.6540\n",
      "current lr = 0.05680150242861342\n",
      "E232  train 92.44% | loss 0.4748   val 85.97% | loss 0.6425   test 86.01% | loss 0.6412\n",
      "current lr = 0.05648698469181208\n",
      "E233  train 92.42% | loss 0.4734   val 84.58% | loss 0.6784   test 84.76% | loss 0.6760\n",
      "current lr = 0.05617220566041462\n",
      "E234  train 92.41% | loss 0.4758   val 85.50% | loss 0.6547   test 85.02% | loss 0.6579\n",
      "current lr = 0.05585717801367457\n",
      "E235  train 92.74% | loss 0.4703   val 85.29% | loss 0.6642   test 85.31% | loss 0.6638\n",
      "current lr = 0.055541914440859555\n",
      "E236  train 92.53% | loss 0.4709   val 84.98% | loss 0.6687   test 85.82% | loss 0.6532\n",
      "current lr = 0.05522642764074037\n",
      "E237  train 92.70% | loss 0.4693   val 86.45% | loss 0.6334   test 86.62% | loss 0.6288\n",
      "current lr = 0.05491073032107929\n",
      "E238  train 92.74% | loss 0.4675   val 86.12% | loss 0.6324   test 86.60% | loss 0.6172\n",
      "current lr = 0.054594835198118315\n",
      "E239  train 92.82% | loss 0.4654   val 85.89% | loss 0.6506   test 85.12% | loss 0.6624\n",
      "current lr = 0.05427875499606696\n",
      "E240  train 92.94% | loss 0.4632   val 85.47% | loss 0.6643   test 85.01% | loss 0.6735\n",
      "current lr = 0.05396250244658964\n",
      "E241  train 92.68% | loss 0.4684   val 80.25% | loss 0.7988   test 79.99% | loss 0.8051\n",
      "current lr = 0.05364609028829299\n",
      "E242  train 92.97% | loss 0.4655   val 84.53% | loss 0.6912   test 84.44% | loss 0.6856\n",
      "current lr = 0.053329531266212545\n",
      "E243  train 93.00% | loss 0.4613   val 85.54% | loss 0.6567   test 85.59% | loss 0.6526\n",
      "current lr = 0.05301283813129959\n",
      "E244  train 92.99% | loss 0.4601   val 85.54% | loss 0.6560   test 85.64% | loss 0.6571\n",
      "current lr = 0.05269602363990743\n",
      "E245  train 93.00% | loss 0.4598   val 85.25% | loss 0.6549   test 85.39% | loss 0.6564\n",
      "current lr = 0.05237910055327755\n",
      "E246  train 93.04% | loss 0.4602   val 85.72% | loss 0.6447   test 85.95% | loss 0.6435\n",
      "current lr = 0.05206208163702568\n",
      "E247  train 93.02% | loss 0.4587   val 85.67% | loss 0.6398   test 85.84% | loss 0.6372\n",
      "current lr = 0.05174497966062756\n",
      "E248  train 93.06% | loss 0.4586   val 86.33% | loss 0.6222   test 86.68% | loss 0.6160\n",
      "current lr = 0.05142780739690456\n",
      "E249  train 93.19% | loss 0.4567   val 86.39% | loss 0.6267   test 85.87% | loss 0.6322\n",
      "current lr = 0.0511105776215092\n",
      "E250  train 93.26% | loss 0.4543   val 86.49% | loss 0.6248   test 86.72% | loss 0.6123\n",
      "current lr = 0.050793303112410586\n",
      "E251  train 93.29% | loss 0.4532   val 86.18% | loss 0.6334   test 86.33% | loss 0.6335\n",
      "current lr = 0.05047599664937968\n",
      "E252  train 93.44% | loss 0.4494   val 85.44% | loss 0.6643   test 85.69% | loss 0.6561\n",
      "current lr = 0.05015867101347452\n",
      "E253  train 93.47% | loss 0.4487   val 86.43% | loss 0.6375   test 86.64% | loss 0.6299\n",
      "current lr = 0.0498413389865255\n",
      "E254  train 93.60% | loss 0.4469   val 86.84% | loss 0.6193   test 86.85% | loss 0.6205\n",
      "current lr = 0.04952401335062036\n",
      "E255  train 93.36% | loss 0.4516   val 85.11% | loss 0.6614   test 85.25% | loss 0.6602\n",
      "current lr = 0.04920670688758944\n",
      "E256  train 93.68% | loss 0.4442   val 86.21% | loss 0.6325   test 86.41% | loss 0.6334\n",
      "current lr = 0.04888943237849081\n",
      "E257  train 93.28% | loss 0.4527   val 86.33% | loss 0.6368   test 86.46% | loss 0.6359\n",
      "current lr = 0.04857220260309544\n",
      "E258  train 93.64% | loss 0.4462   val 86.77% | loss 0.6115   test 87.00% | loss 0.6138\n",
      "current lr = 0.04825503033937245\n",
      "E259  train 93.77% | loss 0.4411   val 86.74% | loss 0.6212   test 87.01% | loss 0.6188\n",
      "current lr = 0.047937928362974336\n",
      "E260  train 93.76% | loss 0.4409   val 86.35% | loss 0.6350   test 86.18% | loss 0.6390\n",
      "current lr = 0.047620909446722474\n",
      "E261  train 93.67% | loss 0.4434   val 86.71% | loss 0.6258   test 86.86% | loss 0.6279\n",
      "current lr = 0.047303986360092615\n",
      "E262  train 94.02% | loss 0.4350   val 85.78% | loss 0.6539   test 86.23% | loss 0.6430\n",
      "current lr = 0.04698717186870044\n",
      "E263  train 93.87% | loss 0.4375   val 87.24% | loss 0.6153   test 87.52% | loss 0.6127\n",
      "current lr = 0.04667047873378749\n",
      "E264  train 93.88% | loss 0.4385   val 86.81% | loss 0.6302   test 87.00% | loss 0.6172\n",
      "current lr = 0.04635391971170705\n",
      "E265  train 94.03% | loss 0.4366   val 87.76% | loss 0.5990   test 87.94% | loss 0.5840\n",
      "current lr = 0.04603750755341038\n",
      "E266  train 94.00% | loss 0.4366   val 86.42% | loss 0.6338   test 86.59% | loss 0.6338\n",
      "current lr = 0.045721255003933085\n",
      "E267  train 94.02% | loss 0.4354   val 86.19% | loss 0.6413   test 86.48% | loss 0.6289\n",
      "current lr = 0.045405174801881706\n",
      "E268  train 94.01% | loss 0.4337   val 86.40% | loss 0.6357   test 86.62% | loss 0.6309\n",
      "current lr = 0.04508927967892074\n",
      "E269  train 94.08% | loss 0.4323   val 86.47% | loss 0.6519   test 86.32% | loss 0.6476\n",
      "current lr = 0.044773582359259656\n",
      "E270  train 94.25% | loss 0.4289   val 86.05% | loss 0.6431   test 86.26% | loss 0.6386\n",
      "current lr = 0.044458095559140466\n",
      "E271  train 94.21% | loss 0.4310   val 86.04% | loss 0.6502   test 86.08% | loss 0.6499\n",
      "current lr = 0.04414283198632547\n",
      "E272  train 94.20% | loss 0.4308   val 86.54% | loss 0.6329   test 86.77% | loss 0.6295\n",
      "current lr = 0.043827804339585406\n",
      "E273  train 94.28% | loss 0.4275   val 87.17% | loss 0.6221   test 87.23% | loss 0.6146\n",
      "current lr = 0.04351302530818797\n",
      "E274  train 94.23% | loss 0.4281   val 87.04% | loss 0.6243   test 87.35% | loss 0.6159\n",
      "current lr = 0.04319850757138662\n",
      "E275  train 94.35% | loss 0.4271   val 85.69% | loss 0.6632   test 85.72% | loss 0.6591\n",
      "current lr = 0.04288426379790996\n",
      "E276  train 94.32% | loss 0.4266   val 84.17% | loss 0.7028   test 83.79% | loss 0.7056\n",
      "current lr = 0.042570306645451354\n",
      "E277  train 94.38% | loss 0.4246   val 86.71% | loss 0.6335   test 86.49% | loss 0.6436\n",
      "current lr = 0.0422566487601591\n",
      "E278  train 94.43% | loss 0.4232   val 86.94% | loss 0.6202   test 86.51% | loss 0.6259\n",
      "current lr = 0.04194330277612712\n",
      "E279  train 94.65% | loss 0.4180   val 87.85% | loss 0.6030   test 87.92% | loss 0.6009\n",
      "current lr = 0.04163028131488594\n",
      "E280  train 94.64% | loss 0.4180   val 87.90% | loss 0.5982   test 87.31% | loss 0.6081\n",
      "current lr = 0.04131759698489439\n",
      "E281  train 94.69% | loss 0.4170   val 87.10% | loss 0.6209   test 87.14% | loss 0.6107\n",
      "current lr = 0.04100526238103168\n",
      "E282  train 94.66% | loss 0.4179   val 86.42% | loss 0.6427   test 86.78% | loss 0.6361\n",
      "current lr = 0.04069329008409011\n",
      "E283  train 94.76% | loss 0.4141   val 87.44% | loss 0.6092   test 87.76% | loss 0.6082\n",
      "current lr = 0.040381692660268316\n",
      "E284  train 94.70% | loss 0.4161   val 87.28% | loss 0.6083   test 87.84% | loss 0.6039\n",
      "current lr = 0.04007048266066508\n",
      "E285  train 94.81% | loss 0.4133   val 87.88% | loss 0.6086   test 87.76% | loss 0.5976\n",
      "current lr = 0.03975967262077383\n",
      "E286  train 94.90% | loss 0.4117   val 88.09% | loss 0.5904   test 87.52% | loss 0.6036\n",
      "current lr = 0.03944927505997763\n",
      "E287  train 94.92% | loss 0.4113   val 87.04% | loss 0.6190   test 87.35% | loss 0.6077\n",
      "current lr = 0.039139302481044996\n",
      "E288  train 95.13% | loss 0.4065   val 87.28% | loss 0.6180   test 86.75% | loss 0.6277\n",
      "current lr = 0.03882976736962618\n",
      "E289  train 95.06% | loss 0.4074   val 87.69% | loss 0.6032   test 87.40% | loss 0.6131\n",
      "current lr = 0.03852068219375035\n",
      "E290  train 95.16% | loss 0.4062   val 87.67% | loss 0.6141   test 87.41% | loss 0.6163\n",
      "current lr = 0.03821205940332333\n",
      "E291  train 95.03% | loss 0.4051   val 87.64% | loss 0.6117   test 87.92% | loss 0.6014\n",
      "current lr = 0.037903911429626104\n",
      "E292  train 95.17% | loss 0.4052   val 87.06% | loss 0.6308   test 87.10% | loss 0.6238\n",
      "current lr = 0.037596250684814134\n",
      "E293  train 95.19% | loss 0.4057   val 86.79% | loss 0.6375   test 87.19% | loss 0.6266\n",
      "current lr = 0.03728908956141735\n",
      "E294  train 95.30% | loss 0.4010   val 82.00% | loss 0.7827   test 82.56% | loss 0.7677\n",
      "current lr = 0.036982440431841\n",
      "E295  train 95.35% | loss 0.4008   val 87.55% | loss 0.6150   test 87.37% | loss 0.6209\n",
      "current lr = 0.03667631564786734\n",
      "E296  train 95.36% | loss 0.3978   val 87.61% | loss 0.6217   test 88.08% | loss 0.6106\n",
      "current lr = 0.03637072754015801\n",
      "E297  train 95.46% | loss 0.3980   val 88.20% | loss 0.5993   test 87.99% | loss 0.6029\n",
      "current lr = 0.03606568841775744\n",
      "E298  train 95.47% | loss 0.3966   val 87.78% | loss 0.6182   test 87.99% | loss 0.6057\n",
      "current lr = 0.035761210567597006\n",
      "E299  train 95.60% | loss 0.3938   val 88.33% | loss 0.5926   test 88.12% | loss 0.5984\n",
      "current lr = 0.03545730625400008\n",
      "E300  train 95.52% | loss 0.3950   val 86.47% | loss 0.6350   test 86.25% | loss 0.6432\n",
      "current lr = 0.03515398771818815\n",
      "E301  train 95.59% | loss 0.3943   val 88.32% | loss 0.6028   test 88.48% | loss 0.5954\n",
      "current lr = 0.034851267177787575\n",
      "E302  train 95.60% | loss 0.3932   val 88.14% | loss 0.5940   test 88.38% | loss 0.5860\n",
      "current lr = 0.034549156826337614\n",
      "E303  train 95.64% | loss 0.3904   val 89.20% | loss 0.5652   test 89.28% | loss 0.5615\n",
      "current lr = 0.034247668832799144\n",
      "E304  train 95.70% | loss 0.3899   val 88.09% | loss 0.5979   test 88.21% | loss 0.5960\n",
      "current lr = 0.0339468153410646\n",
      "E305  train 95.69% | loss 0.3893   val 88.57% | loss 0.5961   test 88.57% | loss 0.5922\n",
      "current lr = 0.03364660846946876\n",
      "E306  train 95.66% | loss 0.3924   val 87.84% | loss 0.6012   test 88.08% | loss 0.5928\n",
      "current lr = 0.03334706031030061\n",
      "E307  train 95.89% | loss 0.3861   val 88.88% | loss 0.5791   test 88.76% | loss 0.5839\n",
      "current lr = 0.03304818292931636\n",
      "E308  train 95.82% | loss 0.3871   val 88.26% | loss 0.5932   test 88.35% | loss 0.5915\n",
      "current lr = 0.032749988365253314\n",
      "E309  train 96.00% | loss 0.3837   val 88.65% | loss 0.5795   test 88.39% | loss 0.5856\n",
      "current lr = 0.03245248862934505\n",
      "E310  train 95.93% | loss 0.3833   val 87.56% | loss 0.6160   test 87.36% | loss 0.6220\n",
      "current lr = 0.03215569570483753\n",
      "E311  train 95.92% | loss 0.3847   val 88.99% | loss 0.5781   test 88.86% | loss 0.5811\n",
      "current lr = 0.03185962154650648\n",
      "E312  train 96.03% | loss 0.3808   val 88.52% | loss 0.5864   test 88.59% | loss 0.5851\n",
      "current lr = 0.03156427808017585\n",
      "E313  train 96.01% | loss 0.3838   val 88.26% | loss 0.6097   test 88.32% | loss 0.6007\n",
      "current lr = 0.03126967720223737\n",
      "E314  train 96.22% | loss 0.3775   val 88.90% | loss 0.5897   test 89.56% | loss 0.5662\n",
      "current lr = 0.030975830779171466\n",
      "E315  train 96.05% | loss 0.3801   val 88.86% | loss 0.5857   test 88.78% | loss 0.5884\n",
      "current lr = 0.0306827506470692\n",
      "E316  train 96.21% | loss 0.3775   val 88.96% | loss 0.5892   test 89.28% | loss 0.5711\n",
      "current lr = 0.03039044861115557\n",
      "E317  train 96.26% | loss 0.3750   val 89.43% | loss 0.5594   test 89.10% | loss 0.5705\n",
      "current lr = 0.030098936445313966\n",
      "E318  train 96.38% | loss 0.3731   val 88.42% | loss 0.5977   test 87.99% | loss 0.6111\n",
      "current lr = 0.02980822589161189\n",
      "E319  train 96.33% | loss 0.3735   val 88.12% | loss 0.6103   test 88.81% | loss 0.5899\n",
      "current lr = 0.029518328659828084\n",
      "E320  train 96.50% | loss 0.3708   val 88.46% | loss 0.6009   test 88.22% | loss 0.6029\n",
      "current lr = 0.029229256426980748\n",
      "E321  train 96.38% | loss 0.3739   val 88.96% | loss 0.5813   test 88.94% | loss 0.5835\n",
      "current lr = 0.028941020836857307\n",
      "E322  train 96.44% | loss 0.3720   val 89.65% | loss 0.5638   test 89.28% | loss 0.5697\n",
      "current lr = 0.028653633499545285\n",
      "E323  train 96.56% | loss 0.3686   val 88.81% | loss 0.5814   test 88.80% | loss 0.5834\n",
      "current lr = 0.028367105990964724\n",
      "E324  train 96.37% | loss 0.3725   val 88.34% | loss 0.5990   test 88.51% | loss 0.5974\n",
      "current lr = 0.028081449852401866\n",
      "E325  train 96.54% | loss 0.3684   val 89.33% | loss 0.5785   test 89.78% | loss 0.5709\n",
      "current lr = 0.02779667659004436\n",
      "E326  train 96.62% | loss 0.3660   val 89.50% | loss 0.5663   test 89.58% | loss 0.5698\n",
      "current lr = 0.027512797674517683\n",
      "E327  train 96.80% | loss 0.3623   val 89.42% | loss 0.5799   test 89.53% | loss 0.5691\n",
      "current lr = 0.027229824540423143\n",
      "E328  train 96.72% | loss 0.3642   val 89.58% | loss 0.5717   test 89.53% | loss 0.5676\n",
      "current lr = 0.026947768585877323\n",
      "E329  train 96.69% | loss 0.3647   val 89.38% | loss 0.5753   test 89.63% | loss 0.5722\n",
      "current lr = 0.026666641172052932\n",
      "E330  train 96.73% | loss 0.3642   val 89.41% | loss 0.5697   test 89.64% | loss 0.5637\n",
      "current lr = 0.026386453622721247\n",
      "E331  train 96.82% | loss 0.3600   val 89.16% | loss 0.5910   test 88.76% | loss 0.5939\n",
      "current lr = 0.026107217223795873\n",
      "E332  train 96.89% | loss 0.3592   val 89.81% | loss 0.5616   test 89.88% | loss 0.5566\n",
      "current lr = 0.025828943222878223\n",
      "E333  train 96.97% | loss 0.3573   val 89.25% | loss 0.5792   test 89.31% | loss 0.5767\n",
      "current lr = 0.025551642828804545\n",
      "E334  train 97.02% | loss 0.3562   val 86.94% | loss 0.6621   test 87.28% | loss 0.6502\n",
      "current lr = 0.025275327211194194\n",
      "E335  train 96.95% | loss 0.3582   val 89.88% | loss 0.5658   test 90.24% | loss 0.5512\n",
      "current lr = 0.025000007500000015\n",
      "E336  train 97.02% | loss 0.3565   val 89.77% | loss 0.5691   test 89.94% | loss 0.5573\n",
      "current lr = 0.02472569478505979\n",
      "E337  train 97.21% | loss 0.3513   val 89.71% | loss 0.5685   test 89.42% | loss 0.5743\n",
      "current lr = 0.02445240011564972\n",
      "E338  train 97.08% | loss 0.3539   val 89.19% | loss 0.5766   test 89.27% | loss 0.5712\n",
      "current lr = 0.02418013450003922\n",
      "E339  train 97.23% | loss 0.3516   val 90.36% | loss 0.5550   test 90.43% | loss 0.5514\n",
      "current lr = 0.023908908905047568\n",
      "E340  train 97.25% | loss 0.3512   val 89.25% | loss 0.5891   test 89.66% | loss 0.5705\n",
      "current lr = 0.023638734255602232\n",
      "E341  train 97.18% | loss 0.3513   val 89.41% | loss 0.5777   test 89.40% | loss 0.5758\n",
      "current lr = 0.023369621434298632\n",
      "E342  train 97.32% | loss 0.3484   val 89.76% | loss 0.5704   test 90.00% | loss 0.5683\n",
      "current lr = 0.02310158128096202\n",
      "E343  train 97.36% | loss 0.3473   val 89.92% | loss 0.5664   test 90.18% | loss 0.5617\n",
      "current lr = 0.02283462459221065\n",
      "E344  train 97.41% | loss 0.3454   val 90.28% | loss 0.5622   test 90.31% | loss 0.5557\n",
      "current lr = 0.022568762121021078\n",
      "E345  train 97.47% | loss 0.3453   val 90.14% | loss 0.5518   test 89.99% | loss 0.5523\n",
      "current lr = 0.022304004576294806\n",
      "E346  train 97.47% | loss 0.3452   val 90.06% | loss 0.5630   test 90.29% | loss 0.5562\n",
      "current lr = 0.022040362622427184\n",
      "E347  train 97.61% | loss 0.3412   val 89.99% | loss 0.5678   test 90.19% | loss 0.5597\n",
      "current lr = 0.021777846878877648\n",
      "E348  train 97.60% | loss 0.3418   val 90.24% | loss 0.5512   test 90.34% | loss 0.5546\n",
      "current lr = 0.021516467919742054\n",
      "E349  train 97.59% | loss 0.3400   val 90.06% | loss 0.5675   test 90.34% | loss 0.5622\n",
      "current lr = 0.0212562362733268\n",
      "E350  train 97.64% | loss 0.3403   val 90.05% | loss 0.5606   test 90.28% | loss 0.5566\n",
      "current lr = 0.020997162421724645\n",
      "E351  train 97.62% | loss 0.3407   val 90.51% | loss 0.5469   test 90.86% | loss 0.5406\n",
      "current lr = 0.020739256800392552\n",
      "E352  train 97.85% | loss 0.3336   val 90.80% | loss 0.5444   test 90.91% | loss 0.5376\n",
      "current lr = 0.020482529797731318\n",
      "E353  train 97.84% | loss 0.3334   val 90.65% | loss 0.5507   test 90.59% | loss 0.5425\n",
      "current lr = 0.020226991754667224\n",
      "E354  train 97.96% | loss 0.3313   val 90.44% | loss 0.5638   test 90.66% | loss 0.5545\n",
      "current lr = 0.019972652964235355\n",
      "E355  train 97.89% | loss 0.3332   val 90.46% | loss 0.5622   test 90.81% | loss 0.5511\n",
      "current lr = 0.01971952367116511\n",
      "E356  train 97.98% | loss 0.3296   val 90.59% | loss 0.5581   test 90.64% | loss 0.5560\n",
      "current lr = 0.019467614071467486\n",
      "E357  train 98.02% | loss 0.3293   val 90.67% | loss 0.5539   test 91.04% | loss 0.5444\n",
      "current lr = 0.019216934312024484\n",
      "E358  train 98.00% | loss 0.3305   val 90.19% | loss 0.5661   test 90.59% | loss 0.5546\n",
      "current lr = 0.01896749449018018\n",
      "E359  train 98.03% | loss 0.3302   val 90.80% | loss 0.5448   test 90.85% | loss 0.5472\n",
      "current lr = 0.01871930465333427\n",
      "E360  train 98.05% | loss 0.3290   val 90.51% | loss 0.5565   test 90.92% | loss 0.5445\n",
      "current lr = 0.01847237479853721\n",
      "E361  train 98.20% | loss 0.3254   val 90.91% | loss 0.5470   test 90.99% | loss 0.5402\n",
      "current lr = 0.018226714872087522\n",
      "E362  train 98.19% | loss 0.3247   val 91.08% | loss 0.5439   test 91.03% | loss 0.5351\n",
      "current lr = 0.017982334769131227\n",
      "E363  train 98.16% | loss 0.3250   val 91.56% | loss 0.5257   test 91.34% | loss 0.5336\n",
      "current lr = 0.017739244333263224\n",
      "E364  train 98.25% | loss 0.3238   val 91.34% | loss 0.5305   test 91.20% | loss 0.5360\n",
      "current lr = 0.017497453356130874\n",
      "E365  train 98.34% | loss 0.3208   val 91.53% | loss 0.5340   test 91.77% | loss 0.5287\n",
      "current lr = 0.017256971577039412\n",
      "E366  train 98.41% | loss 0.3189   val 91.27% | loss 0.5431   test 91.27% | loss 0.5423\n",
      "current lr = 0.01701780868255988\n",
      "E367  train 98.38% | loss 0.3194   val 91.72% | loss 0.5307   test 91.81% | loss 0.5254\n",
      "current lr = 0.016779974306138784\n",
      "E368  train 98.38% | loss 0.3207   val 91.56% | loss 0.5296   test 91.49% | loss 0.5364\n",
      "current lr = 0.01654347802771011\n",
      "E369  train 98.51% | loss 0.3174   val 91.62% | loss 0.5273   test 91.31% | loss 0.5331\n",
      "current lr = 0.016308329373309494\n",
      "E370  train 98.65% | loss 0.3134   val 91.70% | loss 0.5314   test 91.61% | loss 0.5277\n",
      "current lr = 0.016074537814690426\n",
      "E371  train 98.56% | loss 0.3150   val 92.10% | loss 0.5074   test 91.86% | loss 0.5134\n",
      "current lr = 0.01584211276894284\n",
      "E372  train 98.60% | loss 0.3139   val 90.79% | loss 0.5575   test 90.97% | loss 0.5510\n",
      "current lr = 0.01561106359811362\n",
      "E373  train 98.72% | loss 0.3117   val 91.97% | loss 0.5181   test 91.81% | loss 0.5257\n",
      "current lr = 0.015381399608829737\n",
      "E374  train 98.72% | loss 0.3104   val 92.04% | loss 0.5162   test 91.97% | loss 0.5223\n",
      "current lr = 0.015153130051923164\n",
      "E375  train 98.75% | loss 0.3093   val 92.50% | loss 0.5049   test 92.51% | loss 0.5064\n",
      "current lr = 0.014926264122058374\n",
      "E376  train 98.80% | loss 0.3083   val 92.13% | loss 0.5138   test 92.03% | loss 0.5168\n",
      "current lr = 0.014700810957361922\n",
      "E377  train 98.83% | loss 0.3075   val 92.01% | loss 0.5222   test 91.80% | loss 0.5250\n",
      "current lr = 0.014476779639054456\n",
      "E378  train 98.85% | loss 0.3074   val 91.96% | loss 0.5158   test 92.09% | loss 0.5155\n",
      "current lr = 0.014254179191084753\n",
      "E379  train 98.92% | loss 0.3050   val 91.54% | loss 0.5359   test 91.50% | loss 0.5334\n",
      "current lr = 0.01403301857976643\n",
      "E380  train 98.98% | loss 0.3033   val 92.10% | loss 0.5175   test 92.09% | loss 0.5180\n",
      "current lr = 0.01381330671341667\n",
      "E381  train 99.07% | loss 0.3006   val 92.51% | loss 0.5045   test 92.85% | loss 0.4900\n",
      "current lr = 0.013595052441997404\n",
      "E382  train 99.05% | loss 0.3022   val 92.96% | loss 0.4962   test 92.76% | loss 0.5000\n",
      "current lr = 0.013378264556758864\n",
      "E383  train 99.03% | loss 0.3020   val 92.44% | loss 0.5108   test 92.49% | loss 0.5087\n",
      "current lr = 0.013162951789885429\n",
      "E384  train 99.11% | loss 0.2984   val 92.65% | loss 0.5009   test 92.36% | loss 0.5098\n",
      "current lr = 0.012949122814144013\n",
      "E385  train 99.11% | loss 0.2995   val 92.36% | loss 0.5168   test 92.61% | loss 0.5062\n",
      "current lr = 0.012736786242534506\n",
      "E386  train 99.19% | loss 0.2972   val 92.47% | loss 0.5087   test 92.64% | loss 0.5034\n",
      "current lr = 0.01252595062794311\n",
      "E387  train 99.30% | loss 0.2941   val 92.78% | loss 0.5122   test 92.55% | loss 0.5143\n",
      "current lr = 0.012316624462797599\n",
      "E388  train 99.36% | loss 0.2927   val 92.31% | loss 0.5174   test 92.54% | loss 0.5019\n",
      "current lr = 0.012108816178725426\n",
      "E389  train 99.27% | loss 0.2951   val 92.67% | loss 0.5058   test 93.13% | loss 0.4842\n",
      "current lr = 0.01190253414621392\n",
      "E390  train 99.34% | loss 0.2928   val 92.76% | loss 0.4997   test 93.19% | loss 0.4897\n",
      "current lr = 0.011697786674273295\n",
      "E391  train 99.31% | loss 0.2939   val 93.02% | loss 0.4933   test 93.29% | loss 0.4826\n",
      "current lr = 0.011494582010101905\n",
      "E392  train 99.42% | loss 0.2904   val 93.03% | loss 0.4907   test 93.13% | loss 0.4927\n",
      "current lr = 0.01129292833875392\n",
      "E393  train 99.48% | loss 0.2898   val 93.53% | loss 0.4820   test 93.54% | loss 0.4812\n",
      "current lr = 0.011092833782809876\n",
      "E394  train 99.50% | loss 0.2886   val 93.08% | loss 0.4923   test 93.11% | loss 0.4894\n",
      "current lr = 0.01089430640204927\n",
      "E395  train 99.54% | loss 0.2879   val 93.38% | loss 0.4847   test 93.49% | loss 0.4791\n",
      "current lr = 0.010697354193126103\n",
      "E396  train 99.49% | loss 0.2890   val 92.96% | loss 0.4933   test 93.56% | loss 0.4738\n",
      "current lr = 0.01050198508924657\n",
      "E397  train 99.51% | loss 0.2883   val 93.45% | loss 0.4757   test 93.85% | loss 0.4674\n",
      "current lr = 0.010308206959849731\n",
      "E398  train 99.52% | loss 0.2876   val 93.79% | loss 0.4720   test 93.83% | loss 0.4701\n",
      "current lr = 0.010116027610290367\n",
      "E399  train 99.62% | loss 0.2845   val 93.82% | loss 0.4769   test 94.12% | loss 0.4612\n",
      "current lr = 0.009925454781524661\n",
      "E400  train 99.62% | loss 0.2844   val 93.76% | loss 0.4743   test 93.88% | loss 0.4618\n",
      "current lr = 0.009736496149798348\n",
      "E401  train 99.66% | loss 0.2835   val 94.12% | loss 0.4609   test 94.22% | loss 0.4551\n",
      "current lr = 0.009549159326337597\n",
      "E402  train 99.70% | loss 0.2828   val 93.81% | loss 0.4717   test 93.96% | loss 0.4635\n",
      "current lr = 0.009363451857042319\n",
      "E403  train 99.70% | loss 0.2825   val 94.05% | loss 0.4610   test 94.01% | loss 0.4524\n",
      "current lr = 0.009179381222182285\n",
      "E404  train 99.65% | loss 0.2838   val 94.18% | loss 0.4553   test 94.58% | loss 0.4464\n",
      "current lr = 0.008996954836095864\n",
      "E405  train 99.73% | loss 0.2822   val 94.44% | loss 0.4463   test 94.34% | loss 0.4483\n",
      "current lr = 0.008816180046891265\n",
      "E406  train 99.73% | loss 0.2821   val 93.67% | loss 0.4704   test 93.89% | loss 0.4597\n",
      "current lr = 0.00863706413615064\n",
      "E407  train 99.75% | loss 0.2811   val 93.84% | loss 0.4688   test 94.29% | loss 0.4517\n",
      "current lr = 0.008459614318636743\n",
      "E408  train 99.83% | loss 0.2790   val 94.50% | loss 0.4464   test 94.74% | loss 0.4322\n",
      "current lr = 0.00828383774200239\n",
      "E409  train 99.81% | loss 0.2789   val 94.91% | loss 0.4340   test 95.04% | loss 0.4296\n",
      "current lr = 0.0081097414865024\n",
      "E410  train 99.84% | loss 0.2783   val 94.77% | loss 0.4383   test 94.88% | loss 0.4329\n",
      "current lr = 0.007937332564708603\n",
      "E411  train 99.84% | loss 0.2784   val 94.72% | loss 0.4416   test 94.87% | loss 0.4306\n",
      "current lr = 0.007766617921227219\n",
      "E412  train 99.86% | loss 0.2782   val 94.63% | loss 0.4385   test 95.02% | loss 0.4264\n",
      "current lr = 0.007597604432419178\n",
      "E413  train 99.89% | loss 0.2774   val 94.58% | loss 0.4416   test 95.24% | loss 0.4219\n",
      "current lr = 0.007430298906123185\n",
      "E414  train 99.91% | loss 0.2768   val 94.86% | loss 0.4338   test 95.19% | loss 0.4247\n",
      "current lr = 0.00726470808138143\n",
      "E415  train 99.89% | loss 0.2772   val 95.12% | loss 0.4230   test 95.39% | loss 0.4110\n",
      "current lr = 0.007100838628168218\n",
      "E416  train 99.91% | loss 0.2768   val 94.89% | loss 0.4320   test 95.38% | loss 0.4137\n",
      "current lr = 0.006938697147121181\n",
      "E417  train 99.91% | loss 0.2767   val 95.09% | loss 0.4203   test 95.48% | loss 0.4098\n",
      "current lr = 0.006778290169275578\n",
      "E418  train 99.95% | loss 0.2754   val 95.22% | loss 0.4181   test 95.47% | loss 0.4077\n",
      "current lr = 0.0066196241558010615\n",
      "E419  train 99.93% | loss 0.2759   val 95.41% | loss 0.4155   test 95.53% | loss 0.4069\n",
      "current lr = 0.006462705497741529\n",
      "E420  train 99.95% | loss 0.2754   val 95.42% | loss 0.4139   test 95.54% | loss 0.4051\n",
      "current lr = 0.006307540515757634\n",
      "E421  train 99.95% | loss 0.2756   val 95.53% | loss 0.4087   test 95.56% | loss 0.4021\n",
      "current lr = 0.006154135459872232\n",
      "E422  train 99.95% | loss 0.2752   val 95.41% | loss 0.4160   test 95.69% | loss 0.4017\n",
      "current lr = 0.006002496509218632\n",
      "E423  train 99.96% | loss 0.2753   val 95.58% | loss 0.4076   test 95.75% | loss 0.3991\n",
      "current lr = 0.0058526297717916105\n",
      "E424  train 99.96% | loss 0.2750   val 95.66% | loss 0.4023   test 95.96% | loss 0.3940\n",
      "current lr = 0.005704541284201531\n",
      "E425  train 99.97% | loss 0.2748   val 95.96% | loss 0.3936   test 95.89% | loss 0.3950\n",
      "current lr = 0.005558237011431071\n",
      "E426  train 99.97% | loss 0.2747   val 95.82% | loss 0.3925   test 95.81% | loss 0.3946\n",
      "current lr = 0.005413722846594997\n",
      "E427  train 99.97% | loss 0.2747   val 95.88% | loss 0.3928   test 95.93% | loss 0.3936\n",
      "current lr = 0.005271004610702797\n",
      "E428  train 99.98% | loss 0.2745   val 95.94% | loss 0.3937   test 96.17% | loss 0.3876\n",
      "current lr = 0.005130088052424229\n",
      "E429  train 99.98% | loss 0.2746   val 96.04% | loss 0.3869   test 96.29% | loss 0.3864\n",
      "current lr = 0.004990978847857671\n",
      "E430  train 99.98% | loss 0.2744   val 96.17% | loss 0.3818   test 96.11% | loss 0.3799\n",
      "current lr = 0.004853682600301635\n",
      "E431  train 99.99% | loss 0.2743   val 96.23% | loss 0.3792   test 96.21% | loss 0.3797\n",
      "current lr = 0.004718204840028921\n",
      "E432  train 99.99% | loss 0.2742   val 96.30% | loss 0.3777   test 96.47% | loss 0.3737\n",
      "current lr = 0.004584551024063988\n",
      "E433  train 99.99% | loss 0.2741   val 96.24% | loss 0.3784   test 96.31% | loss 0.3751\n",
      "current lr = 0.004452726535963043\n",
      "E434  train 99.98% | loss 0.2744   val 96.23% | loss 0.3791   test 96.41% | loss 0.3732\n",
      "current lr = 0.004322736685597236\n",
      "E435  train 99.99% | loss 0.2743   val 96.33% | loss 0.3792   test 96.52% | loss 0.3720\n",
      "current lr = 0.00419458670893881\n",
      "E436  train 100.00% | loss 0.2740   val 96.41% | loss 0.3736   test 96.47% | loss 0.3717\n",
      "current lr = 0.004068281767850108\n",
      "E437  train 99.99% | loss 0.2740   val 96.49% | loss 0.3692   test 96.51% | loss 0.3678\n",
      "current lr = 0.003943826949875762\n",
      "E438  train 99.99% | loss 0.2742   val 96.58% | loss 0.3702   test 96.54% | loss 0.3676\n",
      "current lr = 0.003821227268037674\n",
      "E439  train 100.00% | loss 0.2740   val 96.45% | loss 0.3699   test 96.59% | loss 0.3663\n",
      "current lr = 0.003700487660633181\n",
      "E440  train 100.00% | loss 0.2740   val 96.61% | loss 0.3641   test 96.59% | loss 0.3642\n",
      "current lr = 0.0035816129910360337\n",
      "E441  train 100.00% | loss 0.2740   val 96.47% | loss 0.3674   test 96.69% | loss 0.3624\n",
      "current lr = 0.0034646080475006073\n",
      "E442  train 100.00% | loss 0.2739   val 96.64% | loss 0.3658   test 96.76% | loss 0.3632\n",
      "current lr = 0.003349477542968961\n",
      "E443  train 100.00% | loss 0.2740   val 96.61% | loss 0.3643   test 96.76% | loss 0.3611\n",
      "current lr = 0.003236226114881024\n",
      "E444  train 100.00% | loss 0.2738   val 96.67% | loss 0.3615   test 96.91% | loss 0.3583\n",
      "current lr = 0.003124858324987822\n",
      "E445  train 100.00% | loss 0.2740   val 96.61% | loss 0.3634   test 96.77% | loss 0.3610\n",
      "current lr = 0.003015378659167686\n",
      "E446  train 100.00% | loss 0.2739   val 96.64% | loss 0.3614   test 96.83% | loss 0.3585\n",
      "current lr = 0.002907791527245585\n",
      "E447  train 100.00% | loss 0.2740   val 96.67% | loss 0.3613   test 96.97% | loss 0.3560\n",
      "current lr = 0.0028021012628155033\n",
      "E448  train 100.00% | loss 0.2739   val 96.69% | loss 0.3624   test 96.83% | loss 0.3562\n",
      "current lr = 0.002698312123065899\n",
      "E449  train 100.00% | loss 0.2738   val 96.76% | loss 0.3584   test 96.85% | loss 0.3551\n",
      "current lr = 0.002596428288608172\n",
      "E450  train 100.00% | loss 0.2738   val 96.73% | loss 0.3601   test 96.83% | loss 0.3563\n",
      "current lr = 0.0024964538633083203\n",
      "E451  train 100.00% | loss 0.2739   val 96.86% | loss 0.3574   test 97.04% | loss 0.3529\n",
      "current lr = 0.0023983928741216154\n",
      "E452  train 100.00% | loss 0.2738   val 96.89% | loss 0.3566   test 97.01% | loss 0.3520\n",
      "current lr = 0.002302249270930422\n",
      "E453  train 100.00% | loss 0.2738   val 96.92% | loss 0.3558   test 97.03% | loss 0.3511\n",
      "current lr = 0.002208026926385031\n",
      "E454  train 100.00% | loss 0.2739   val 96.92% | loss 0.3558   test 97.02% | loss 0.3517\n",
      "current lr = 0.00211572963574775\n",
      "E455  train 100.00% | loss 0.2738   val 96.94% | loss 0.3557   test 97.05% | loss 0.3521\n",
      "current lr = 0.0020253611167399986\n",
      "E456  train 100.00% | loss 0.2738   val 96.86% | loss 0.3562   test 97.02% | loss 0.3519\n",
      "current lr = 0.001936925009392534\n",
      "E457  train 100.00% | loss 0.2738   val 96.92% | loss 0.3550   test 96.99% | loss 0.3521\n",
      "current lr = 0.0018504248758988786\n",
      "E458  train 100.00% | loss 0.2738   val 96.93% | loss 0.3551   test 96.97% | loss 0.3511\n",
      "current lr = 0.0017658642004717746\n",
      "E459  train 100.00% | loss 0.2738   val 96.94% | loss 0.3550   test 96.96% | loss 0.3522\n",
      "current lr = 0.001683246389202923\n",
      "E460  train 100.00% | loss 0.2738   val 96.94% | loss 0.3544   test 97.01% | loss 0.3508\n",
      "current lr = 0.0016025747699256938\n",
      "E461  train 100.00% | loss 0.2738   val 96.93% | loss 0.3540   test 96.97% | loss 0.3506\n",
      "current lr = 0.0015238525920811617\n",
      "E462  train 100.00% | loss 0.2738   val 96.89% | loss 0.3543   test 97.06% | loss 0.3504\n",
      "current lr = 0.0014470830265871707\n",
      "E463  train 100.00% | loss 0.2738   val 96.90% | loss 0.3535   test 97.09% | loss 0.3501\n",
      "current lr = 0.0013722691657106151\n",
      "E464  train 100.00% | loss 0.2738   val 96.99% | loss 0.3530   test 97.07% | loss 0.3510\n",
      "current lr = 0.0012994140229429108\n",
      "E465  train 100.00% | loss 0.2738   val 96.86% | loss 0.3548   test 97.02% | loss 0.3511\n",
      "current lr = 0.0012285205328785762\n",
      "E466  train 100.00% | loss 0.2738   val 96.97% | loss 0.3536   test 97.01% | loss 0.3506\n",
      "current lr = 0.0011595915510970757\n",
      "E467  train 100.00% | loss 0.2738   val 96.96% | loss 0.3529   test 96.97% | loss 0.3501\n",
      "current lr = 0.001092629854047719\n",
      "E468  train 100.00% | loss 0.2738   val 97.02% | loss 0.3521   test 97.03% | loss 0.3498\n",
      "current lr = 0.0010276381389379154\n",
      "E469  train 100.00% | loss 0.2737   val 97.01% | loss 0.3521   test 97.09% | loss 0.3500\n",
      "current lr = 0.0009646190236244679\n",
      "E470  train 100.00% | loss 0.2738   val 96.97% | loss 0.3524   test 97.06% | loss 0.3497\n",
      "current lr = 0.0009035750465081564\n",
      "E471  train 100.00% | loss 0.2738   val 96.96% | loss 0.3525   test 97.12% | loss 0.3498\n",
      "current lr = 0.0008445086664314643\n",
      "E472  train 100.00% | loss 0.2738   val 96.99% | loss 0.3525   test 97.06% | loss 0.3500\n",
      "current lr = 0.0007874222625795801\n",
      "E473  train 100.00% | loss 0.2738   val 97.02% | loss 0.3523   test 97.06% | loss 0.3491\n",
      "current lr = 0.0007323181343845299\n",
      "E474  train 100.00% | loss 0.2738   val 97.02% | loss 0.3524   test 97.02% | loss 0.3500\n",
      "current lr = 0.0006791985014325562\n",
      "E475  train 100.00% | loss 0.2738   val 96.97% | loss 0.3526   test 97.04% | loss 0.3500\n",
      "current lr = 0.0006280655033747289\n",
      "E476  train 100.00% | loss 0.2738   val 96.99% | loss 0.3519   test 97.11% | loss 0.3496\n",
      "current lr = 0.000578921199840753\n",
      "E477  train 100.00% | loss 0.2738   val 96.99% | loss 0.3521   test 97.05% | loss 0.3506\n",
      "current lr = 0.0005317675703560131\n",
      "E478  train 100.00% | loss 0.2738   val 96.97% | loss 0.3516   test 97.06% | loss 0.3494\n",
      "current lr = 0.00048660651426182545\n",
      "E479  train 100.00% | loss 0.2738   val 97.00% | loss 0.3520   test 97.11% | loss 0.3499\n",
      "current lr = 0.00044343985063895517\n",
      "E480  train 100.00% | loss 0.2738   val 97.02% | loss 0.3524   test 97.10% | loss 0.3503\n",
      "current lr = 0.00040226931823429695\n",
      "E481  train 100.00% | loss 0.2738   val 96.93% | loss 0.3522   test 97.10% | loss 0.3496\n",
      "current lr = 0.0003630965753909143\n",
      "E482  train 100.00% | loss 0.2738   val 97.00% | loss 0.3519   test 97.09% | loss 0.3495\n",
      "current lr = 0.0003259231999811655\n",
      "E483  train 100.00% | loss 0.2738   val 96.99% | loss 0.3525   test 97.08% | loss 0.3499\n",
      "current lr = 0.00029075068934319856\n",
      "E484  train 100.00% | loss 0.2738   val 96.97% | loss 0.3526   test 97.05% | loss 0.3492\n",
      "current lr = 0.0002575804602206052\n",
      "E485  train 100.00% | loss 0.2737   val 97.01% | loss 0.3524   test 97.08% | loss 0.3500\n",
      "current lr = 0.00022641384870538328\n",
      "E486  train 100.00% | loss 0.2737   val 96.97% | loss 0.3527   test 97.06% | loss 0.3498\n",
      "current lr = 0.00019725211018410178\n",
      "E487  train 100.00% | loss 0.2738   val 96.95% | loss 0.3524   test 97.02% | loss 0.3491\n",
      "current lr = 0.0001700964192873249\n",
      "E488  train 100.00% | loss 0.2737   val 97.01% | loss 0.3521   test 97.11% | loss 0.3488\n",
      "current lr = 0.00014494786984233855\n",
      "E489  train 100.00% | loss 0.2738   val 97.02% | loss 0.3526   test 97.08% | loss 0.3492\n",
      "current lr = 0.00012180747482903579\n",
      "E490  train 100.00% | loss 0.2738   val 97.02% | loss 0.3528   test 97.04% | loss 0.3496\n",
      "current lr = 0.00010067616633916598\n",
      "E491  train 100.00% | loss 0.2739   val 97.01% | loss 0.3521   test 97.02% | loss 0.3495\n",
      "current lr = 8.155479553873718e-05\n",
      "E492  train 100.00% | loss 0.2738   val 97.01% | loss 0.3540   test 96.97% | loss 0.3520\n",
      "current lr = 6.444413263378791e-05\n",
      "E493  train 100.00% | loss 0.2738   val 96.99% | loss 0.3520   test 97.04% | loss 0.3495\n",
      "current lr = 4.934486683931762e-05\n",
      "E494  train 100.00% | loss 0.2738   val 97.01% | loss 0.3532   test 96.96% | loss 0.3507\n",
      "current lr = 3.625760635156435e-05\n",
      "E495  train 100.00% | loss 0.2738   val 97.01% | loss 0.3529   test 97.07% | loss 0.3494\n",
      "current lr = 2.5182878323457775e-05\n",
      "E496  train 100.00% | loss 0.2738   val 97.04% | loss 0.3522   test 97.08% | loss 0.3494\n",
      "current lr = 1.6121128843430547e-05\n",
      "E497  train 100.00% | loss 0.2737   val 96.98% | loss 0.3522   test 97.09% | loss 0.3491\n",
      "current lr = 9.072722917438264e-06\n",
      "E498  train 100.00% | loss 0.2737   val 97.01% | loss 0.3527   test 97.06% | loss 0.3494\n",
      "current lr = 4.0379444542379074e-06\n",
      "E499  train 100.00% | loss 0.2738   val 96.96% | loss 0.3524   test 97.04% | loss 0.3489\n",
      "current lr = 1.0169962539747473e-06\n",
      "E500  train 100.00% | loss 0.2738   val 96.98% | loss 0.3522   test 97.05% | loss 0.3495\n",
      "current lr = 1e-08\n",
      "✅ Best val acc: 0.9704375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31595/3861110918.py:218: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(\"best_scd_resnet.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Final Test Accuracy: 97.08%\n",
      "⏱️ Inference time for 1 image: 179.82 ms\n"
     ]
    }
   ],
   "source": [
    "import os, random, numpy as np, math, gc, time, torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 0. 环境\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "import os, random, numpy as np, torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(0);  np.random.seed(0);  random.seed(0)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"💻 device =\", device)\n",
    "\n",
    "ALPHA = alpha_train      # 1 / (std_train + 1e-5)\n",
    "BETA  = beta_train        # -mean_train * ALPHA\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 1. 数据集：128×128 + log1p\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "class SCDTensorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    每个 .pt 内:  X (N,128,128)  y (N,1)\n",
    "    \"\"\"\n",
    "    def __init__(self, pt_files):\n",
    "        self.meta, self.cache, off = [], {}, 0\n",
    "        for f in pt_files:\n",
    "            n = torch.load(f, map_location=\"cpu\")[0].shape[0]\n",
    "            self.meta.append((f, off, off+n));  off += n\n",
    "        self.N = off\n",
    "\n",
    "    def __len__(self): return self.N\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        for f, beg, end in self.meta:\n",
    "            if beg <= idx < end:\n",
    "                if f not in self.cache:\n",
    "                    self.cache[f] = torch.load(f, map_location=\"cpu\",weights_only=True)\n",
    "                X, y = self.cache[f]\n",
    "                img = X[idx - beg].float()          # (1,256,256)\n",
    "\n",
    "                #################  per-image Z-score 标准化(只在PCA中被验证为效果最好) ----------\n",
    "                # ───── 固定 α/β 归一化 ─────\n",
    "                img = img * ALPHA + BETA        # 只 1 乘 1 加\n",
    "                img = img.unsqueeze(0)          # (1,128,128)\n",
    "                # ───────────────────────────\n",
    "                ################### -----------------------\n",
    "\n",
    "                return img, y[idx - beg].long().squeeze()\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 2. 改良版 ResNet：SCDResNet64\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, stride, 1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_c)\n",
    "\n",
    "        self.down = nn.Identity()\n",
    "        if stride != 1 or in_c != out_c:\n",
    "            self.down = nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_c))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        return F.relu(y + self.down(x))\n",
    "\n",
    "def make_layer(in_c, out_c, blocks, stride):\n",
    "    layers = [BasicBlock(in_c, out_c, stride)]\n",
    "    layers += [BasicBlock(out_c, out_c) for _ in range(blocks-1)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class SCDResNet64(nn.Module):\n",
    "    def __init__(self, n_cls=8):\n",
    "        super().__init__()\n",
    "        # Stem：1×64×64 → 64×32×32\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=1, padding=3, bias=False),  # 步长由 2 改 1\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)                   # 保留\n",
    "        )\n",
    "        # 残差 stages 与原版完全一致\n",
    "        self.layer1 = make_layer( 64,  64, blocks=3, stride=1)  # 64×32×32\n",
    "        self.layer2 = make_layer( 64, 128, blocks=3, stride=2)  # 128×16×16\n",
    "        self.layer3 = make_layer(128, 192, blocks=3, stride=2)  # 192×8×8\n",
    "        self.layer4 = make_layer(192, 256, blocks=3, stride=2)  # 256×4×4\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, n_cls)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x);  x = self.layer2(x)\n",
    "        x = self.layer3(x);  x = self.layer4(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 3. 参数量检查\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "net = SCDResNet64().to(device)\n",
    "print(f\"Total params: {sum(p.numel() for p in net.parameters())/1e6:.2f} M\")\n",
    "\n",
    "\n",
    "# %% choose your split  ——  shuffle-split 全样本\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "\n",
    "# 文件划分：前10 + 后10 → 训练，中间4+4 → 验证和测试\n",
    "\n",
    "pt_files = sorted([f\"../../cached_scd_tim/2022_c_64_512sample_hamming/2022_batch_{i:02d}.pt\" for i in range(1, 29)])  # 01-28\n",
    "train_files = pt_files[:10] + pt_files[-10:]   # 第01-10, 第19-28\n",
    "val_files   = pt_files[10:14]                  # 第11-14\n",
    "test_files  = pt_files[14:18]                  # 第15-18\n",
    "\n",
    "train_ds = SCDTensorDataset(train_files)\n",
    "val_ds   = SCDTensorDataset(val_files)\n",
    "test_ds  = SCDTensorDataset(test_files)\n",
    "\n",
    "print(f\"🔀 Shuffled split → train {len(train_ds):,} | val {len(val_ds):,} | test {len(test_ds):,}\")\n",
    "\n",
    "# 4. DataLoader（其余参数保持原样）\n",
    "num_workers = 4\n",
    "batch_size  = 32\n",
    "train_ld = DataLoader(train_ds, batch_size, shuffle=True,\n",
    "                      num_workers=num_workers, pin_memory=(device == \"cuda\"))\n",
    "val_ld   = DataLoader(val_ds,   batch_size, shuffle=False,\n",
    "                      num_workers=num_workers, pin_memory=(device == \"cuda\"))\n",
    "test_ld  = DataLoader(test_ds,  batch_size, shuffle=False,\n",
    "                      num_workers=num_workers, pin_memory=(device == \"cuda\"))\n",
    "\n",
    "# %% training\n",
    "net = SCDResNet64(n_cls=8).to(device)\n",
    "for m in net.modules():                           # ★ 新增：减缓 BN 统计抖动\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        m.momentum = 0.1          # 原默认 0.1\n",
    "\n",
    "\n",
    "# ------------------- 1️⃣  定义优化器 -------------------\n",
    "optim = torch.optim.SGD(net.parameters(), lr=0.1,\n",
    "                        momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# ------------------- 2️⃣  构造调度器 -------------------\n",
    "warm_epochs   = 5          # 线性预热 5 epoch\n",
    "total_epochs  = 500        # 总训练周期\n",
    "eta_min       = 1e-8       # Cosine 末尾最小 lr\n",
    "\n",
    "warmup = torch.optim.lr_scheduler.LinearLR(\n",
    "            optim, start_factor=0.1, end_factor=1.0, total_iters=warm_epochs)\n",
    "\n",
    "cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optim, T_max=total_epochs - warm_epochs, eta_min=eta_min)\n",
    "\n",
    "# SequentialLR 会先跑 warmup，再接 cosine\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "               optim, schedulers=[warmup, cosine],\n",
    "               milestones=[warm_epochs])        # 切换点=5\n",
    "\n",
    "crit = nn.CrossEntropyLoss(label_smoothing=0.05)   # ★ 改：加入 label-smoothing\n",
    "\n",
    "def run_epoch(loader, training=True):\n",
    "    net.train(training)\n",
    "    tot=loss_sum=acc=0\n",
    "    for xb, yb in loader:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        with torch.set_grad_enabled(training):\n",
    "            out  = net(xb)\n",
    "            loss = crit(out,yb)\n",
    "            if training:\n",
    "                optim.zero_grad(); loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)   # ★ 新增：梯度裁剪\n",
    "                optim.step()\n",
    "        pred = out.argmax(1)\n",
    "        bsz  = yb.size(0)\n",
    "        tot  += bsz\n",
    "        loss_sum += loss.item()*bsz\n",
    "        acc  += (pred==yb).sum().item()\n",
    "    return loss_sum/tot, acc/tot\n",
    "\n",
    "\n",
    "best = 0\n",
    "epochs=total_epochs\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    tr_l, tr_a = run_epoch(train_ld, True)\n",
    "    vl_l, vl_a = run_epoch(val_ld, False)\n",
    "    ts_l, ts_a = run_epoch(test_ld, False)   # 👈 直接监控 test\n",
    "\n",
    "    print(f\"E{ep:02d}  train {tr_a*100:5.2f}% | loss {tr_l:.4f}   \"\n",
    "          f\"val {vl_a*100:5.2f}% | loss {vl_l:.4f}   \"\n",
    "          f\"test {ts_a*100:5.2f}% | loss {ts_l:.4f}\")\n",
    "\n",
    "    scheduler.step()        # ← 每个 epoch 调一次\n",
    "\n",
    "    if vl_a > best:\n",
    "        best = vl_a\n",
    "        torch.save(net.state_dict(), \"best_scd_resnet.pth\")\n",
    "\n",
    "    print(\"current lr =\", optim.param_groups[0]['lr'])\n",
    "\n",
    "print(\"✅ Best val acc:\", best)\n",
    "\n",
    "# evaluate best on test\n",
    "net.load_state_dict(torch.load(\"best_scd_resnet.pth\"))\n",
    "net.eval()\n",
    "test_loss, test_acc = run_epoch(test_ld, training=False)\n",
    "print(f\"🧪 Final Test Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# ⏱️ 单张图像推理时间评估\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "import time\n",
    "\n",
    "# 从 test_ld 中取一张图像（确保在 eval 模式，无梯度）\n",
    "net.eval()\n",
    "xb, yb = next(iter(test_ld))\n",
    "img = xb[0:1].to(device)    # 取第 0 张，保持 shape = (1, 1, 256, 256)\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.synchronize() if device == \"cuda\" else None\n",
    "    t0 = time.time()\n",
    "    out = net(img)\n",
    "    torch.cuda.synchronize() if device == \"cuda\" else None\n",
    "    t1 = time.time()\n",
    "\n",
    "print(f\"⏱️ Inference time for 1 image: {(t1 - t0)*1000:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💻 device = cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_350822/1792734876.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  n = torch.load(f, map_location=\"cpu\")[0].shape[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 train 80,000 | val 16,000 | test 16,000\n",
      "Student params: 0.15 M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_350822/1792734876.py:191: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\"best_scd_resnet.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E001 | train 16.90% / 2.0848   val 17.93% / 2.0239   test 17.84% / 2.0242   lr 1.400e-02\n",
      "E002 | train 17.52% / 2.0710   val 17.83% / 2.0107   test 17.81% / 2.0111   lr 2.300e-02\n",
      "E003 | train 17.67% / 2.0618   val 17.66% / 2.0040   test 17.65% / 2.0040   lr 3.200e-02\n",
      "E004 | train 17.81% / 2.0579   val 18.09% / 1.9947   test 18.08% / 1.9948   lr 4.100e-02\n",
      "E005 | train 17.82% / 2.0557   val 18.59% / 1.9947   test 18.44% / 1.9951   lr 5.000e-02\n",
      "E006 | train 17.89% / 2.0542   val 18.44% / 1.9928   test 18.72% / 1.9932   lr 5.000e-02\n",
      "E007 | train 17.98% / 2.0487   val 18.28% / 1.9884   test 18.15% / 1.9885   lr 5.000e-02\n",
      "E008 | train 18.46% / 2.0415   val 18.88% / 1.9806   test 18.76% / 1.9808   lr 5.000e-02\n",
      "E009 | train 18.75% / 2.0357   val 19.83% / 1.9670   test 19.82% / 1.9683   lr 4.999e-02\n",
      "E010 | train 19.27% / 2.0284   val 20.26% / 1.9566   test 20.12% / 1.9572   lr 4.999e-02\n",
      "E011 | train 20.24% / 2.0189   val 20.92% / 1.9544   test 20.92% / 1.9564   lr 4.998e-02\n",
      "E012 | train 20.93% / 2.0112   val 21.16% / 1.9547   test 20.87% / 1.9583   lr 4.998e-02\n",
      "E013 | train 22.01% / 1.9992   val 23.04% / 1.9397   test 22.86% / 1.9393   lr 4.997e-02\n",
      "E014 | train 23.13% / 1.9839   val 25.49% / 1.8826   test 25.37% / 1.8849   lr 4.996e-02\n",
      "E015 | train 24.56% / 1.9679   val 26.02% / 1.8799   test 26.08% / 1.8841   lr 4.995e-02\n",
      "E016 | train 26.37% / 1.9429   val 29.90% / 1.8146   test 29.72% / 1.8156   lr 4.994e-02\n",
      "E017 | train 28.61% / 1.9079   val 32.34% / 1.7790   test 31.59% / 1.7850   lr 4.993e-02\n",
      "E018 | train 30.89% / 1.8718   val 34.10% / 1.7330   test 33.78% / 1.7361   lr 4.991e-02\n",
      "E019 | train 33.05% / 1.8335   val 36.31% / 1.6922   test 36.04% / 1.7005   lr 4.990e-02\n",
      "E020 | train 34.57% / 1.8045   val 37.62% / 1.6681   test 37.43% / 1.6748   lr 4.989e-02\n",
      "E021 | train 35.64% / 1.7792   val 40.04% / 1.6186   test 39.86% / 1.6252   lr 4.987e-02\n",
      "E022 | train 37.07% / 1.7543   val 40.71% / 1.6114   test 40.72% / 1.6084   lr 4.985e-02\n",
      "E023 | train 38.31% / 1.7324   val 41.50% / 1.5863   test 40.83% / 1.5922   lr 4.984e-02\n",
      "E024 | train 38.76% / 1.7218   val 43.02% / 1.5608   test 42.91% / 1.5624   lr 4.982e-02\n",
      "E025 | train 40.00% / 1.6990   val 45.57% / 1.5119   test 45.02% / 1.5205   lr 4.980e-02\n",
      "E026 | train 40.97% / 1.6795   val 44.59% / 1.5114   test 44.59% / 1.5140   lr 4.978e-02\n",
      "E027 | train 41.56% / 1.6690   val 46.67% / 1.4822   test 46.67% / 1.4864   lr 4.976e-02\n",
      "E028 | train 42.09% / 1.6566   val 47.62% / 1.4431   test 47.66% / 1.4439   lr 4.973e-02\n",
      "E029 | train 42.95% / 1.6404   val 45.64% / 1.4988   test 45.65% / 1.5005   lr 4.971e-02\n",
      "E030 | train 43.39% / 1.6314   val 48.71% / 1.4379   test 47.94% / 1.4447   lr 4.969e-02\n",
      "E031 | train 43.43% / 1.6330   val 47.11% / 1.4569   test 46.80% / 1.4614   lr 4.966e-02\n",
      "E032 | train 43.75% / 1.6229   val 48.07% / 1.4402   test 48.19% / 1.4476   lr 4.963e-02\n",
      "E033 | train 44.39% / 1.6064   val 49.49% / 1.4111   test 49.20% / 1.4181   lr 4.961e-02\n",
      "E034 | train 44.87% / 1.6021   val 48.24% / 1.4331   test 48.34% / 1.4359   lr 4.958e-02\n",
      "E035 | train 44.94% / 1.5992   val 49.63% / 1.4101   test 49.12% / 1.4169   lr 4.955e-02\n",
      "E036 | train 45.46% / 1.5868   val 49.90% / 1.4018   test 49.57% / 1.4107   lr 4.952e-02\n",
      "E037 | train 45.67% / 1.5834   val 50.18% / 1.3949   test 49.65% / 1.4036   lr 4.949e-02\n",
      "E038 | train 46.01% / 1.5771   val 50.56% / 1.3845   test 50.23% / 1.3964   lr 4.945e-02\n",
      "E039 | train 46.25% / 1.5724   val 51.26% / 1.3740   test 50.71% / 1.3835   lr 4.942e-02\n",
      "E040 | train 46.64% / 1.5637   val 51.67% / 1.3599   test 51.28% / 1.3677   lr 4.939e-02\n",
      "E041 | train 46.76% / 1.5591   val 51.79% / 1.3680   test 51.85% / 1.3664   lr 4.935e-02\n",
      "E042 | train 46.96% / 1.5544   val 51.21% / 1.3667   test 51.02% / 1.3750   lr 4.931e-02\n",
      "E043 | train 47.07% / 1.5538   val 51.12% / 1.3914   test 50.59% / 1.3977   lr 4.928e-02\n",
      "E044 | train 47.45% / 1.5450   val 52.78% / 1.3437   test 52.00% / 1.3520   lr 4.924e-02\n",
      "E045 | train 47.72% / 1.5370   val 52.30% / 1.3461   test 52.48% / 1.3533   lr 4.920e-02\n",
      "E046 | train 47.82% / 1.5351   val 52.97% / 1.3487   test 52.34% / 1.3498   lr 4.916e-02\n",
      "E047 | train 47.99% / 1.5369   val 50.14% / 1.3945   test 50.40% / 1.3969   lr 4.912e-02\n",
      "E048 | train 48.07% / 1.5324   val 53.59% / 1.3222   test 53.42% / 1.3291   lr 4.907e-02\n",
      "E049 | train 48.17% / 1.5298   val 52.08% / 1.3501   test 52.11% / 1.3526   lr 4.903e-02\n",
      "E050 | train 48.37% / 1.5257   val 52.99% / 1.3307   test 52.64% / 1.3390   lr 4.899e-02\n",
      "E051 | train 48.49% / 1.5217   val 53.16% / 1.3281   test 53.01% / 1.3330   lr 4.894e-02\n",
      "E052 | train 48.43% / 1.5199   val 52.87% / 1.3346   test 52.85% / 1.3359   lr 4.890e-02\n",
      "E053 | train 48.80% / 1.5132   val 54.13% / 1.3237   test 53.21% / 1.3372   lr 4.885e-02\n",
      "E054 | train 48.66% / 1.5169   val 54.21% / 1.3101   test 54.06% / 1.3169   lr 4.880e-02\n",
      "E055 | train 49.44% / 1.5051   val 53.42% / 1.3307   test 52.83% / 1.3438   lr 4.875e-02\n",
      "E056 | train 49.36% / 1.5033   val 53.10% / 1.3370   test 52.48% / 1.3462   lr 4.870e-02\n",
      "E057 | train 49.56% / 1.4998   val 51.23% / 1.3715   test 51.01% / 1.3808   lr 4.865e-02\n",
      "E058 | train 49.50% / 1.4957   val 55.57% / 1.2766   test 54.80% / 1.2919   lr 4.860e-02\n",
      "E059 | train 49.48% / 1.4992   val 54.71% / 1.3038   test 54.49% / 1.3147   lr 4.855e-02\n",
      "E060 | train 49.78% / 1.4934   val 54.68% / 1.2987   test 53.91% / 1.3123   lr 4.849e-02\n",
      "E061 | train 50.00% / 1.4901   val 52.90% / 1.3359   test 52.38% / 1.3459   lr 4.844e-02\n",
      "E062 | train 50.09% / 1.4860   val 52.33% / 1.3489   test 52.12% / 1.3527   lr 4.838e-02\n",
      "E063 | train 49.90% / 1.4892   val 54.54% / 1.2893   test 54.16% / 1.3010   lr 4.833e-02\n",
      "E064 | train 50.32% / 1.4819   val 55.92% / 1.2638   test 55.57% / 1.2773   lr 4.827e-02\n",
      "E065 | train 49.95% / 1.4882   val 55.84% / 1.2797   test 55.18% / 1.2882   lr 4.821e-02\n",
      "E066 | train 50.22% / 1.4790   val 55.33% / 1.2738   test 55.17% / 1.2793   lr 4.815e-02\n",
      "E067 | train 50.52% / 1.4787   val 54.26% / 1.2946   test 54.24% / 1.3056   lr 4.809e-02\n",
      "E068 | train 50.39% / 1.4775   val 55.39% / 1.2726   test 54.77% / 1.2802   lr 4.803e-02\n",
      "E069 | train 50.70% / 1.4715   val 52.60% / 1.3468   test 52.06% / 1.3531   lr 4.797e-02\n",
      "E070 | train 50.90% / 1.4669   val 56.01% / 1.2577   test 56.01% / 1.2684   lr 4.790e-02\n",
      "E071 | train 50.88% / 1.4662   val 56.57% / 1.2569   test 55.99% / 1.2684   lr 4.784e-02\n",
      "E072 | train 50.94% / 1.4647   val 54.27% / 1.3028   test 53.90% / 1.3148   lr 4.777e-02\n",
      "E073 | train 50.82% / 1.4681   val 53.83% / 1.3036   test 53.57% / 1.3105   lr 4.771e-02\n",
      "E074 | train 51.24% / 1.4603   val 57.41% / 1.2327   test 57.58% / 1.2397   lr 4.764e-02\n",
      "E075 | train 51.56% / 1.4541   val 57.21% / 1.2384   test 57.43% / 1.2453   lr 4.757e-02\n",
      "E076 | train 51.18% / 1.4574   val 54.33% / 1.3174   test 54.35% / 1.3203   lr 4.750e-02\n",
      "E077 | train 51.66% / 1.4508   val 56.99% / 1.2428   test 56.60% / 1.2538   lr 4.743e-02\n",
      "E078 | train 51.40% / 1.4540   val 56.29% / 1.2644   test 56.30% / 1.2634   lr 4.736e-02\n",
      "E079 | train 51.57% / 1.4521   val 55.46% / 1.2711   test 55.27% / 1.2716   lr 4.729e-02\n",
      "E080 | train 51.52% / 1.4502   val 55.98% / 1.2614   test 55.93% / 1.2659   lr 4.722e-02\n",
      "E081 | train 51.70% / 1.4505   val 55.84% / 1.2675   test 55.23% / 1.2724   lr 4.715e-02\n",
      "E082 | train 51.78% / 1.4457   val 54.53% / 1.2950   test 54.19% / 1.3118   lr 4.707e-02\n",
      "E083 | train 51.74% / 1.4478   val 54.19% / 1.3136   test 53.56% / 1.3277   lr 4.700e-02\n",
      "E084 | train 51.83% / 1.4434   val 56.47% / 1.2535   test 55.54% / 1.2702   lr 4.692e-02\n",
      "E085 | train 51.86% / 1.4454   val 57.02% / 1.2443   test 56.21% / 1.2533   lr 4.685e-02\n",
      "E086 | train 52.20% / 1.4380   val 56.47% / 1.2642   test 56.21% / 1.2650   lr 4.677e-02\n",
      "E087 | train 51.97% / 1.4410   val 56.61% / 1.2517   test 56.89% / 1.2514   lr 4.669e-02\n",
      "E088 | train 52.30% / 1.4360   val 57.20% / 1.2538   test 55.55% / 1.2655   lr 4.661e-02\n",
      "E089 | train 52.18% / 1.4358   val 56.88% / 1.2408   test 56.73% / 1.2483   lr 4.653e-02\n",
      "E090 | train 52.40% / 1.4288   val 56.69% / 1.2447   test 56.91% / 1.2485   lr 4.645e-02\n",
      "E091 | train 52.26% / 1.4316   val 57.74% / 1.2180   test 57.35% / 1.2296   lr 4.637e-02\n",
      "E092 | train 52.63% / 1.4251   val 57.11% / 1.2375   test 56.35% / 1.2526   lr 4.628e-02\n",
      "E093 | train 52.64% / 1.4254   val 57.88% / 1.2199   test 57.03% / 1.2321   lr 4.620e-02\n",
      "E094 | train 52.64% / 1.4276   val 57.63% / 1.2203   test 57.19% / 1.2250   lr 4.612e-02\n",
      "E095 | train 52.41% / 1.4269   val 58.98% / 1.1911   test 58.79% / 1.1997   lr 4.603e-02\n",
      "E096 | train 52.71% / 1.4248   val 57.00% / 1.2445   test 56.91% / 1.2494   lr 4.595e-02\n",
      "E097 | train 52.85% / 1.4232   val 55.36% / 1.2688   test 55.59% / 1.2696   lr 4.586e-02\n",
      "E098 | train 52.58% / 1.4254   val 58.82% / 1.2061   test 58.56% / 1.2098   lr 4.577e-02\n",
      "E099 | train 53.07% / 1.4172   val 57.70% / 1.2139   test 57.57% / 1.2290   lr 4.568e-02\n",
      "E100 | train 52.95% / 1.4168   val 58.08% / 1.2093   test 57.84% / 1.2126   lr 4.559e-02\n",
      "E101 | train 53.37% / 1.4096   val 57.10% / 1.2343   test 56.94% / 1.2357   lr 4.550e-02\n",
      "E102 | train 52.92% / 1.4171   val 58.14% / 1.2117   test 58.02% / 1.2169   lr 4.541e-02\n",
      "E103 | train 53.13% / 1.4135   val 56.43% / 1.2542   test 56.21% / 1.2598   lr 4.532e-02\n",
      "E104 | train 53.13% / 1.4134   val 58.84% / 1.1934   test 58.69% / 1.2030   lr 4.523e-02\n",
      "E105 | train 53.41% / 1.4081   val 59.41% / 1.1923   test 59.19% / 1.2016   lr 4.513e-02\n",
      "E106 | train 52.99% / 1.4133   val 56.17% / 1.2594   test 56.04% / 1.2658   lr 4.504e-02\n",
      "E107 | train 53.30% / 1.4113   val 55.60% / 1.2563   test 55.84% / 1.2592   lr 4.494e-02\n",
      "E108 | train 53.62% / 1.4038   val 58.11% / 1.2282   test 57.63% / 1.2322   lr 4.485e-02\n",
      "E109 | train 53.57% / 1.4032   val 58.28% / 1.2031   test 58.33% / 1.2072   lr 4.475e-02\n",
      "E110 | train 53.64% / 1.4013   val 56.66% / 1.2467   test 56.44% / 1.2559   lr 4.465e-02\n",
      "E111 | train 53.39% / 1.4021   val 57.17% / 1.2325   test 57.29% / 1.2311   lr 4.455e-02\n",
      "E112 | train 53.54% / 1.4024   val 57.66% / 1.2240   test 57.16% / 1.2360   lr 4.445e-02\n",
      "E113 | train 53.74% / 1.3955   val 59.73% / 1.1701   test 59.50% / 1.1743   lr 4.435e-02\n",
      "E114 | train 53.70% / 1.3995   val 59.31% / 1.1911   test 59.84% / 1.1869   lr 4.425e-02\n",
      "E115 | train 53.79% / 1.3964   val 59.48% / 1.1804   test 58.98% / 1.1920   lr 4.415e-02\n",
      "E116 | train 53.99% / 1.3971   val 58.79% / 1.1987   test 59.02% / 1.2029   lr 4.405e-02\n",
      "E117 | train 53.83% / 1.3996   val 58.38% / 1.2052   test 58.11% / 1.2163   lr 4.395e-02\n",
      "E118 | train 54.19% / 1.3911   val 57.71% / 1.2060   test 57.75% / 1.2125   lr 4.384e-02\n",
      "E119 | train 54.14% / 1.3862   val 58.05% / 1.2058   test 57.78% / 1.2131   lr 4.374e-02\n",
      "E120 | train 54.07% / 1.3900   val 59.40% / 1.1854   test 59.39% / 1.1880   lr 4.363e-02\n",
      "E121 | train 54.29% / 1.3841   val 57.57% / 1.2149   test 57.04% / 1.2233   lr 4.353e-02\n",
      "E122 | train 54.43% / 1.3792   val 57.16% / 1.2342   test 56.52% / 1.2526   lr 4.342e-02\n",
      "E123 | train 54.33% / 1.3831   val 58.99% / 1.1982   test 58.43% / 1.2016   lr 4.331e-02\n",
      "E124 | train 54.47% / 1.3804   val 59.26% / 1.1872   test 59.04% / 1.1923   lr 4.320e-02\n",
      "E125 | train 54.63% / 1.3772   val 60.94% / 1.1471   test 60.06% / 1.1661   lr 4.309e-02\n",
      "E126 | train 54.81% / 1.3784   val 59.10% / 1.1890   test 58.61% / 1.2026   lr 4.298e-02\n",
      "E127 | train 54.60% / 1.3776   val 59.72% / 1.1822   test 59.74% / 1.1880   lr 4.287e-02\n",
      "E128 | train 54.74% / 1.3760   val 59.46% / 1.1846   test 58.55% / 1.1951   lr 4.276e-02\n",
      "E129 | train 54.55% / 1.3791   val 59.61% / 1.1734   test 59.15% / 1.1859   lr 4.265e-02\n",
      "E130 | train 54.66% / 1.3753   val 59.62% / 1.1697   test 59.21% / 1.1765   lr 4.254e-02\n",
      "E131 | train 55.00% / 1.3676   val 60.34% / 1.1721   test 59.38% / 1.1835   lr 4.242e-02\n",
      "E132 | train 55.10% / 1.3692   val 58.76% / 1.1898   test 59.27% / 1.1846   lr 4.231e-02\n",
      "E133 | train 55.04% / 1.3682   val 60.59% / 1.1499   test 60.73% / 1.1554   lr 4.219e-02\n",
      "E134 | train 54.98% / 1.3690   val 60.56% / 1.1556   test 60.43% / 1.1641   lr 4.208e-02\n",
      "E135 | train 55.05% / 1.3660   val 57.16% / 1.2255   test 56.94% / 1.2383   lr 4.196e-02\n",
      "E136 | train 55.25% / 1.3629   val 59.33% / 1.1759   test 59.72% / 1.1802   lr 4.185e-02\n",
      "E137 | train 55.02% / 1.3658   val 61.03% / 1.1451   test 60.64% / 1.1585   lr 4.173e-02\n",
      "E138 | train 55.39% / 1.3588   val 60.72% / 1.1465   test 60.69% / 1.1419   lr 4.161e-02\n",
      "E139 | train 55.37% / 1.3582   val 61.37% / 1.1433   test 60.94% / 1.1495   lr 4.149e-02\n",
      "E140 | train 55.47% / 1.3533   val 59.95% / 1.1648   test 59.29% / 1.1762   lr 4.137e-02\n",
      "E141 | train 55.55% / 1.3577   val 59.83% / 1.1625   test 59.52% / 1.1667   lr 4.125e-02\n",
      "E142 | train 55.33% / 1.3582   val 60.99% / 1.1420   test 60.81% / 1.1488   lr 4.113e-02\n",
      "E143 | train 55.44% / 1.3581   val 60.60% / 1.1461   test 60.31% / 1.1518   lr 4.101e-02\n",
      "E144 | train 55.62% / 1.3527   val 59.60% / 1.1760   test 59.00% / 1.1931   lr 4.089e-02\n",
      "E145 | train 55.77% / 1.3519   val 58.07% / 1.2089   test 57.86% / 1.2125   lr 4.076e-02\n",
      "E146 | train 55.66% / 1.3483   val 60.40% / 1.1523   test 60.11% / 1.1653   lr 4.064e-02\n",
      "E147 | train 55.91% / 1.3470   val 60.38% / 1.1575   test 60.40% / 1.1571   lr 4.052e-02\n",
      "E148 | train 55.70% / 1.3516   val 60.92% / 1.1489   test 61.04% / 1.1478   lr 4.039e-02\n",
      "E149 | train 55.74% / 1.3507   val 59.66% / 1.1682   test 59.14% / 1.1817   lr 4.027e-02\n",
      "E150 | train 56.22% / 1.3375   val 61.04% / 1.1340   test 60.48% / 1.1419   lr 4.014e-02\n",
      "E151 | train 56.10% / 1.3424   val 59.24% / 1.1785   test 59.51% / 1.1803   lr 4.001e-02\n",
      "E152 | train 56.16% / 1.3375   val 61.47% / 1.1333   test 60.93% / 1.1429   lr 3.989e-02\n",
      "E153 | train 56.37% / 1.3378   val 60.52% / 1.1426   test 59.90% / 1.1630   lr 3.976e-02\n",
      "E154 | train 56.31% / 1.3361   val 60.91% / 1.1518   test 60.78% / 1.1547   lr 3.963e-02\n",
      "E155 | train 56.40% / 1.3344   val 60.01% / 1.1718   test 59.51% / 1.1776   lr 3.950e-02\n",
      "E156 | train 56.73% / 1.3291   val 61.84% / 1.1225   test 61.26% / 1.1212   lr 3.937e-02\n",
      "E157 | train 56.73% / 1.3310   val 61.38% / 1.1262   test 61.46% / 1.1310   lr 3.924e-02\n",
      "E158 | train 56.64% / 1.3318   val 62.90% / 1.1053   test 62.31% / 1.1139   lr 3.911e-02\n",
      "E159 | train 56.92% / 1.3210   val 62.38% / 1.1126   test 61.84% / 1.1194   lr 3.898e-02\n",
      "E160 | train 56.53% / 1.3294   val 61.32% / 1.1329   test 60.32% / 1.1390   lr 3.885e-02\n",
      "E161 | train 56.88% / 1.3233   val 58.76% / 1.1904   test 58.24% / 1.2039   lr 3.872e-02\n",
      "E162 | train 56.97% / 1.3218   val 61.32% / 1.1343   test 61.70% / 1.1328   lr 3.858e-02\n",
      "E163 | train 56.85% / 1.3217   val 61.49% / 1.1282   test 60.59% / 1.1398   lr 3.845e-02\n",
      "E164 | train 56.72% / 1.3227   val 62.54% / 1.1017   test 62.54% / 1.1088   lr 3.832e-02\n",
      "E165 | train 57.13% / 1.3168   val 59.28% / 1.1774   test 58.93% / 1.1823   lr 3.818e-02\n",
      "E166 | train 57.25% / 1.3118   val 62.92% / 1.0983   test 62.74% / 1.0978   lr 3.805e-02\n",
      "E167 | train 57.15% / 1.3119   val 61.50% / 1.1187   test 61.48% / 1.1232   lr 3.791e-02\n",
      "E168 | train 57.36% / 1.3091   val 61.24% / 1.1317   test 61.57% / 1.1326   lr 3.777e-02\n",
      "E169 | train 57.28% / 1.3145   val 60.56% / 1.1531   test 60.76% / 1.1446   lr 3.764e-02\n",
      "E170 | train 57.24% / 1.3114   val 62.72% / 1.1005   test 62.79% / 1.0970   lr 3.750e-02\n",
      "E171 | train 57.53% / 1.3064   val 63.76% / 1.0705   test 63.49% / 1.0710   lr 3.736e-02\n",
      "E172 | train 57.33% / 1.3104   val 62.34% / 1.1035   test 62.12% / 1.1083   lr 3.722e-02\n",
      "E173 | train 57.47% / 1.3038   val 62.29% / 1.1033   test 62.25% / 1.1141   lr 3.709e-02\n",
      "E174 | train 57.39% / 1.3096   val 62.16% / 1.1123   test 61.53% / 1.1211   lr 3.695e-02\n",
      "E175 | train 57.55% / 1.3008   val 62.13% / 1.1142   test 62.05% / 1.1088   lr 3.681e-02\n",
      "E176 | train 57.79% / 1.3005   val 62.71% / 1.0932   test 63.18% / 1.0948   lr 3.667e-02\n",
      "E177 | train 58.06% / 1.2950   val 63.74% / 1.0804   test 63.08% / 1.0852   lr 3.653e-02\n",
      "E178 | train 57.60% / 1.3002   val 61.82% / 1.1237   test 61.88% / 1.1221   lr 3.639e-02\n",
      "E179 | train 57.79% / 1.2982   val 63.46% / 1.0847   test 63.51% / 1.0865   lr 3.624e-02\n",
      "E180 | train 57.78% / 1.2965   val 62.26% / 1.1008   test 62.64% / 1.1006   lr 3.610e-02\n",
      "E181 | train 58.02% / 1.2906   val 62.53% / 1.1081   test 62.73% / 1.1039   lr 3.596e-02\n",
      "E182 | train 58.23% / 1.2861   val 62.12% / 1.1139   test 61.82% / 1.1173   lr 3.582e-02\n",
      "E183 | train 58.06% / 1.2912   val 63.02% / 1.0907   test 62.88% / 1.0921   lr 3.567e-02\n",
      "E184 | train 58.17% / 1.2863   val 63.99% / 1.0645   test 63.96% / 1.0622   lr 3.553e-02\n",
      "E185 | train 58.38% / 1.2840   val 62.48% / 1.1032   test 62.37% / 1.1094   lr 3.539e-02\n",
      "E186 | train 58.32% / 1.2860   val 63.19% / 1.0886   test 62.59% / 1.0964   lr 3.524e-02\n",
      "E187 | train 58.45% / 1.2816   val 63.34% / 1.0851   test 63.45% / 1.0872   lr 3.510e-02\n",
      "E188 | train 58.48% / 1.2808   val 63.26% / 1.0872   test 63.51% / 1.0860   lr 3.495e-02\n",
      "E189 | train 58.49% / 1.2815   val 62.65% / 1.1033   test 62.29% / 1.1041   lr 3.480e-02\n",
      "E190 | train 58.61% / 1.2793   val 63.39% / 1.0775   test 63.28% / 1.0769   lr 3.466e-02\n",
      "E191 | train 58.73% / 1.2741   val 62.17% / 1.1235   test 62.32% / 1.1197   lr 3.451e-02\n",
      "E192 | train 58.49% / 1.2793   val 63.04% / 1.0956   test 63.08% / 1.0883   lr 3.437e-02\n",
      "E193 | train 58.70% / 1.2741   val 62.41% / 1.1072   test 62.79% / 1.1002   lr 3.422e-02\n",
      "E194 | train 58.78% / 1.2712   val 64.64% / 1.0431   test 64.89% / 1.0399   lr 3.407e-02\n",
      "E195 | train 58.82% / 1.2744   val 63.65% / 1.0706   test 63.66% / 1.0626   lr 3.392e-02\n",
      "E196 | train 58.77% / 1.2690   val 63.33% / 1.0805   test 62.89% / 1.0799   lr 3.377e-02\n",
      "E197 | train 58.65% / 1.2733   val 63.54% / 1.0801   test 63.59% / 1.0760   lr 3.363e-02\n",
      "E198 | train 59.08% / 1.2662   val 63.29% / 1.0830   test 64.09% / 1.0717   lr 3.348e-02\n",
      "E199 | train 58.99% / 1.2676   val 64.47% / 1.0690   test 65.04% / 1.0534   lr 3.333e-02\n",
      "E200 | train 59.03% / 1.2675   val 65.19% / 1.0419   test 65.21% / 1.0376   lr 3.318e-02\n",
      "E201 | train 59.01% / 1.2664   val 63.28% / 1.0823   test 63.22% / 1.0791   lr 3.303e-02\n",
      "E202 | train 59.10% / 1.2639   val 65.64% / 1.0369   test 65.05% / 1.0426   lr 3.288e-02\n",
      "E203 | train 59.64% / 1.2517   val 62.94% / 1.0958   test 63.39% / 1.0901   lr 3.273e-02\n",
      "E204 | train 59.20% / 1.2597   val 64.79% / 1.0444   test 64.38% / 1.0454   lr 3.257e-02\n",
      "E205 | train 59.35% / 1.2569   val 64.54% / 1.0473   test 65.14% / 1.0392   lr 3.242e-02\n",
      "E206 | train 59.52% / 1.2558   val 64.98% / 1.0557   test 64.92% / 1.0514   lr 3.227e-02\n",
      "E207 | train 59.77% / 1.2481   val 64.96% / 1.0389   test 65.44% / 1.0278   lr 3.212e-02\n",
      "E208 | train 59.74% / 1.2487   val 65.19% / 1.0461   test 64.96% / 1.0434   lr 3.197e-02\n",
      "E209 | train 59.82% / 1.2451   val 64.51% / 1.0456   test 64.94% / 1.0421   lr 3.181e-02\n",
      "E210 | train 60.18% / 1.2413   val 65.88% / 1.0327   test 65.57% / 1.0254   lr 3.166e-02\n",
      "E211 | train 59.88% / 1.2443   val 64.18% / 1.0550   test 64.50% / 1.0444   lr 3.151e-02\n",
      "E212 | train 59.90% / 1.2436   val 66.00% / 1.0146   test 66.22% / 1.0161   lr 3.136e-02\n",
      "E213 | train 60.19% / 1.2341   val 65.86% / 1.0118   test 65.75% / 1.0104   lr 3.120e-02\n",
      "E214 | train 60.23% / 1.2369   val 65.53% / 1.0292   test 65.68% / 1.0214   lr 3.105e-02\n",
      "E215 | train 60.41% / 1.2319   val 65.29% / 1.0204   test 65.54% / 1.0175   lr 3.089e-02\n",
      "E216 | train 60.48% / 1.2301   val 64.53% / 1.0450   test 64.61% / 1.0432   lr 3.074e-02\n",
      "E217 | train 60.30% / 1.2369   val 66.43% / 1.0125   test 66.30% / 1.0051   lr 3.059e-02\n",
      "E218 | train 60.31% / 1.2277   val 64.51% / 1.0510   test 64.74% / 1.0458   lr 3.043e-02\n",
      "E219 | train 60.67% / 1.2249   val 65.76% / 1.0207   test 65.65% / 1.0130   lr 3.028e-02\n",
      "E220 | train 60.71% / 1.2272   val 64.86% / 1.0446   test 65.17% / 1.0401   lr 3.012e-02\n",
      "E221 | train 60.61% / 1.2239   val 65.77% / 1.0189   test 65.73% / 1.0136   lr 2.996e-02\n",
      "E222 | train 60.79% / 1.2210   val 65.92% / 1.0073   test 66.49% / 1.0023   lr 2.981e-02\n",
      "E223 | train 61.00% / 1.2184   val 66.78% / 0.9930   test 67.14% / 0.9888   lr 2.965e-02\n",
      "E224 | train 60.84% / 1.2166   val 65.68% / 1.0242   test 65.33% / 1.0183   lr 2.950e-02\n",
      "E225 | train 60.84% / 1.2214   val 66.55% / 1.0112   test 66.85% / 1.0064   lr 2.934e-02\n",
      "E226 | train 60.82% / 1.2210   val 66.65% / 1.0025   test 67.34% / 0.9945   lr 2.918e-02\n",
      "E227 | train 61.37% / 1.2093   val 64.84% / 1.0469   test 64.76% / 1.0479   lr 2.903e-02\n",
      "E228 | train 61.20% / 1.2095   val 65.67% / 1.0264   test 65.83% / 1.0198   lr 2.887e-02\n",
      "E229 | train 61.26% / 1.2115   val 64.39% / 1.0510   test 64.14% / 1.0605   lr 2.871e-02\n",
      "E230 | train 61.54% / 1.2050   val 66.08% / 1.0080   test 66.06% / 1.0091   lr 2.856e-02\n",
      "E231 | train 61.38% / 1.2022   val 66.11% / 1.0109   test 65.92% / 1.0119   lr 2.840e-02\n",
      "E232 | train 61.46% / 1.2054   val 66.77% / 1.0228   test 66.82% / 1.0195   lr 2.824e-02\n",
      "E233 | train 61.21% / 1.2089   val 65.79% / 1.0225   test 65.28% / 1.0288   lr 2.809e-02\n",
      "E234 | train 61.60% / 1.2009   val 66.09% / 1.0125   test 65.55% / 1.0153   lr 2.793e-02\n",
      "E235 | train 62.00% / 1.1915   val 66.17% / 1.0045   test 66.55% / 1.0019   lr 2.777e-02\n",
      "E236 | train 61.79% / 1.1938   val 65.69% / 1.0280   test 65.64% / 1.0264   lr 2.761e-02\n",
      "E237 | train 62.12% / 1.1918   val 65.05% / 1.0270   test 64.93% / 1.0348   lr 2.746e-02\n",
      "E238 | train 61.66% / 1.1916   val 66.03% / 1.0055   test 66.47% / 1.0018   lr 2.730e-02\n",
      "E239 | train 61.91% / 1.1921   val 67.27% / 0.9845   test 67.60% / 0.9761   lr 2.714e-02\n",
      "E240 | train 62.08% / 1.1855   val 67.27% / 0.9958   test 67.03% / 0.9998   lr 2.698e-02\n",
      "E241 | train 62.15% / 1.1859   val 67.33% / 0.9779   test 66.48% / 0.9936   lr 2.682e-02\n",
      "E242 | train 62.22% / 1.1822   val 67.35% / 0.9707   test 67.53% / 0.9711   lr 2.666e-02\n",
      "E243 | train 62.44% / 1.1771   val 67.41% / 0.9760   test 66.51% / 0.9819   lr 2.651e-02\n",
      "E244 | train 62.25% / 1.1790   val 68.16% / 0.9621   test 67.89% / 0.9644   lr 2.635e-02\n",
      "E245 | train 62.13% / 1.1826   val 66.51% / 1.0008   test 67.12% / 0.9893   lr 2.619e-02\n",
      "E246 | train 62.45% / 1.1755   val 67.38% / 0.9757   test 67.44% / 0.9733   lr 2.603e-02\n",
      "E247 | train 62.64% / 1.1708   val 67.20% / 0.9846   test 67.01% / 0.9801   lr 2.587e-02\n",
      "E248 | train 62.94% / 1.1682   val 65.92% / 1.0130   test 66.27% / 1.0087   lr 2.571e-02\n",
      "E249 | train 62.47% / 1.1745   val 67.38% / 0.9759   test 67.53% / 0.9761   lr 2.556e-02\n",
      "E250 | train 62.97% / 1.1641   val 66.47% / 0.9932   test 66.70% / 0.9876   lr 2.540e-02\n",
      "E251 | train 62.76% / 1.1676   val 68.00% / 0.9788   test 67.91% / 0.9736   lr 2.524e-02\n",
      "E252 | train 63.37% / 1.1573   val 68.44% / 0.9500   test 68.34% / 0.9481   lr 2.508e-02\n",
      "E253 | train 63.19% / 1.1639   val 68.59% / 0.9472   test 68.73% / 0.9396   lr 2.492e-02\n",
      "E254 | train 63.57% / 1.1516   val 68.64% / 0.9577   test 68.24% / 0.9604   lr 2.476e-02\n",
      "E255 | train 63.48% / 1.1485   val 68.74% / 0.9445   test 69.17% / 0.9435   lr 2.460e-02\n",
      "E256 | train 63.42% / 1.1527   val 68.08% / 0.9652   test 67.96% / 0.9652   lr 2.444e-02\n",
      "E257 | train 63.37% / 1.1507   val 68.97% / 0.9386   test 69.14% / 0.9364   lr 2.429e-02\n",
      "E258 | train 63.55% / 1.1502   val 68.73% / 0.9464   test 68.62% / 0.9455   lr 2.413e-02\n",
      "E259 | train 63.65% / 1.1475   val 68.43% / 0.9468   test 68.21% / 0.9505   lr 2.397e-02\n",
      "E260 | train 63.50% / 1.1497   val 68.17% / 0.9592   test 68.33% / 0.9476   lr 2.381e-02\n",
      "E261 | train 63.46% / 1.1476   val 69.39% / 0.9357   test 69.12% / 0.9366   lr 2.365e-02\n",
      "E262 | train 63.83% / 1.1422   val 68.48% / 0.9503   test 67.99% / 0.9612   lr 2.349e-02\n",
      "E263 | train 63.99% / 1.1380   val 67.32% / 0.9754   test 67.49% / 0.9724   lr 2.334e-02\n",
      "E264 | train 63.80% / 1.1425   val 68.69% / 0.9356   test 68.69% / 0.9356   lr 2.318e-02\n",
      "E265 | train 64.30% / 1.1322   val 69.09% / 0.9303   test 68.97% / 0.9364   lr 2.302e-02\n",
      "E266 | train 63.79% / 1.1388   val 68.88% / 0.9435   test 68.70% / 0.9399   lr 2.286e-02\n",
      "E267 | train 63.95% / 1.1392   val 69.71% / 0.9232   test 69.46% / 0.9291   lr 2.270e-02\n",
      "E268 | train 64.42% / 1.1257   val 68.66% / 0.9482   test 68.38% / 0.9568   lr 2.254e-02\n",
      "E269 | train 64.38% / 1.1225   val 68.46% / 0.9483   test 68.04% / 0.9461   lr 2.239e-02\n",
      "E270 | train 64.48% / 1.1232   val 68.26% / 0.9516   test 68.86% / 0.9439   lr 2.223e-02\n",
      "E271 | train 64.49% / 1.1231   val 69.26% / 0.9358   test 68.83% / 0.9361   lr 2.207e-02\n",
      "E272 | train 64.79% / 1.1137   val 69.56% / 0.9200   test 69.79% / 0.9207   lr 2.191e-02\n",
      "E273 | train 64.73% / 1.1146   val 68.99% / 0.9280   test 68.81% / 0.9331   lr 2.176e-02\n",
      "E274 | train 64.49% / 1.1231   val 68.73% / 0.9390   test 68.48% / 0.9475   lr 2.160e-02\n",
      "E275 | train 64.75% / 1.1162   val 69.10% / 0.9256   test 69.56% / 0.9250   lr 2.144e-02\n",
      "E276 | train 64.88% / 1.1098   val 70.09% / 0.9069   test 70.01% / 0.9098   lr 2.129e-02\n",
      "E277 | train 65.06% / 1.1065   val 69.46% / 0.9293   test 69.34% / 0.9372   lr 2.113e-02\n",
      "E278 | train 65.00% / 1.1094   val 69.43% / 0.9277   test 68.84% / 0.9258   lr 2.097e-02\n",
      "E279 | train 65.22% / 1.1030   val 70.69% / 0.8980   test 70.28% / 0.8968   lr 2.082e-02\n",
      "E280 | train 65.31% / 1.0996   val 70.71% / 0.8964   test 70.35% / 0.8937   lr 2.066e-02\n",
      "E281 | train 65.30% / 1.1004   val 69.66% / 0.9128   test 69.53% / 0.9192   lr 2.050e-02\n",
      "E282 | train 65.24% / 1.1045   val 69.89% / 0.9094   test 69.76% / 0.9109   lr 2.035e-02\n",
      "E283 | train 65.48% / 1.0975   val 70.76% / 0.9004   test 70.30% / 0.9023   lr 2.019e-02\n",
      "E284 | train 65.48% / 1.0948   val 70.19% / 0.9083   test 70.28% / 0.9073   lr 2.004e-02\n",
      "E285 | train 65.74% / 1.0909   val 71.19% / 0.8821   test 71.76% / 0.8717   lr 1.988e-02\n",
      "E286 | train 65.70% / 1.0904   val 70.26% / 0.9094   test 70.50% / 0.9070   lr 1.972e-02\n",
      "E287 | train 66.08% / 1.0847   val 70.73% / 0.8996   test 70.15% / 0.9081   lr 1.957e-02\n",
      "E288 | train 65.72% / 1.0874   val 71.22% / 0.8823   test 71.15% / 0.8782   lr 1.941e-02\n",
      "E289 | train 65.90% / 1.0841   val 70.63% / 0.8963   test 70.94% / 0.8946   lr 1.926e-02\n",
      "E290 | train 66.09% / 1.0782   val 71.44% / 0.8658   test 71.96% / 0.8652   lr 1.911e-02\n",
      "E291 | train 66.45% / 1.0748   val 69.41% / 0.9151   test 69.49% / 0.9220   lr 1.895e-02\n",
      "E292 | train 66.13% / 1.0790   val 71.16% / 0.8839   test 71.61% / 0.8831   lr 1.880e-02\n",
      "E293 | train 66.55% / 1.0688   val 69.43% / 0.9272   test 69.59% / 0.9249   lr 1.864e-02\n",
      "E294 | train 66.22% / 1.0745   val 70.28% / 0.9072   test 69.99% / 0.9071   lr 1.849e-02\n",
      "E295 | train 66.48% / 1.0627   val 70.17% / 0.8999   test 70.73% / 0.8902   lr 1.834e-02\n",
      "E296 | train 66.37% / 1.0688   val 70.74% / 0.8808   test 70.86% / 0.8842   lr 1.819e-02\n",
      "E297 | train 66.29% / 1.0701   val 70.89% / 0.8891   test 70.85% / 0.8906   lr 1.803e-02\n",
      "E298 | train 66.58% / 1.0637   val 71.00% / 0.8766   test 71.39% / 0.8689   lr 1.788e-02\n",
      "E299 | train 66.93% / 1.0552   val 71.61% / 0.8697   test 71.36% / 0.8740   lr 1.773e-02\n",
      "E300 | train 66.77% / 1.0598   val 70.91% / 0.8677   test 70.96% / 0.8787   lr 1.758e-02\n",
      "E301 | train 66.93% / 1.0558   val 70.94% / 0.8700   test 71.38% / 0.8678   lr 1.743e-02\n",
      "E302 | train 67.11% / 1.0518   val 72.05% / 0.8730   test 71.67% / 0.8688   lr 1.727e-02\n",
      "E303 | train 66.95% / 1.0512   val 71.73% / 0.8623   test 71.82% / 0.8636   lr 1.712e-02\n",
      "E304 | train 67.22% / 1.0477   val 72.20% / 0.8486   test 72.16% / 0.8497   lr 1.697e-02\n",
      "E305 | train 67.20% / 1.0478   val 72.81% / 0.8410   test 72.47% / 0.8441   lr 1.682e-02\n",
      "E306 | train 67.46% / 1.0413   val 71.91% / 0.8588   test 71.68% / 0.8645   lr 1.667e-02\n",
      "E307 | train 67.54% / 1.0402   val 72.36% / 0.8487   test 72.34% / 0.8608   lr 1.652e-02\n",
      "E308 | train 67.54% / 1.0404   val 71.18% / 0.8750   test 71.23% / 0.8787   lr 1.637e-02\n",
      "E309 | train 67.85% / 1.0338   val 72.14% / 0.8554   test 71.98% / 0.8539   lr 1.623e-02\n",
      "E310 | train 67.81% / 1.0316   val 72.63% / 0.8432   test 72.38% / 0.8479   lr 1.608e-02\n",
      "E311 | train 68.05% / 1.0279   val 72.47% / 0.8409   test 72.29% / 0.8425   lr 1.593e-02\n",
      "E312 | train 67.83% / 1.0303   val 71.75% / 0.8606   test 71.76% / 0.8670   lr 1.578e-02\n",
      "E313 | train 68.06% / 1.0283   val 72.59% / 0.8389   test 72.95% / 0.8329   lr 1.563e-02\n",
      "E314 | train 68.44% / 1.0203   val 72.76% / 0.8505   test 72.73% / 0.8496   lr 1.549e-02\n",
      "E315 | train 68.50% / 1.0200   val 72.31% / 0.8564   test 72.47% / 0.8518   lr 1.534e-02\n",
      "E316 | train 68.48% / 1.0147   val 72.44% / 0.8468   test 73.11% / 0.8416   lr 1.520e-02\n",
      "E317 | train 68.57% / 1.0115   val 73.46% / 0.8260   test 72.97% / 0.8307   lr 1.505e-02\n",
      "E318 | train 68.38% / 1.0156   val 72.62% / 0.8338   test 73.03% / 0.8341   lr 1.490e-02\n",
      "E319 | train 68.40% / 1.0151   val 72.58% / 0.8539   test 72.22% / 0.8572   lr 1.476e-02\n",
      "E320 | train 68.93% / 1.0036   val 72.67% / 0.8407   test 72.60% / 0.8469   lr 1.461e-02\n",
      "E321 | train 68.89% / 1.0041   val 73.86% / 0.8141   test 73.46% / 0.8194   lr 1.447e-02\n",
      "E322 | train 69.02% / 0.9989   val 72.66% / 0.8354   test 73.05% / 0.8287   lr 1.433e-02\n",
      "E323 | train 68.89% / 1.0048   val 74.03% / 0.8158   test 73.86% / 0.8127   lr 1.418e-02\n",
      "E324 | train 69.03% / 0.9949   val 72.65% / 0.8403   test 72.84% / 0.8362   lr 1.404e-02\n",
      "E325 | train 69.23% / 0.9935   val 73.59% / 0.8191   test 73.25% / 0.8234   lr 1.390e-02\n",
      "E326 | train 69.11% / 0.9966   val 73.18% / 0.8334   test 73.42% / 0.8306   lr 1.376e-02\n",
      "E327 | train 69.16% / 0.9933   val 73.32% / 0.8181   test 73.66% / 0.8162   lr 1.361e-02\n",
      "E328 | train 69.27% / 0.9916   val 73.98% / 0.8069   test 73.66% / 0.8096   lr 1.347e-02\n",
      "E329 | train 69.54% / 0.9849   val 73.74% / 0.8156   test 73.69% / 0.8173   lr 1.333e-02\n",
      "E330 | train 69.67% / 0.9835   val 73.61% / 0.8106   test 73.52% / 0.8118   lr 1.319e-02\n",
      "E331 | train 69.77% / 0.9789   val 73.47% / 0.8166   test 73.78% / 0.8110   lr 1.305e-02\n",
      "E332 | train 69.87% / 0.9766   val 73.96% / 0.8009   test 73.58% / 0.8057   lr 1.291e-02\n",
      "E333 | train 69.93% / 0.9738   val 74.22% / 0.7997   test 74.63% / 0.7919   lr 1.278e-02\n",
      "E334 | train 70.15% / 0.9693   val 74.88% / 0.7888   test 75.09% / 0.7864   lr 1.264e-02\n",
      "E335 | train 70.14% / 0.9679   val 74.04% / 0.8011   test 74.36% / 0.7981   lr 1.250e-02\n",
      "E336 | train 70.23% / 0.9656   val 74.79% / 0.7839   test 75.00% / 0.7829   lr 1.236e-02\n",
      "E337 | train 70.30% / 0.9626   val 74.01% / 0.8143   test 74.00% / 0.8017   lr 1.223e-02\n",
      "E338 | train 70.47% / 0.9592   val 74.21% / 0.7855   test 74.54% / 0.7807   lr 1.209e-02\n",
      "E339 | train 70.67% / 0.9527   val 73.70% / 0.8107   test 73.96% / 0.8021   lr 1.195e-02\n",
      "E340 | train 70.62% / 0.9553   val 74.97% / 0.7887   test 74.16% / 0.7979   lr 1.182e-02\n",
      "E341 | train 70.77% / 0.9464   val 73.21% / 0.8271   test 73.14% / 0.8255   lr 1.168e-02\n",
      "E342 | train 70.50% / 0.9563   val 75.06% / 0.7752   test 74.45% / 0.7777   lr 1.155e-02\n",
      "E343 | train 71.06% / 0.9433   val 74.98% / 0.7835   test 74.65% / 0.7857   lr 1.142e-02\n",
      "E344 | train 70.85% / 0.9454   val 74.50% / 0.7908   test 74.49% / 0.7895   lr 1.128e-02\n",
      "E345 | train 71.54% / 0.9351   val 75.10% / 0.7787   test 74.90% / 0.7774   lr 1.115e-02\n",
      "E346 | train 71.27% / 0.9344   val 74.60% / 0.7967   test 74.60% / 0.7978   lr 1.102e-02\n",
      "E347 | train 71.06% / 0.9406   val 74.79% / 0.7886   test 74.83% / 0.7866   lr 1.089e-02\n",
      "E348 | train 71.45% / 0.9343   val 75.70% / 0.7596   test 75.71% / 0.7570   lr 1.076e-02\n",
      "E349 | train 71.66% / 0.9263   val 75.50% / 0.7705   test 75.79% / 0.7769   lr 1.063e-02\n",
      "E350 | train 71.61% / 0.9285   val 75.99% / 0.7563   test 75.76% / 0.7654   lr 1.050e-02\n",
      "E351 | train 71.66% / 0.9222   val 75.71% / 0.7636   test 75.46% / 0.7660   lr 1.037e-02\n",
      "E352 | train 71.77% / 0.9202   val 75.88% / 0.7593   test 75.59% / 0.7589   lr 1.024e-02\n",
      "E353 | train 71.84% / 0.9212   val 75.82% / 0.7639   test 75.70% / 0.7669   lr 1.011e-02\n",
      "E354 | train 71.82% / 0.9151   val 76.08% / 0.7503   test 75.77% / 0.7559   lr 9.986e-03\n",
      "E355 | train 72.19% / 0.9123   val 75.91% / 0.7567   test 75.36% / 0.7632   lr 9.860e-03\n",
      "E356 | train 72.04% / 0.9143   val 75.99% / 0.7571   test 75.58% / 0.7607   lr 9.734e-03\n",
      "E357 | train 72.40% / 0.9055   val 76.22% / 0.7427   test 76.22% / 0.7460   lr 9.608e-03\n",
      "E358 | train 72.28% / 0.9062   val 75.70% / 0.7636   test 76.23% / 0.7550   lr 9.484e-03\n",
      "E359 | train 72.60% / 0.9052   val 76.78% / 0.7400   test 76.53% / 0.7438   lr 9.360e-03\n",
      "E360 | train 72.47% / 0.9009   val 76.19% / 0.7395   test 77.06% / 0.7266   lr 9.236e-03\n",
      "E361 | train 72.65% / 0.8991   val 76.58% / 0.7466   test 76.25% / 0.7472   lr 9.113e-03\n",
      "E362 | train 72.81% / 0.8921   val 75.97% / 0.7474   test 75.96% / 0.7439   lr 8.991e-03\n",
      "E363 | train 73.13% / 0.8870   val 76.22% / 0.7472   test 76.18% / 0.7492   lr 8.870e-03\n",
      "E364 | train 73.34% / 0.8825   val 76.91% / 0.7453   test 76.47% / 0.7477   lr 8.749e-03\n",
      "E365 | train 73.01% / 0.8857   val 76.79% / 0.7284   test 76.68% / 0.7328   lr 8.628e-03\n",
      "E366 | train 73.28% / 0.8815   val 77.12% / 0.7214   test 77.47% / 0.7194   lr 8.509e-03\n",
      "E367 | train 73.51% / 0.8750   val 76.38% / 0.7395   test 76.76% / 0.7357   lr 8.390e-03\n",
      "E368 | train 73.71% / 0.8702   val 77.48% / 0.7092   test 77.29% / 0.7149   lr 8.272e-03\n",
      "E369 | train 73.45% / 0.8743   val 76.92% / 0.7215   test 76.88% / 0.7280   lr 8.154e-03\n",
      "E370 | train 73.61% / 0.8684   val 77.22% / 0.7184   test 77.28% / 0.7127   lr 8.037e-03\n",
      "E371 | train 73.70% / 0.8663   val 77.51% / 0.7118   test 77.29% / 0.7190   lr 7.921e-03\n",
      "E372 | train 73.86% / 0.8641   val 77.43% / 0.7151   test 77.28% / 0.7220   lr 7.806e-03\n",
      "E373 | train 74.08% / 0.8579   val 77.28% / 0.7219   test 76.74% / 0.7296   lr 7.691e-03\n",
      "E374 | train 73.92% / 0.8597   val 77.61% / 0.7096   test 77.14% / 0.7125   lr 7.577e-03\n",
      "E375 | train 74.08% / 0.8606   val 77.59% / 0.7185   test 77.71% / 0.7154   lr 7.463e-03\n",
      "E376 | train 74.31% / 0.8501   val 77.28% / 0.7198   test 77.24% / 0.7200   lr 7.350e-03\n",
      "E377 | train 74.36% / 0.8515   val 77.14% / 0.7203   test 76.99% / 0.7175   lr 7.238e-03\n",
      "E378 | train 74.52% / 0.8430   val 78.04% / 0.7005   test 77.74% / 0.7049   lr 7.127e-03\n",
      "E379 | train 74.80% / 0.8403   val 78.40% / 0.6961   test 78.04% / 0.6971   lr 7.017e-03\n",
      "E380 | train 74.83% / 0.8393   val 78.01% / 0.7021   test 77.69% / 0.7114   lr 6.907e-03\n",
      "E381 | train 74.92% / 0.8335   val 78.22% / 0.6904   test 77.90% / 0.7026   lr 6.798e-03\n",
      "E382 | train 74.85% / 0.8348   val 77.04% / 0.7225   test 77.35% / 0.7191   lr 6.689e-03\n",
      "E383 | train 75.06% / 0.8336   val 78.04% / 0.7018   test 78.14% / 0.7014   lr 6.581e-03\n",
      "E384 | train 75.16% / 0.8285   val 78.33% / 0.6900   test 78.09% / 0.6902   lr 6.475e-03\n",
      "E385 | train 75.46% / 0.8206   val 78.22% / 0.7018   test 77.89% / 0.7049   lr 6.368e-03\n",
      "E386 | train 75.51% / 0.8196   val 78.70% / 0.6801   test 78.09% / 0.6926   lr 6.263e-03\n",
      "E387 | train 75.62% / 0.8160   val 78.36% / 0.6907   test 78.36% / 0.6891   lr 6.158e-03\n",
      "E388 | train 75.62% / 0.8130   val 77.63% / 0.7105   test 77.77% / 0.7113   lr 6.054e-03\n",
      "E389 | train 75.69% / 0.8130   val 79.03% / 0.6850   test 78.27% / 0.6943   lr 5.951e-03\n",
      "E390 | train 75.90% / 0.8041   val 79.06% / 0.6849   test 78.99% / 0.6832   lr 5.849e-03\n",
      "E391 | train 75.92% / 0.8073   val 79.19% / 0.6785   test 78.75% / 0.6864   lr 5.747e-03\n",
      "E392 | train 75.85% / 0.8071   val 78.92% / 0.6799   test 78.77% / 0.6821   lr 5.646e-03\n",
      "E393 | train 75.99% / 0.8012   val 79.22% / 0.6783   test 79.14% / 0.6767   lr 5.546e-03\n",
      "E394 | train 76.06% / 0.7965   val 79.20% / 0.6780   test 78.54% / 0.6832   lr 5.447e-03\n",
      "E395 | train 76.02% / 0.8013   val 79.16% / 0.6727   test 78.94% / 0.6739   lr 5.349e-03\n",
      "E396 | train 76.42% / 0.7910   val 79.06% / 0.6711   test 79.01% / 0.6756   lr 5.251e-03\n",
      "E397 | train 76.44% / 0.7905   val 79.72% / 0.6639   test 79.16% / 0.6763   lr 5.154e-03\n",
      "E398 | train 76.84% / 0.7822   val 79.24% / 0.6715   test 79.19% / 0.6793   lr 5.058e-03\n",
      "E399 | train 76.49% / 0.7875   val 79.08% / 0.6720   test 78.92% / 0.6732   lr 4.963e-03\n",
      "E400 | train 76.94% / 0.7797   val 79.19% / 0.6689   test 78.68% / 0.6773   lr 4.868e-03\n",
      "E401 | train 76.84% / 0.7794   val 79.64% / 0.6598   test 79.72% / 0.6599   lr 4.775e-03\n",
      "E402 | train 76.86% / 0.7736   val 79.41% / 0.6577   test 79.52% / 0.6640   lr 4.682e-03\n",
      "E403 | train 76.96% / 0.7720   val 79.80% / 0.6576   test 79.56% / 0.6661   lr 4.590e-03\n",
      "E404 | train 77.45% / 0.7624   val 79.59% / 0.6500   test 79.19% / 0.6621   lr 4.498e-03\n",
      "E405 | train 77.22% / 0.7664   val 79.90% / 0.6547   test 79.34% / 0.6619   lr 4.408e-03\n",
      "E406 | train 77.55% / 0.7604   val 80.22% / 0.6503   test 79.73% / 0.6595   lr 4.319e-03\n",
      "E407 | train 77.44% / 0.7579   val 79.95% / 0.6457   test 79.97% / 0.6537   lr 4.230e-03\n",
      "E408 | train 77.34% / 0.7589   val 80.08% / 0.6370   test 80.61% / 0.6372   lr 4.142e-03\n",
      "E409 | train 77.94% / 0.7484   val 80.35% / 0.6379   test 80.36% / 0.6453   lr 4.055e-03\n",
      "E410 | train 78.00% / 0.7461   val 80.22% / 0.6480   test 79.56% / 0.6565   lr 3.969e-03\n",
      "E411 | train 78.02% / 0.7434   val 80.40% / 0.6412   test 80.19% / 0.6442   lr 3.883e-03\n",
      "E412 | train 77.90% / 0.7465   val 80.04% / 0.6400   test 80.12% / 0.6471   lr 3.799e-03\n",
      "E413 | train 78.12% / 0.7418   val 80.50% / 0.6309   test 80.57% / 0.6361   lr 3.715e-03\n",
      "E414 | train 78.20% / 0.7362   val 80.51% / 0.6338   test 80.00% / 0.6440   lr 3.632e-03\n",
      "E415 | train 78.22% / 0.7340   val 80.60% / 0.6367   test 80.21% / 0.6447   lr 3.550e-03\n",
      "E416 | train 78.65% / 0.7265   val 80.79% / 0.6315   test 80.59% / 0.6388   lr 3.469e-03\n",
      "E417 | train 78.39% / 0.7366   val 80.92% / 0.6296   test 80.28% / 0.6403   lr 3.389e-03\n",
      "E418 | train 78.71% / 0.7248   val 80.69% / 0.6356   test 80.80% / 0.6409   lr 3.310e-03\n",
      "E419 | train 78.70% / 0.7235   val 80.35% / 0.6425   test 79.93% / 0.6488   lr 3.231e-03\n",
      "E420 | train 78.70% / 0.7244   val 80.79% / 0.6294   test 80.76% / 0.6309   lr 3.154e-03\n",
      "E421 | train 78.98% / 0.7165   val 80.98% / 0.6261   test 80.92% / 0.6291   lr 3.077e-03\n",
      "E422 | train 79.08% / 0.7125   val 80.70% / 0.6295   test 80.60% / 0.6341   lr 3.001e-03\n",
      "E423 | train 79.25% / 0.7081   val 80.93% / 0.6246   test 81.09% / 0.6257   lr 2.926e-03\n",
      "E424 | train 79.07% / 0.7129   val 81.07% / 0.6212   test 80.90% / 0.6307   lr 2.852e-03\n",
      "E425 | train 79.33% / 0.7054   val 81.49% / 0.6243   test 80.82% / 0.6307   lr 2.779e-03\n",
      "E426 | train 79.43% / 0.7010   val 81.36% / 0.6222   test 81.19% / 0.6310   lr 2.707e-03\n",
      "E427 | train 79.58% / 0.6982   val 81.28% / 0.6137   test 81.10% / 0.6184   lr 2.635e-03\n",
      "E428 | train 79.72% / 0.6937   val 80.96% / 0.6199   test 81.10% / 0.6199   lr 2.565e-03\n",
      "E429 | train 79.63% / 0.6969   val 81.17% / 0.6209   test 81.31% / 0.6222   lr 2.495e-03\n",
      "E430 | train 79.85% / 0.6898   val 81.56% / 0.6087   test 81.21% / 0.6166   lr 2.427e-03\n",
      "E431 | train 80.16% / 0.6855   val 81.69% / 0.6121   test 81.17% / 0.6181   lr 2.359e-03\n",
      "E432 | train 80.12% / 0.6871   val 81.76% / 0.6060   test 81.27% / 0.6217   lr 2.292e-03\n",
      "E433 | train 80.09% / 0.6834   val 81.69% / 0.6010   test 81.38% / 0.6074   lr 2.226e-03\n",
      "E434 | train 80.17% / 0.6810   val 81.89% / 0.6085   test 81.21% / 0.6196   lr 2.161e-03\n",
      "E435 | train 80.15% / 0.6837   val 81.96% / 0.5998   test 81.33% / 0.6094   lr 2.097e-03\n",
      "E436 | train 80.07% / 0.6805   val 81.67% / 0.6048   test 81.52% / 0.6128   lr 2.034e-03\n",
      "E437 | train 80.27% / 0.6748   val 81.80% / 0.5932   test 81.81% / 0.6061   lr 1.972e-03\n",
      "E438 | train 80.44% / 0.6720   val 81.84% / 0.6030   test 81.47% / 0.6176   lr 1.911e-03\n",
      "E439 | train 80.61% / 0.6689   val 81.71% / 0.6117   test 81.55% / 0.6156   lr 1.850e-03\n",
      "E440 | train 80.63% / 0.6674   val 81.73% / 0.6039   test 81.58% / 0.6134   lr 1.791e-03\n",
      "E441 | train 80.58% / 0.6656   val 82.11% / 0.5984   test 81.56% / 0.6096   lr 1.732e-03\n",
      "E442 | train 80.78% / 0.6610   val 82.04% / 0.5994   test 81.84% / 0.6059   lr 1.675e-03\n",
      "E443 | train 80.66% / 0.6608   val 82.41% / 0.5907   test 81.81% / 0.6018   lr 1.618e-03\n",
      "E444 | train 80.91% / 0.6565   val 82.41% / 0.5929   test 81.62% / 0.6050   lr 1.562e-03\n",
      "E445 | train 81.04% / 0.6524   val 81.97% / 0.6012   test 81.71% / 0.6118   lr 1.508e-03\n",
      "E446 | train 81.02% / 0.6546   val 82.23% / 0.5964   test 81.56% / 0.6051   lr 1.454e-03\n",
      "E447 | train 81.22% / 0.6474   val 82.86% / 0.5801   test 81.88% / 0.5944   lr 1.401e-03\n",
      "E448 | train 81.32% / 0.6454   val 82.53% / 0.5888   test 81.97% / 0.5944   lr 1.349e-03\n",
      "E449 | train 81.32% / 0.6436   val 82.84% / 0.5841   test 82.22% / 0.5954   lr 1.298e-03\n",
      "E450 | train 81.34% / 0.6429   val 82.34% / 0.5893   test 82.12% / 0.5960   lr 1.248e-03\n",
      "E451 | train 81.46% / 0.6416   val 82.81% / 0.5893   test 82.29% / 0.5962   lr 1.199e-03\n",
      "E452 | train 81.44% / 0.6448   val 82.77% / 0.5824   test 82.41% / 0.5903   lr 1.151e-03\n",
      "E453 | train 81.53% / 0.6399   val 82.75% / 0.5826   test 82.14% / 0.5902   lr 1.104e-03\n",
      "E454 | train 81.43% / 0.6396   val 82.78% / 0.5797   test 82.23% / 0.5902   lr 1.058e-03\n",
      "E455 | train 81.57% / 0.6353   val 82.80% / 0.5817   test 82.36% / 0.5949   lr 1.013e-03\n",
      "E456 | train 81.67% / 0.6339   val 83.10% / 0.5797   test 82.31% / 0.5945   lr 9.685e-04\n",
      "E457 | train 81.75% / 0.6301   val 83.34% / 0.5747   test 82.36% / 0.5906   lr 9.252e-04\n",
      "E458 | train 81.95% / 0.6325   val 82.79% / 0.5820   test 82.33% / 0.5942   lr 8.829e-04\n",
      "E459 | train 81.83% / 0.6291   val 83.11% / 0.5747   test 82.21% / 0.5903   lr 8.416e-04\n",
      "E460 | train 81.98% / 0.6266   val 83.03% / 0.5800   test 82.73% / 0.5906   lr 8.013e-04\n",
      "E461 | train 81.77% / 0.6293   val 82.99% / 0.5798   test 82.24% / 0.5943   lr 7.619e-04\n",
      "E462 | train 81.97% / 0.6253   val 83.09% / 0.5726   test 82.39% / 0.5846   lr 7.235e-04\n",
      "E463 | train 81.97% / 0.6248   val 82.99% / 0.5759   test 82.24% / 0.5887   lr 6.861e-04\n",
      "E464 | train 82.10% / 0.6194   val 82.91% / 0.5798   test 82.67% / 0.5909   lr 6.497e-04\n",
      "E465 | train 82.18% / 0.6187   val 83.16% / 0.5732   test 82.31% / 0.5886   lr 6.143e-04\n",
      "E466 | train 82.06% / 0.6191   val 83.00% / 0.5760   test 82.58% / 0.5884   lr 5.798e-04\n",
      "E467 | train 82.39% / 0.6123   val 83.16% / 0.5718   test 82.50% / 0.5854   lr 5.463e-04\n",
      "E468 | train 82.36% / 0.6116   val 83.35% / 0.5700   test 82.47% / 0.5848   lr 5.138e-04\n",
      "E469 | train 82.28% / 0.6137   val 83.27% / 0.5728   test 82.38% / 0.5866   lr 4.823e-04\n",
      "E470 | train 82.39% / 0.6122   val 83.06% / 0.5737   test 82.41% / 0.5874   lr 4.518e-04\n",
      "E471 | train 82.40% / 0.6135   val 83.34% / 0.5782   test 82.52% / 0.5924   lr 4.222e-04\n",
      "E472 | train 82.54% / 0.6075   val 83.08% / 0.5737   test 82.34% / 0.5883   lr 3.937e-04\n",
      "E473 | train 82.46% / 0.6084   val 83.19% / 0.5745   test 82.41% / 0.5888   lr 3.662e-04\n",
      "E474 | train 82.67% / 0.6056   val 83.19% / 0.5712   test 82.62% / 0.5831   lr 3.396e-04\n",
      "E475 | train 82.58% / 0.6047   val 83.12% / 0.5744   test 82.70% / 0.5859   lr 3.140e-04\n",
      "E476 | train 82.55% / 0.6063   val 83.24% / 0.5697   test 82.61% / 0.5850   lr 2.895e-04\n",
      "E477 | train 82.83% / 0.5991   val 83.26% / 0.5693   test 82.71% / 0.5827   lr 2.659e-04\n",
      "E478 | train 82.62% / 0.6041   val 83.11% / 0.5793   test 82.39% / 0.5926   lr 2.433e-04\n",
      "E479 | train 82.56% / 0.6036   val 83.41% / 0.5644   test 82.88% / 0.5790   lr 2.217e-04\n",
      "E480 | train 82.71% / 0.6014   val 83.36% / 0.5678   test 82.78% / 0.5813   lr 2.011e-04\n",
      "E481 | train 82.68% / 0.6012   val 83.47% / 0.5639   test 82.94% / 0.5781   lr 1.815e-04\n",
      "E482 | train 82.83% / 0.5986   val 83.27% / 0.5661   test 82.86% / 0.5804   lr 1.630e-04\n",
      "E483 | train 82.73% / 0.5998   val 83.58% / 0.5632   test 82.79% / 0.5782   lr 1.454e-04\n",
      "E484 | train 82.69% / 0.5979   val 83.50% / 0.5629   test 82.89% / 0.5774   lr 1.288e-04\n",
      "E485 | train 82.92% / 0.5941   val 83.64% / 0.5631   test 82.81% / 0.5787   lr 1.132e-04\n",
      "E486 | train 82.78% / 0.5955   val 83.38% / 0.5692   test 82.83% / 0.5855   lr 9.862e-05\n",
      "E487 | train 82.94% / 0.5941   val 83.61% / 0.5652   test 82.96% / 0.5792   lr 8.504e-05\n",
      "E488 | train 82.95% / 0.5941   val 83.48% / 0.5686   test 82.66% / 0.5842   lr 7.247e-05\n",
      "E489 | train 82.96% / 0.5950   val 83.49% / 0.5625   test 83.02% / 0.5771   lr 6.090e-05\n",
      "E490 | train 82.88% / 0.5974   val 83.51% / 0.5648   test 83.14% / 0.5777   lr 5.033e-05\n",
      "E491 | train 82.84% / 0.5962   val 83.56% / 0.5623   test 82.94% / 0.5755   lr 4.077e-05\n",
      "E492 | train 83.04% / 0.5909   val 83.41% / 0.5672   test 82.76% / 0.5831   lr 3.222e-05\n",
      "E493 | train 82.93% / 0.5936   val 83.28% / 0.5639   test 82.95% / 0.5775   lr 2.467e-05\n",
      "E494 | train 82.89% / 0.5935   val 83.21% / 0.5735   test 82.67% / 0.5878   lr 1.812e-05\n",
      "E495 | train 82.91% / 0.5904   val 83.65% / 0.5621   test 83.16% / 0.5761   lr 1.259e-05\n",
      "E496 | train 82.80% / 0.5974   val 83.51% / 0.5639   test 82.84% / 0.5784   lr 8.056e-06\n",
      "E497 | train 83.02% / 0.5923   val 83.56% / 0.5620   test 82.79% / 0.5770   lr 4.531e-06\n",
      "E498 | train 82.67% / 0.5975   val 83.55% / 0.5625   test 82.96% / 0.5771   lr 2.014e-06\n",
      "E499 | train 83.04% / 0.5911   val 83.61% / 0.5647   test 83.08% / 0.5778   lr 5.036e-07\n",
      "E500 | train 82.97% / 0.5926   val 83.53% / 0.5611   test 83.29% / 0.5739   lr 1.000e-10\n",
      "✅ KD finished. Best val acc: 83.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_350822/1792734876.py:268: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load(\"best_student.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Student Test Accuracy: 83.16%\n",
      "⏱️ Student inference (1 img): 3.28 ms\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# 0. 环境与随机种子\n",
    "# =============================================================\n",
    "import os, random, time, gc, math, numpy as np, torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"💻 device =\", device)\n",
    "# 训练集统计得到的常量\n",
    "ALPHA = 2.57639e-07      # 1 / (std_train + 1e-5)\n",
    "BETA  = -0.306756        # -mean_train * ALPHA\n",
    "# =============================================================\n",
    "# 1. 数据集：SCDTensorDataset  (与 Teacher 版完全一致)\n",
    "# =============================================================\n",
    "class SCDTensorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    每个 .pt: (X, y)  X shape=(N,128,128) or (N,256,256)  y shape=(N,1)\n",
    "    \"\"\"\n",
    "    def __init__(self, pt_files):\n",
    "        self.meta, self.cache, off = [], {}, 0\n",
    "        for f in pt_files:\n",
    "            n = torch.load(f, map_location=\"cpu\")[0].shape[0]\n",
    "            self.meta.append((f, off, off+n)); off += n\n",
    "        self.N = off\n",
    "\n",
    "    def __len__(self): return self.N\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        for f, beg, end in self.meta:\n",
    "            if beg <= idx < end:\n",
    "                if f not in self.cache:\n",
    "                    self.cache[f] = torch.load(f, map_location=\"cpu\", weights_only=True)\n",
    "                X, y = self.cache[f]\n",
    "                img  = X[idx - beg].float()\n",
    "                # ───── 固定 α/β 归一化 ─────\n",
    "                img = img * ALPHA + BETA        # 只 1 乘 1 加\n",
    "                img = img.unsqueeze(0)          # (1,128,128)\n",
    "                # ───────────────────────────\n",
    "                return img, y[idx - beg].long().squeeze()\n",
    "\n",
    "# =============================================================\n",
    "# 2. Teacher 网络 (与你保存的完全一致) + Student 网络\n",
    "# =============================================================\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, p=0.1):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p == 0.:\n",
    "            return x\n",
    "        keep = torch.rand((x.size(0), 1, 1, 1), device=x.device) > self.p\n",
    "        return x * keep / (1 - self.p)\n",
    "   \n",
    "\n",
    "\n",
    "# ---------- Teacher 基本块 ----------\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, stride=1, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, stride, 1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_c)\n",
    "        self.down  = nn.Identity()\n",
    "        if stride != 1 or in_c != out_c:\n",
    "            self.down = nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_c))\n",
    "        # ★ 预先实例化 DropPath\n",
    "        self.drop = DropPath(drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        y = self.drop(y)                 # ★ 训练时随机丢残差\n",
    "        return F.relu(y + self.down(x))\n",
    "\n",
    "def make_layer(in_c, out_c, blocks, stride, p=0.1):\n",
    "    layers = [BasicBlock(in_c, out_c, stride, drop_prob=p)]\n",
    "    layers += [BasicBlock(out_c, out_c, 1, drop_prob=p) for _ in range(blocks-1)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# ---------- Teacher 全网 ----------\n",
    "class SCDResNet64(nn.Module):\n",
    "    def __init__(self, n_cls=8):\n",
    "        super().__init__()\n",
    "        self.stem   = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 7, 1, 3, bias=False),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, 2, 1))\n",
    "        self.layer1 = make_layer( 64,  64, 3, 1)\n",
    "        self.layer2 = make_layer( 64, 128, 3, 2)\n",
    "        self.layer3 = make_layer(128, 192, 3, 2)\n",
    "        self.layer4 = make_layer(192, 256, 3, 2)\n",
    "        self.head   = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, n_cls))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x); x = self.layer2(x)\n",
    "        x = self.layer3(x); x = self.layer4(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# ---------- Student 0.59M 参数 ----------\n",
    "class SCDResNet64_Student(nn.Module):\n",
    "    def __init__(self, n_cls=8):\n",
    "        super().__init__()\n",
    "        ch = [14, 18, 26, 36, 52]   # 总参数≈0.154 M\n",
    "        self.stem   = nn.Sequential(\n",
    "            nn.Conv2d(1, ch[0], 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(ch[0]), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3,2,1))\n",
    "        self.layer1 = make_layer(ch[0], ch[1], 1, 1)\n",
    "        self.layer2 = make_layer(ch[1], ch[2], 1, 2)\n",
    "        self.layer3 = make_layer(ch[2], ch[3], 2, 2)  # ★ 多 1 个块\n",
    "        self.layer4 = make_layer(ch[3], ch[4], 2, 2)  # ★ 多 1 个块\n",
    "        self.head   = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "            nn.Linear(ch[4], n_cls))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x); x = self.layer2(x)\n",
    "        x = self.layer3(x); x = self.layer4(x)\n",
    "        return self.head(x)\n",
    "\n",
    " \n",
    "\n",
    "# =============================================================\n",
    "# 3. 数据划分（固定 10+10 训练，4 验证，4 测试）\n",
    "# =============================================================\n",
    "pt_files = sorted([f\"../../cached_scd_tim/2022_c_64/2022_batch_{i:02d}.pt\"\n",
    "                   for i in range(1, 29)])       # 01-28\n",
    "train_files = pt_files[:10] + pt_files[-10:]     # 01-10 & 19-28\n",
    "val_files   = pt_files[10:14]                    # 11-14\n",
    "test_files  = pt_files[14:18]                    # 15-18\n",
    "\n",
    "train_ds = SCDTensorDataset(train_files)\n",
    "val_ds   = SCDTensorDataset(val_files)\n",
    "test_ds  = SCDTensorDataset(test_files)\n",
    "print(f\"📂 train {len(train_ds):,} | val {len(val_ds):,} | test {len(test_ds):,}\")\n",
    "\n",
    "loader_cfg = dict(batch_size=32, num_workers=4,\n",
    "                  pin_memory=(device=='cuda'))\n",
    "train_ld = DataLoader(train_ds, shuffle=True,  **loader_cfg)\n",
    "val_ld   = DataLoader(val_ds,   shuffle=False, **loader_cfg)\n",
    "test_ld  = DataLoader(test_ds,  shuffle=False, **loader_cfg)\n",
    "\n",
    "# =============================================================\n",
    "# 4. 蒸馏工具函数\n",
    "# =============================================================\n",
    "def kd_loss(s_logit, t_logit, y, T=6.0, alpha=0.9, ls=0.05):\n",
    "    kd = F.kl_div(\n",
    "        F.log_softmax(s_logit/T, dim=1),\n",
    "        F.softmax(t_logit/T, dim=1),\n",
    "        reduction='batchmean') * (T*T)\n",
    "    ce = F.cross_entropy(s_logit, y, label_smoothing=ls)\n",
    "    return alpha*kd + (1-alpha)*ce\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, loader):\n",
    "    model.eval()\n",
    "    tot = loss_sum = acc = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        out = model(xb)\n",
    "        loss = F.cross_entropy(out, yb)\n",
    "        pred = out.argmax(1)\n",
    "        bsz  = yb.size(0)\n",
    "        tot += bsz\n",
    "        loss_sum += loss.item()*bsz\n",
    "        acc += (pred==yb).sum().item()\n",
    "    return loss_sum/tot, acc/tot\n",
    "\n",
    "# =============================================================\n",
    "# 5. 训练 KD（调度与 Teacher 对齐）\n",
    "# =============================================================\n",
    "teacher = SCDResNet64(); teacher.load_state_dict(\n",
    "    torch.load(\"best_scd_resnet.pth\", map_location=device))\n",
    "teacher.eval().to(device)\n",
    "\n",
    "student = SCDResNet64_Student().to(device)\n",
    "print(f\"Student params: {sum(p.numel() for p in student.parameters())/1e6:.2f} M\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---- 优化器 ----\n",
    "opt = torch.optim.SGD(student.parameters(), lr=0.05,\n",
    "                      momentum=0.9, weight_decay= 1e-4)\n",
    "\n",
    "# ---- 学习率调度：5 个 epoch 线性 warm-up，然后 Cosine ----\n",
    "warm_epochs   = 5\n",
    "total_epochs  = 500\n",
    "eta_min       = 1e-10                       # 余弦最低 lr\n",
    "\n",
    "warmup = torch.optim.lr_scheduler.LinearLR(\n",
    "            opt, start_factor=0.1, end_factor=1.0, total_iters=warm_epochs)\n",
    "cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            opt, T_max=total_epochs - warm_epochs, eta_min=eta_min)\n",
    "\n",
    "sched = torch.optim.lr_scheduler.SequentialLR(\n",
    "            opt, schedulers=[warmup, cosine], milestones=[warm_epochs])\n",
    "\n",
    "best_val = 0.0\n",
    "for epoch in range(1, total_epochs + 1):\n",
    "    # ===== 训练阶段 =====\n",
    "    student.train()\n",
    "    tr_tot = tr_loss_sum = tr_correct = 0\n",
    "    for xb, yb in train_ld:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        with torch.no_grad():\n",
    "            t_out = teacher(xb)\n",
    "        s_out  = student(xb)\n",
    "        loss   = kd_loss(s_out, t_out, yb)\n",
    "\n",
    "        opt.zero_grad(); loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        # 累计训练指标\n",
    "        bsz = yb.size(0)\n",
    "        tr_tot       += bsz\n",
    "        tr_loss_sum  += loss.item() * bsz\n",
    "        tr_correct   += (s_out.argmax(1) == yb).sum().item()\n",
    "\n",
    "    sched.step()           # 更新 lr\n",
    "\n",
    "    # 计算 train epoch 指标\n",
    "    train_loss = tr_loss_sum / tr_tot\n",
    "    train_acc  = tr_correct  / tr_tot\n",
    "\n",
    "    # ===== 验证 & 测试阶段 =====\n",
    "    val_loss,  val_acc  = eval_model(student, val_ld)\n",
    "    test_loss, test_acc = eval_model(student, test_ld)\n",
    "\n",
    "    # 打印与保存\n",
    "    print(f\"E{epoch:03d} | \"\n",
    "          f\"train {train_acc*100:5.2f}% / {train_loss:.4f}   \"\n",
    "          f\"val {val_acc*100:5.2f}% / {val_loss:.4f}   \"\n",
    "          f\"test {test_acc*100:5.2f}% / {test_loss:.4f}   \"\n",
    "          f\"lr {sched.get_last_lr()[0]:.3e}\")\n",
    "\n",
    "    if val_acc > best_val:\n",
    "        best_val = val_acc\n",
    "        torch.save(student.state_dict(), \"best_student.pth\")\n",
    "\n",
    "print(\"✅ KD finished. Best val acc:\", best_val*100)\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 6. 最终测试\n",
    "# =============================================================\n",
    "student.load_state_dict(torch.load(\"best_student.pth\"))\n",
    "_, test_acc = eval_model(student, test_ld)\n",
    "print(f\"🧪 Student Test Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "# =============================================================\n",
    "# 7. 单张推理时延\n",
    "# =============================================================\n",
    "xb, _ = next(iter(test_ld))\n",
    "img = xb[0:1].to(device)\n",
    "torch.cuda.synchronize() if device=='cuda' else None\n",
    "t0 = time.time()\n",
    "_ = student(img)\n",
    "torch.cuda.synchronize() if device=='cuda' else None\n",
    "t1 = time.time()\n",
    "print(f\"⏱️ Student inference (1 img): {(t1-t0)*1000:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module                           Type           Input→Output                   ElemsIn   ElemsOut Kernel/Stride/Padding          #P         MACs\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "stem.0                           Conv2d         [1, 64, 64]→[24, 64, 64]         4,096     98,304 K(3,3) S(1, 1) P(1, 1)        216      884,736\n",
      "stem.1                           BatchNorm2d    [24, 64, 64]→[24, 64, 64]       98,304     98,304                                48            0\n",
      "stem.2                           ReLU           [24, 64, 64]→[24, 64, 64]       98,304     98,304                                 0            0\n",
      "stem.3                           MaxPool2d      [24, 64, 64]→[24, 32, 32]       98,304     24,576 K3 S2 P1                        0            0\n",
      "layer1.0.conv1                   Conv2d         [24, 32, 32]→[24, 32, 32]       24,576     24,576 K(3,3) S(1, 1) P(1, 1)      5,184    5,308,416\n",
      "layer1.0.bn1                     BatchNorm2d    [24, 32, 32]→[24, 32, 32]       24,576     24,576                                48            0\n",
      "layer1.0.conv2                   Conv2d         [24, 32, 32]→[24, 32, 32]       24,576     24,576 K(3,3) S(1, 1) P(1, 1)      5,184    5,308,416\n",
      "layer1.0.bn2                     BatchNorm2d    [24, 32, 32]→[24, 32, 32]       24,576     24,576                                48            0\n",
      "layer1.0.down                    Identity       [24, 32, 32]→[24, 32, 32]       24,576     24,576                                 0            0\n",
      "layer1.1.conv1                   Conv2d         [24, 32, 32]→[24, 32, 32]       24,576     24,576 K(3,3) S(1, 1) P(1, 1)      5,184    5,308,416\n",
      "layer1.1.bn1                     BatchNorm2d    [24, 32, 32]→[24, 32, 32]       24,576     24,576                                48            0\n",
      "layer1.1.conv2                   Conv2d         [24, 32, 32]→[24, 32, 32]       24,576     24,576 K(3,3) S(1, 1) P(1, 1)      5,184    5,308,416\n",
      "layer1.1.bn2                     BatchNorm2d    [24, 32, 32]→[24, 32, 32]       24,576     24,576                                48            0\n",
      "layer1.1.down                    Identity       [24, 32, 32]→[24, 32, 32]       24,576     24,576                                 0            0\n",
      "layer2.0.conv1                   Conv2d         [24, 32, 32]→[48, 16, 16]       24,576     12,288 K(3,3) S(2, 2) P(1, 1)     10,368    2,654,208\n",
      "layer2.0.bn1                     BatchNorm2d    [48, 16, 16]→[48, 16, 16]       12,288     12,288                                96            0\n",
      "layer2.0.conv2                   Conv2d         [48, 16, 16]→[48, 16, 16]       12,288     12,288 K(3,3) S(1, 1) P(1, 1)     20,736    5,308,416\n",
      "layer2.0.bn2                     BatchNorm2d    [48, 16, 16]→[48, 16, 16]       12,288     12,288                                96            0\n",
      "layer2.0.down.0                  Conv2d         [24, 32, 32]→[48, 16, 16]       24,576     12,288 K(1,1) S(2, 2) P(0, 0)      1,152      294,912\n",
      "layer2.0.down.1                  BatchNorm2d    [48, 16, 16]→[48, 16, 16]       12,288     12,288                                96            0\n",
      "layer2.1.conv1                   Conv2d         [48, 16, 16]→[48, 16, 16]       12,288     12,288 K(3,3) S(1, 1) P(1, 1)     20,736    5,308,416\n",
      "layer2.1.bn1                     BatchNorm2d    [48, 16, 16]→[48, 16, 16]       12,288     12,288                                96            0\n",
      "layer2.1.conv2                   Conv2d         [48, 16, 16]→[48, 16, 16]       12,288     12,288 K(3,3) S(1, 1) P(1, 1)     20,736    5,308,416\n",
      "layer2.1.bn2                     BatchNorm2d    [48, 16, 16]→[48, 16, 16]       12,288     12,288                                96            0\n",
      "layer2.1.down                    Identity       [48, 16, 16]→[48, 16, 16]       12,288     12,288                                 0            0\n",
      "layer3.0.conv1                   Conv2d         [48, 16, 16]→[72, 8, 8]         12,288      4,608 K(3,3) S(2, 2) P(1, 1)     31,104    1,990,656\n",
      "layer3.0.bn1                     BatchNorm2d    [72, 8, 8]→[72, 8, 8]            4,608      4,608                               144            0\n",
      "layer3.0.conv2                   Conv2d         [72, 8, 8]→[72, 8, 8]            4,608      4,608 K(3,3) S(1, 1) P(1, 1)     46,656    2,985,984\n",
      "layer3.0.bn2                     BatchNorm2d    [72, 8, 8]→[72, 8, 8]            4,608      4,608                               144            0\n",
      "layer3.0.down.0                  Conv2d         [48, 16, 16]→[72, 8, 8]         12,288      4,608 K(1,1) S(2, 2) P(0, 0)      3,456      221,184\n",
      "layer3.0.down.1                  BatchNorm2d    [72, 8, 8]→[72, 8, 8]            4,608      4,608                               144            0\n",
      "layer3.1.conv1                   Conv2d         [72, 8, 8]→[72, 8, 8]            4,608      4,608 K(3,3) S(1, 1) P(1, 1)     46,656    2,985,984\n",
      "layer3.1.bn1                     BatchNorm2d    [72, 8, 8]→[72, 8, 8]            4,608      4,608                               144            0\n",
      "layer3.1.conv2                   Conv2d         [72, 8, 8]→[72, 8, 8]            4,608      4,608 K(3,3) S(1, 1) P(1, 1)     46,656    2,985,984\n",
      "layer3.1.bn2                     BatchNorm2d    [72, 8, 8]→[72, 8, 8]            4,608      4,608                               144            0\n",
      "layer3.1.down                    Identity       [72, 8, 8]→[72, 8, 8]            4,608      4,608                                 0            0\n",
      "layer4.0.conv1                   Conv2d         [72, 8, 8]→[96, 4, 4]            4,608      1,536 K(3,3) S(2, 2) P(1, 1)     62,208      995,328\n",
      "layer4.0.bn1                     BatchNorm2d    [96, 4, 4]→[96, 4, 4]            1,536      1,536                               192            0\n",
      "layer4.0.conv2                   Conv2d         [96, 4, 4]→[96, 4, 4]            1,536      1,536 K(3,3) S(1, 1) P(1, 1)     82,944    1,327,104\n",
      "layer4.0.bn2                     BatchNorm2d    [96, 4, 4]→[96, 4, 4]            1,536      1,536                               192            0\n",
      "layer4.0.down.0                  Conv2d         [72, 8, 8]→[96, 4, 4]            4,608      1,536 K(1,1) S(2, 2) P(0, 0)      6,912      110,592\n",
      "layer4.0.down.1                  BatchNorm2d    [96, 4, 4]→[96, 4, 4]            1,536      1,536                               192            0\n",
      "layer4.1.conv1                   Conv2d         [96, 4, 4]→[96, 4, 4]            1,536      1,536 K(3,3) S(1, 1) P(1, 1)     82,944    1,327,104\n",
      "layer4.1.bn1                     BatchNorm2d    [96, 4, 4]→[96, 4, 4]            1,536      1,536                               192            0\n",
      "layer4.1.conv2                   Conv2d         [96, 4, 4]→[96, 4, 4]            1,536      1,536 K(3,3) S(1, 1) P(1, 1)     82,944    1,327,104\n",
      "layer4.1.bn2                     BatchNorm2d    [96, 4, 4]→[96, 4, 4]            1,536      1,536                               192            0\n",
      "layer4.1.down                    Identity       [96, 4, 4]→[96, 4, 4]            1,536      1,536                                 0            0\n",
      "head.0                           AdaptiveAvgPool2d [96, 4, 4]→[96, 1, 1]            1,536         96                                 0            0\n",
      "head.1                           Flatten        [96, 1, 1]→[96]                     96         96                                 0            0\n",
      "head.2                           Dropout        [96]→[96]                           96         96                                 0            0\n",
      "head.3                           Linear         [96]→[8]                            96          8 [in=96, out=8]                776          768\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total unique parameters                                                                                                    590,336  (0.59 M)\n",
      "Total MACs per sample                                                                                                      57,250,560  (57.25 M)\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "# ──────────────────── Student 网络定义 ────────────────────\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, stride, 1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_c)\n",
    "        self.down  = nn.Identity()\n",
    "        if stride != 1 or in_c != out_c:\n",
    "            self.down = nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_c))\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        return F.relu(y + self.down(x))\n",
    "\n",
    "def make_layer(in_c, out_c, blocks, stride):\n",
    "    lays = [BasicBlock(in_c, out_c, stride)]\n",
    "    lays += [BasicBlock(out_c, out_c) for _ in range(blocks-1)]\n",
    "    return nn.Sequential(*lays)\n",
    "\n",
    "class SCDResNet64_Student(nn.Module):\n",
    "    def __init__(self, n_cls=8):\n",
    "        super().__init__()\n",
    "        ch = [24, 24, 48, 72, 96]\n",
    "        self.stem   = nn.Sequential(\n",
    "            nn.Conv2d(1, ch[0], 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(ch[0]), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, 2, 1))\n",
    "        self.layer1 = make_layer(ch[0], ch[1], 2, 1)\n",
    "        self.layer2 = make_layer(ch[1], ch[2], 2, 2)\n",
    "        self.layer3 = make_layer(ch[2], ch[3], 2, 2)\n",
    "        self.layer4 = make_layer(ch[3], ch[4], 2, 2)\n",
    "        self.head   = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "            nn.Dropout(0.4), nn.Linear(ch[4], n_cls))\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x); x = self.layer2(x)\n",
    "        x = self.layer3(x); x = self.layer4(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# ──────────────────── 逐层统计脚本 ────────────────────\n",
    "device      = \"cpu\"\n",
    "model       = SCDResNet64_Student().to(device)\n",
    "records     = OrderedDict()\n",
    "\n",
    "def is_leaf(m): return len(list(m.children())) == 0\n",
    "def numel(shape): return int(reduce(mul, shape, 1))          # (C,H,W) -> 元素个数\n",
    "\n",
    "def register_hook(mod, name):\n",
    "    def hook(mod, inp, out):\n",
    "        info = {\n",
    "            \"type\":   mod.__class__.__name__,\n",
    "            \"in\":     tuple(inp[0].shape),\n",
    "            \"out\":    tuple(out.shape) if isinstance(out, torch.Tensor) else tuple(out[0].shape),\n",
    "            \"params\": sum(p.numel() for p in mod.parameters()),\n",
    "            \"macs\":   0,      # default\n",
    "        }\n",
    "        # ── 额外字段：核/步幅/填充 & MACs ───────────────────────────────\n",
    "        if isinstance(mod, nn.Conv2d):\n",
    "            Kh, Kw  = mod.kernel_size\n",
    "            Hout, Wout = info[\"out\"][2], info[\"out\"][3]\n",
    "            Cin, Cout = info[\"in\"][1], info[\"out\"][1]\n",
    "            info[\"ksp\"] = f\"K({Kh},{Kw}) S{mod.stride} P{mod.padding}\"\n",
    "            info[\"macs\"] = Cout * Cin // mod.groups * Kh * Kw * Hout * Wout\n",
    "        elif isinstance(mod, nn.MaxPool2d):\n",
    "            info[\"ksp\"] = f\"K{mod.kernel_size} S{mod.stride} P{mod.padding}\"\n",
    "        elif isinstance(mod, nn.AvgPool2d):\n",
    "            info[\"ksp\"] = f\"K{mod.kernel_size} S{mod.stride}\"\n",
    "        elif isinstance(mod, nn.Linear):\n",
    "            info[\"ksp\"] = f\"[in={mod.in_features}, out={mod.out_features}]\"\n",
    "            info[\"macs\"] = mod.in_features * mod.out_features\n",
    "        else:\n",
    "            info[\"ksp\"] = \"\"\n",
    "        # 元素数量\n",
    "        info[\"in_ne\"]  = numel(info[\"in\"][1:])   # 去掉 batch 维\n",
    "        info[\"out_ne\"] = numel(info[\"out\"][1:])\n",
    "        records[name]  = info\n",
    "    return hook\n",
    "\n",
    "for n, m in model.named_modules():\n",
    "    if n and is_leaf(m):\n",
    "        m.register_forward_hook(register_hook(m, n))\n",
    "\n",
    "_ = model(torch.randn(1, 1, 64, 64, device=device))\n",
    "\n",
    "# ──────────────────── 打印结果 ────────────────────\n",
    "hdr = (\"Module\", \"Type\", \"Input→Output\", \"ElemsIn\", \"ElemsOut\",\n",
    "       \"Kernel/Stride/Padding\", \"#P\", \"MACs\")\n",
    "print(f\"{hdr[0]:<32} {hdr[1]:<14} {hdr[2]:<28} {hdr[3]:>9} {hdr[4]:>10} \"\n",
    "      f\"{hdr[5]:<22} {hdr[6]:>10} {hdr[7]:>12}\")\n",
    "print(\"-\"*149)\n",
    "\n",
    "total_params = total_macs = 0\n",
    "for n, d in records.items():\n",
    "    io  = f\"{list(d['in'])[1:]}→{list(d['out'])[1:]}\"\n",
    "    print(f\"{n:<32} {d['type']:<14} {io:<28} \"\n",
    "          f\"{d['in_ne']:>9,} {d['out_ne']:>10,} \"\n",
    "          f\"{d['ksp']:<22} {d['params']:>10,} {d['macs']:>12,}\")\n",
    "    total_params += d['params']\n",
    "    total_macs   += d['macs']\n",
    "\n",
    "print(\"-\"*149)\n",
    "print(f\"{'Total unique parameters':<122} {total_params:,}  ({total_params/1e6:.2f} M)\")\n",
    "print(f\"{'Total MACs per sample':<122} {total_macs:,}  ({total_macs/1e6:.2f} M)\")\n",
    "\n",
    "'''\n",
    "def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        return F.relu(y + self.down(x))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推理时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️  running on cuda\n",
      "🧠 Student params: 0.59 M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134615/149786502.py:89: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load(\"best_student.pth\", map_location=DEVICE))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b87b4eda0c74b879226e90fa3b7646e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Infer:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊  Average latency per stage (1000 samples)\n",
      "────────────────────────────────────────────────────\n",
      "preprocess:   12.251  ms\n",
      "stem      :    0.353  ms\n",
      "layer1    :    0.723  ms\n",
      "layer2    :    0.798  ms\n",
      "layer3    :    0.849  ms\n",
      "layer4    :    0.906  ms\n",
      "head      :    0.216  ms\n",
      "────────────────────────────────────────────────────\n",
      "Total     :   16.097  ms\n",
      "────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# end2end_latency_tim2infer_nods.py\n",
    "# ------------------------------------------------------------\n",
    "# 只计时：CPU→GPU (2048 IQ) → SCD 计算 → Patch64 → Student 推理\n",
    "# 排除：磁盘读取 & 32768→2048 降采样\n",
    "# ------------------------------------------------------------\n",
    "import os, glob, time, math, gc, random\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.signal import decimate\n",
    "import os, random, numpy as np, math, gc, time, torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 0. 环境\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "import os, random, numpy as np, torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"⚙️  running on\", DEVICE)\n",
    "\n",
    "ROOT_TXT   = \"../../CSPB.ML_dataset/CSPB_ML_2022_Data_2048\"\n",
    "PATCH_R0C0 = 96                # 中心 64×64 起点\n",
    "N_TEST     = 1000               # 随机抽取文件数\n",
    "DELAY      = 0.05              # 给 GPU 一点休息时间，避免温度联动\n",
    "DTYPE_OUT  = torch.float32                # 最终输出精度\n",
    "# ------------------------------------------------------------\n",
    "# Student 网络（与之前一致，略）……\n",
    "# （此处粘贴你的 SCDResNet64_Student 定义与权重加载）\n",
    "# ------------------------------------------------------------\n",
    "# ---------- Teacher 基本块 ----------\n",
    "# ---------- Teacher 基本块 ----------\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, stride, 1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_c)\n",
    "        self.down  = nn.Identity()\n",
    "        if stride != 1 or in_c != out_c:\n",
    "            self.down = nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_c))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        return F.relu(y + self.down(x))\n",
    "\n",
    "def make_layer(in_c, out_c, blocks, stride):\n",
    "    layers = [BasicBlock(in_c, out_c, stride)]\n",
    "    layers += [BasicBlock(out_c, out_c) for _ in range(blocks-1)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# ---------- Teacher 全网 ----------\n",
    "class SCDResNet64(nn.Module):\n",
    "    def __init__(self, n_cls=8):\n",
    "        super().__init__()\n",
    "        self.stem   = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 7, 1, 3, bias=False),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, 2, 1))\n",
    "        self.layer1 = make_layer( 64,  64, 3, 1)\n",
    "        self.layer2 = make_layer( 64, 128, 3, 2)\n",
    "        self.layer3 = make_layer(128, 192, 3, 2)\n",
    "        self.layer4 = make_layer(192, 256, 3, 2)\n",
    "        self.head   = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, n_cls))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x); x = self.layer2(x)\n",
    "        x = self.layer3(x); x = self.layer4(x)\n",
    "        return self.head(x)\n",
    "\n",
    "    \n",
    "\n",
    "student = SCDResNet64_Student().to(DEVICE)\n",
    "student.load_state_dict(torch.load(\"best_student.pth\", map_location=DEVICE))\n",
    "student.eval()\n",
    "print(f\"🧠 Student params: {sum(p.numel() for p in student.parameters())/1e6:.2f} M\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 工具函数\n",
    "# ------------------------------------------------------------\n",
    "def decimate_complex(x: np.ndarray, q: int = 16) -> np.ndarray:\n",
    "    re = decimate(np.real(x), q, ftype='fir', zero_phase=True)\n",
    "    im = decimate(np.imag(x), q, ftype='fir', zero_phase=True)\n",
    "    return re + 1j * im\n",
    "\n",
    "def iq2_scd_matrix_torch(iq_2048: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    2048 点复数 IQ (CUDA) → (256,256) torch.float32 SCD 矩阵\n",
    "    代码与训练时保持一致（略掉注释以精简）\n",
    "    \"\"\"\n",
    "    Np, L, P = 256, 64, 32\n",
    "    NN = (P - 1) * L + Np\n",
    "    if iq_2048.numel() < NN:\n",
    "        pad = torch.zeros(NN - iq_2048.numel(), dtype=iq_2048.dtype, device=iq_2048.device)\n",
    "        sig = torch.cat([iq_2048, pad])\n",
    "    else:\n",
    "        sig = iq_2048\n",
    "\n",
    "    X = torch.stack([sig[k*L:k*L+Np] for k in range(P)], dim=1)  # (256,32)\n",
    "\n",
    "    # ✅ Chebyshev-500 窗函数（直接复用已有的完整数组）\n",
    "    a_np = np.array([0.079999924,0.08013916,0.080558777,0.081256866,0.082231522,0.083486557,0.085018158,0.086826324,0.088907242,0.091264725,0.093893051,0.096792221,0.099962234,0.10339737,0.10709953,0.111063,0.11528778,0.11976814,0.124506,0.12949562,0.13473511,0.14021873,0.1459465,0.15191269,0.15811539,0.16454887,0.17121124,0.17809677,0.18520355,0.19252396,0.20005608,0.20779419,0.21573448,0.22387123,0.23220062,0.24071503,0.24941254,0.25828743,0.26733208,0.27654266,0.28591156,0.29543686,0.30510902,0.31492615,0.32487679,0.33496094,0.34516716,0.35549355,0.36593056,0.37647438,0.38711739,0.39785194,0.40867424,0.41957474,0.43054962,0.44159126,0.45269203,0.4638443,0.47504425,0.48628426,0.49755669,0.50885391,0.52017021,0.53149986,0.54283333,0.55416489,0.56548882,0.57679749,0.58808327,0.59934044,0.61055946,0.62173843,0.63286591,0.64393806,0.65494537,0.66588593,0.67674828,0.6875267,0.69821739,0.70881081,0.71930122,0.72968483,0.73995209,0.75009727,0.76011467,0.77000046,0.7797451,0.78934479,0.79879189,0.80808449,0.81721115,0.82617188,0.83495903,0.84356499,0.85198784,0.86022186,0.86826134,0.87610054,0.88373566,0.89116287,0.89837646,0.90537262,0.91214752,0.91869545,0.9250145,0.93109894,0.93694687,0.94255447,0.94791603,0.95302963,0.95789337,0.96250343,0.96685791,0.97095108,0.97478485,0.9783535,0.98165512,0.98468971,0.98745537,0.98994827,0.99216843,0.99411201,0.99578285,0.99717522,0.99829102,0.99912834,0.99968529,0.99996567,0.99996567,0.99968529,0.99912834,0.99829102,0.99717522,0.99578285,0.99411201,0.99216843,0.98994827,0.98745537,0.98468971,0.98165512,0.9783535,0.97478485,0.97095108,0.96685791,0.96250343,0.95789337,0.95302963,0.94791603,0.94255447,0.93694687,0.93109894,0.9250145,0.91869545,0.91214752,0.90537262,0.89837646,0.89116287,0.88373566,0.87610054,0.86826134,0.86022186,0.85198784,0.84356499,0.83495903,0.82617188,0.81721115,0.80808449,0.79879189,0.78934479,0.7797451,0.77000046,0.76011467,0.75009727,0.73995209,0.72968483,0.71930122,0.70881081,0.69821739,0.6875267,0.67674828,0.66588593,0.65494537,0.64393806,0.63286591,0.62173843,0.61055946,0.59934044,0.58808327,0.57679749,0.56548882,0.55416489,0.54283333,0.53149986,0.52017021,0.50885391,0.49755669,0.48628426,0.47504425,0.4638443,0.45269203,0.44159126,0.43054962,0.41957474,0.40867424,0.39785194,0.38711739,0.37647438,0.36593056,0.35549355,0.34516716,0.33496094,0.32487679,0.31492615,0.30510902,0.29543686,0.28591156,0.27654266,0.26733208,0.25828743,0.24941254,0.24071503,0.23220062,0.22387123,0.21573448,0.20779419,0.20005608,0.19252396,0.18520355,0.17809677,0.17121124,0.16454887,0.15811539,0.15191269,0.1459465,0.14021873,0.13473511,0.12949562,0.124506,0.11976814,0.11528778,0.111063,0.10709953,0.10339737,0.099962234,0.096792221,0.093893051,0.091264725,0.088907242,0.086826324,0.085018158,0.083486557,0.082231522,0.081256866,0.080558777,0.08013916,0.079999924], dtype=np.float32)  \n",
    "    a = torch.tensor(a_np, device='cuda')     # 转为 CUDA Tensor\n",
    "\n",
    "    # 加窗\n",
    "    XW = a[:, None] * X\n",
    "\n",
    "    XF1 = torch.fft.fft(XW * X, dim=0)\n",
    "    XF1 = torch.cat([XF1[Np//2:], XF1[:Np//2]], dim=0)\n",
    "\n",
    "    k = torch.arange(-Np//2, Np//2, device=sig.device)[:, None]\n",
    "    idx = torch.arange(P, device=sig.device)[None, :]\n",
    "    E = torch.exp(-1j * 2 * math.pi * k * idx * L / Np)\n",
    "    XD = (XF1 * E).T\n",
    "\n",
    "    XM = XD[:, :, None] * XD[:, None, :].conj()\n",
    "    XM = XM.reshape(P, -1)\n",
    "    XF2 = torch.fft.fft(XM, dim=0)\n",
    "    Z = torch.abs(XF2) ** 2\n",
    "\n",
    "    out = torch.zeros((16, Z.shape[1]), device=iq_2048.device)\n",
    "    out[:8] = Z[P//2 : 3*P//4]\n",
    "    out[8:] = Z[P//4 : P//2]\n",
    "\n",
    "   \n",
    "    return out[8].reshape(256, 256).T.to(DTYPE_OUT)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# （1）读取 txt → (2048,) complex64 GPU\n",
    "# ------------------------------------------------------------\n",
    "def txt2048_to_iq_cuda(txt_path: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    读取单个 txt，格式：2048 行  “Re  Im”，float32\n",
    "    返回：torch.complex64 (2048,)  GPU tensor\n",
    "    \"\"\"\n",
    "    data = np.loadtxt(txt_path, dtype=np.float32)      # shape=(2048,2)\n",
    "    iq   = np.ascontiguousarray(data[:,0] + 1j*data[:,1], dtype=np.complex64)\n",
    "    return torch.from_numpy(iq).to(DEVICE)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# （2）SCD → Patch64，按训练时归一化\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "ALPHA = 6.93232e-05      # 1 / (std_train + 1e-5)\n",
    "BETA  = -0.296705       # -mean_train * ALPHA\n",
    "def scd_patch64_from_iq(iq_cuda: torch.Tensor) -> torch.Tensor:\n",
    "    scd256 = iq2_scd_matrix_torch(iq_cuda)\n",
    "    r0 = PATCH_R0C0\n",
    "    patch = scd256[r0:r0+64, r0:r0+64]\n",
    "    patch = patch * ALPHA + BETA                     # 先乘再加\n",
    "    return patch.unsqueeze(0)   # (1,64,64)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. 计时钩子\n",
    "# ------------------------------------------------------------\n",
    "USE_GPU = (DEVICE == \"cuda\")\n",
    "layer_names  = [\"stem\", \"layer1\", \"layer2\", \"layer3\", \"layer4\", \"head\"]\n",
    "timer_sum_ms = {k: 0.0 for k in layer_names}\n",
    "timer_sum_ms[\"preprocess\"] = 0.0\n",
    "timer_cnt = 0\n",
    "\n",
    "def add_layer_hooks(net: nn.Module):\n",
    "    for name in layer_names:\n",
    "        mod = getattr(net, name)\n",
    "\n",
    "        if USE_GPU:                          # GPU 计时\n",
    "            s_evt = torch.cuda.Event(enable_timing=True)\n",
    "            e_evt = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "            def pre(_, __, evt=s_evt):       # ← 把对象写进默认实参\n",
    "                evt.record()\n",
    "\n",
    "            def post(_, __, ___,\n",
    "                     s_evt=s_evt, e_evt=e_evt, layer=name):\n",
    "                e_evt.record(); torch.cuda.synchronize()\n",
    "                dt = s_evt.elapsed_time(e_evt)       # ms\n",
    "                timer_sum_ms[layer] += dt\n",
    "\n",
    "        else:                                # CPU 计时\n",
    "            def pre(_, __, layer=name):\n",
    "                mod._t0 = time.perf_counter()\n",
    "\n",
    "            def post(_, __, ___, layer=name):\n",
    "                dt = (time.perf_counter() - mod._t0) * 1e3\n",
    "                timer_sum_ms[layer] += dt\n",
    "\n",
    "        mod.register_forward_pre_hook(pre)\n",
    "        mod.register_forward_hook(post)\n",
    "\n",
    "\n",
    "add_layer_hooks(student)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. 收集测试文件\n",
    "# ------------------------------------------------------------\n",
    "txt_files = sorted(glob.glob(os.path.join(ROOT_TXT,\n",
    "                                          \"Batch_Dir_*\", \"signal_*.txt\")))\n",
    "random.shuffle(txt_files)\n",
    "txt_files = txt_files[:N_TEST]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. 推理循环\n",
    "# ------------------------------------------------------------\n",
    " \n",
    "for txt_path in tqdm(txt_files, desc=\"Infer\"):\n",
    "    gc.collect(); torch.cuda.empty_cache(); time.sleep(DELAY)\n",
    "\n",
    "    # 预处理计时\n",
    "    if USE_GPU:\n",
    "        p_s, p_e = torch.cuda.Event(True), torch.cuda.Event(True)\n",
    "        p_s.record()\n",
    "    else:\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "    iq_cuda = txt2048_to_iq_cuda(txt_path)\n",
    "    img     = scd_patch64_from_iq(iq_cuda)            # 1×64×64\n",
    "\n",
    "    if USE_GPU:\n",
    "        p_e.record(); torch.cuda.synchronize()\n",
    "        timer_sum_ms[\"preprocess\"] += p_s.elapsed_time(p_e)\n",
    "    else:\n",
    "        timer_sum_ms[\"preprocess\"] += (time.perf_counter()-t0)*1e3\n",
    "\n",
    "    _ = student(img.unsqueeze(0))                     # forward\n",
    "    timer_cnt += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. 打印结果\n",
    "# ------------------------------------------------------------\n",
    "print(f\"\\n📊  Average latency per stage ({timer_cnt} samples)\")\n",
    "print(\"────────────────────────────────────────────────────\")\n",
    "for k in [\"preprocess\"] + layer_names:\n",
    "    print(f\"{k:<10}: {timer_sum_ms[k]/timer_cnt:8.3f}  ms\")\n",
    "\n",
    "total_avg = sum(timer_sum_ms.values()) / timer_cnt\n",
    "print(\"────────────────────────────────────────────────────\")\n",
    "print(f\"{'Total':<10}: {total_avg:8.3f}  ms\")\n",
    "print(\"────────────────────────────────────────────────────\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for 2018 and 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💻 device = cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1571314/3860869149.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  n = torch.load(f, map_location=\"cpu\")[0].shape[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 train 80,000 | val 16,000 | test 16,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1571314/3860869149.py:165: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student_2022.load_state_dict(torch.load(\"best_student.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Student Test Accuracy for 2022: 93.67%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# 0. 环境与随机种子\n",
    "# =============================================================\n",
    "import os, random, time, gc, math, numpy as np, torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"💻 device =\", device)\n",
    "# 训练集统计得到的常量\n",
    "ALPHA = 2.57639e-07      # 1 / (std_train + 1e-5)\n",
    "BETA  = -0.306756        # -mean_train * ALPHA\n",
    "# =============================================================\n",
    "# 1. 数据集：SCDTensorDataset  (与 Teacher 版完全一致)\n",
    "# =============================================================\n",
    "class SCDTensorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    每个 .pt: (X, y)  X shape=(N,128,128) or (N,256,256)  y shape=(N,1)\n",
    "    \"\"\"\n",
    "    def __init__(self, pt_files):\n",
    "        self.meta, self.cache, off = [], {}, 0\n",
    "        for f in pt_files:\n",
    "            n = torch.load(f, map_location=\"cpu\")[0].shape[0]\n",
    "            self.meta.append((f, off, off+n)); off += n\n",
    "        self.N = off\n",
    "\n",
    "    def __len__(self): return self.N\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        for f, beg, end in self.meta:\n",
    "            if beg <= idx < end:\n",
    "                if f not in self.cache:\n",
    "                    self.cache[f] = torch.load(f, map_location=\"cpu\", weights_only=True)\n",
    "                X, y = self.cache[f]\n",
    "                img  = X[idx - beg].float()\n",
    "                # ───── 固定 α/β 归一化 ─────\n",
    "                img = img * ALPHA + BETA        # 只 1 乘 1 加\n",
    "                img = img.unsqueeze(0)          # (1,128,128)\n",
    "                # ───────────────────────────\n",
    "                return img, y[idx - beg].long().squeeze()\n",
    "\n",
    "# =============================================================\n",
    "# 2. Teacher 网络 (与你保存的完全一致) + Student 网络\n",
    "# =============================================================\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------- Teacher 基本块 ----------\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, stride, 1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_c)\n",
    "        self.down  = nn.Identity()\n",
    "        if stride != 1 or in_c != out_c:\n",
    "            self.down = nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_c))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        return F.relu(y + self.down(x))\n",
    "\n",
    "def make_layer(in_c, out_c, blocks, stride):\n",
    "    layers = [BasicBlock(in_c, out_c, stride)]\n",
    "    layers += [BasicBlock(out_c, out_c) for _ in range(blocks-1)]\n",
    "    return nn.Sequential(*layers)\n",
    "@torch.no_grad()\n",
    "def eval_model(model, loader):\n",
    "    model.eval()\n",
    "    tot = loss_sum = acc = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        out = model(xb)\n",
    "        loss = F.cross_entropy(out, yb)\n",
    "        pred = out.argmax(1)\n",
    "        bsz  = yb.size(0)\n",
    "        tot += bsz\n",
    "        loss_sum += loss.item()*bsz\n",
    "        acc += (pred==yb).sum().item()\n",
    "    return loss_sum/tot, acc/tot\n",
    "# ---------- Teacher 全网 ----------\n",
    "class SCDResNet64(nn.Module):\n",
    "    def __init__(self, n_cls=8):\n",
    "        super().__init__()\n",
    "        self.stem   = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 7, 1, 3, bias=False),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, 2, 1))\n",
    "        self.layer1 = make_layer( 64,  64, 3, 1)\n",
    "        self.layer2 = make_layer( 64, 128, 3, 2)\n",
    "        self.layer3 = make_layer(128, 192, 3, 2)\n",
    "        self.layer4 = make_layer(192, 256, 3, 2)\n",
    "        self.head   = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, n_cls))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x); x = self.layer2(x)\n",
    "        x = self.layer3(x); x = self.layer4(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# ---------- Student 0.59M 参数 ----------\n",
    "class SCDResNet64_Student(nn.Module):\n",
    "    def __init__(self, n_cls=8):\n",
    "        super().__init__()\n",
    "        ch = [24, 24, 48, 72, 96]          # ← 再对半\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(1, ch[0], 3, 1, 1, bias=False),  # ★ 改 7→3, pad 3→1\n",
    "            nn.BatchNorm2d(ch[0]), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, 2, 1))\n",
    "        self.layer1 = make_layer(ch[0], ch[1], 2, 1)\n",
    "        self.layer2 = make_layer(ch[1], ch[2], 2, 2)\n",
    "        self.layer3 = make_layer(ch[2], ch[3], 2, 2)\n",
    "        self.layer4 = make_layer(ch[3], ch[4], 2, 2)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(ch[4], n_cls))\n",
    "    def forward(self,x):\n",
    "        x=self.stem(x)\n",
    "        x=self.layer1(x); x=self.layer2(x)\n",
    "        x=self.layer3(x); x=self.layer4(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# =============================================================\n",
    "# 3. 数据划分（固定 10+10 训练，4 验证，4 测试）\n",
    "# =============================================================\n",
    "pt_files = sorted([f\"../../cached_scd_tim/2022_c_64/2022_batch_{i:02d}.pt\"\n",
    "                   for i in range(1, 29)])       # 01-28\n",
    "train_files = pt_files[:10] + pt_files[-10:]     # 01-10 & 19-28\n",
    "val_files   = pt_files[10:14]                    # 11-14\n",
    "test_files  = pt_files[14:18]                    # 15-18\n",
    "\n",
    "train_ds = SCDTensorDataset(train_files)\n",
    "val_ds   = SCDTensorDataset(val_files)\n",
    "test_ds  = SCDTensorDataset(test_files)\n",
    "print(f\"📂 train {len(train_ds):,} | val {len(val_ds):,} | test {len(test_ds):,}\")\n",
    "\n",
    "loader_cfg = dict(batch_size=32, num_workers=4,\n",
    "                  pin_memory=(device=='cuda'))\n",
    "train_ld = DataLoader(train_ds, shuffle=True,  **loader_cfg)\n",
    "val_ld   = DataLoader(val_ds,   shuffle=False, **loader_cfg)\n",
    "test_ld  = DataLoader(test_ds,  shuffle=False, **loader_cfg)\n",
    "\n",
    "student_2022 = SCDResNet64_Student().to(device)\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 6. 最终测试\n",
    "# =============================================================\n",
    "student_2022.load_state_dict(torch.load(\"best_student.pth\"))\n",
    "_, test_acc_2022 = eval_model(student_2022, test_ld)\n",
    "print(f\"🧪 Student Test Accuracy for 2022: {test_acc_2022*100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "内部家族名： Times New Roman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1571314/3341075156.py:117: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load('best_student.pth',    map_location=device))\n",
      "/tmp/ipykernel_1571314/3341075156.py:118: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  teacher.load_state_dict(torch.load('best_scd_resnet.pth', map_location=device))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch, seaborn as sns, matplotlib.pyplot as plt, matplotlib as mpl\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.nn as nn, torch.nn.functional as F\n",
    "import matplotlib as mpl\n",
    "from matplotlib import font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.lines as mlines\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "# =============================================================\n",
    "# 0. 参数 & 主题\n",
    "# =============================================================\n",
    "device  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "classes = ['BPSK','QPSK','8PSK','DQPSK','MSK','16QAM','64QAM','256QAM']\n",
    "n_cls   = len(classes)\n",
    "\n",
    "# 1⃣ 先设 seaborn 风格（若你不用 seaborn，可省略）\n",
    "sns.set_style('white')\n",
    "\n",
    "# 2⃣ 手动把 Times New Roman 的 TTF 注册进 Matplotlib\n",
    "tnr_path = '/usr/share/fonts/truetype/msttcorefonts/Times_New_Roman.ttf'\n",
    "font_manager.fontManager.addfont(tnr_path)\n",
    "\n",
    "# 3⃣ 取出“字体内部名称”，确保写对\n",
    "tnr_name = font_manager.FontProperties(fname=tnr_path).get_name()\n",
    "print(\"内部家族名：\", tnr_name)          # 通常就是 'Times New Roman'\n",
    "\n",
    "# 4⃣ 设置全局 serif 字体优先为 Times New Roman\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams['font.serif']  = [tnr_name]          # 若想回退可追加其它 serif\n",
    "mpl.rcParams.update({\n",
    "    'font.size'      : 14,        # 统一字号\n",
    "    'axes.titlesize' : 16,\n",
    "    'axes.labelsize' : 15,\n",
    "})\n",
    "\n",
    "cmap_blue = sns.color_palette('Blues', as_cmap=True)\n",
    "\n",
    "# =============================================================\n",
    "# 1. 定义网络（与训练保持一致，此处略去重复实现）\n",
    "#    —— 若已 import，可直接跳到 §2 加载权重\n",
    "# =============================================================\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, stride, 1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_c)\n",
    "        self.down  = nn.Identity()\n",
    "        if stride != 1 or in_c != out_c:\n",
    "            self.down = nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_c))\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        return F.relu(y + self.down(x))\n",
    "\n",
    "def make_layer(in_c, out_c, blocks, stride):\n",
    "    layers = [BasicBlock(in_c, out_c, stride)]\n",
    "    layers += [BasicBlock(out_c, out_c) for _ in range(blocks-1)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class SCDResNet64(nn.Module):\n",
    "    def __init__(self, n_cls=8):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 7, 1, 3, bias=False),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, 2, 1))\n",
    "        self.layer1 = make_layer( 64,  64, 3, 1)\n",
    "        self.layer2 = make_layer( 64, 128, 3, 2)\n",
    "        self.layer3 = make_layer(128, 192, 3, 2)\n",
    "        self.layer4 = make_layer(192, 256, 3, 2)\n",
    "        self.head   = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "            nn.Dropout(0.5), nn.Linear(256, n_cls))\n",
    "    def forward(self,x):\n",
    "        x=self.stem(x)\n",
    "        x=self.layer1(x);x=self.layer2(x)\n",
    "        x=self.layer3(x);x=self.layer4(x)\n",
    "        return self.head(x)\n",
    "\n",
    "class SCDResNet64_Student(nn.Module):\n",
    "    def __init__(self, n_cls=8):\n",
    "        super().__init__()\n",
    "        ch=[24,24,48,72,96]\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(1,ch[0],3,1,1,bias=False),\n",
    "            nn.BatchNorm2d(ch[0]), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3,2,1))\n",
    "        self.layer1 = make_layer(ch[0],ch[1],2,1)\n",
    "        self.layer2 = make_layer(ch[1],ch[2],2,2)\n",
    "        self.layer3 = make_layer(ch[2],ch[3],2,2)\n",
    "        self.layer4 = make_layer(ch[3],ch[4],2,2)\n",
    "        self.head   = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "            nn.Dropout(0.4), nn.Linear(ch[4],n_cls))\n",
    "    def forward(self,x):\n",
    "        x=self.stem(x)\n",
    "        x=self.layer1(x);x=self.layer2(x)\n",
    "        x=self.layer3(x);x=self.layer4(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# =============================================================\n",
    "# 2. 加载权重\n",
    "# =============================================================\n",
    "student = SCDResNet64_Student(n_cls).to(device)\n",
    "teacher = SCDResNet64(n_cls).to(device)\n",
    "\n",
    "student.load_state_dict(torch.load('best_student.pth',    map_location=device))\n",
    "teacher.load_state_dict(torch.load('best_scd_resnet.pth', map_location=device))\n",
    "\n",
    "# =============================================================\n",
    "# 3. 预测函数\n",
    "# =============================================================\n",
    "@torch.no_grad()\n",
    "def get_preds(model, loader):\n",
    "    model.eval()\n",
    "    p, y = [], []\n",
    "    for xb, yb in loader:               # test_ld 为你的 DataLoader\n",
    "        p.append(model(xb.to(device)).argmax(1).cpu())\n",
    "        y.append(yb.cpu())\n",
    "    return torch.cat(p).numpy(), torch.cat(y).numpy()\n",
    "\n",
    "y_pred_s, y_true = get_preds(student, test_ld)\n",
    "y_pred_t, _      = get_preds(teacher, test_ld)\n",
    "\n",
    "# =============================================================\n",
    "# 4. 计算混淆矩阵百分比 & 注释文本\n",
    "# =============================================================\n",
    "def cm_percent(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(n_cls))\n",
    "    return cm / cm.sum(1, keepdims=True) * 100, cm\n",
    "\n",
    "cm_s_perc, cm_s = cm_percent(y_true, y_pred_s)\n",
    "cm_t_perc, cm_t = cm_percent(y_true, y_pred_t)\n",
    "\n",
    "# —— 构造双行注释：Student 在上，Teacher 在下 ——  \n",
    "annot = np.empty_like(cm_s_perc, dtype=object)\n",
    "for i in range(n_cls):\n",
    "    for j in range(n_cls):\n",
    "        if cm_s[i,j] == cm_t[i,j] == 0:\n",
    "            annot[i,j] = ''\n",
    "        else:\n",
    "            annot[i,j] = f'{cm_s_perc[i,j]:.1f}%\\n{cm_t_perc[i,j]:.1f}%'\n",
    "\n",
    "# =============================================================\n",
    "# 5. 单图绘制\n",
    "# =============================================================\n",
    "# 自定义配色：明亮蓝色用于热力图\n",
    "cmap_student = LinearSegmentedColormap.from_list(\n",
    "    \"custom_blue\", [\"#e0f3f8\", \"#abd9e9\", \"#74add1\", \"#4575b4\"]\n",
    ")\n",
    "\n",
    "# 创建图形与坐标轴\n",
    "fig, ax = plt.subplots(figsize=(7,6))\n",
    "\n",
    "# 绘制热力图底色（使用学生预测百分比 cm_s_perc）\n",
    "sns.heatmap(cm_s_perc, ax=ax, cmap=cmap_student, cbar=False,\n",
    "            annot=False, linewidths=.4, linecolor='grey',\n",
    "            vmin=0, vmax=100, mask=(cm_s==0), square=True)\n",
    "\n",
    "# 添加对角线橙色方块高亮（可选保留）\n",
    "for i in range(n_cls):\n",
    "    rect = plt.Rectangle((i, i), 1, 1,\n",
    "                         facecolor='#FFC34E',\n",
    "                         edgecolor='none',\n",
    "                         alpha=0.9,\n",
    "                         zorder=3)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "# 写入每个单元格的两行百分比：上为学生（红），下为教师（绿）\n",
    "for i in range(n_cls):\n",
    "    for j in range(n_cls):\n",
    "        if cm_s[i, j] == cm_t[i, j] == 0:\n",
    "            continue\n",
    "        ax.text(j + 0.5, i + 0.35, f'{cm_s_perc[i, j]:.1f}%',\n",
    "                ha='center', va='center', color='#d62728', fontsize=13)\n",
    "        ax.text(j + 0.5, i + 0.65, f'{cm_t_perc[i, j]:.1f}%',\n",
    "                ha='center', va='center', color='#2ca02c', fontsize=13)\n",
    "\n",
    "# 设置坐标轴标签与标题\n",
    "ax.set_xticklabels(classes, rotation=45, ha='right', fontsize=13)\n",
    "ax.set_yticklabels(classes, rotation=0,  fontsize=13)\n",
    "ax.set_xlabel('Predicted Class', fontsize=14)\n",
    "ax.set_ylabel('True Class',      fontsize=14)\n",
    "plt.title('Confusion Matrix (Student ↑  /  Teacher ↓)  on CSPB.ML.2022', fontsize=16)\n",
    "\n",
    "# 设置图例（Legend）\n",
    "student_legend = mlines.Line2D([], [], color='#d62728', marker='s', linestyle='None',\n",
    "                               markersize=5, label='Student')\n",
    "teacher_legend = mlines.Line2D([], [], color='#2ca02c', marker='s', linestyle='None',\n",
    "                               markersize=5, label='Teacher')\n",
    "ax.legend(\n",
    "    handles=[student_legend, teacher_legend],\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1.02, 1),\n",
    "    borderaxespad=0.2,\n",
    "    handletextpad=0.4,\n",
    "    labelspacing=0.2,\n",
    "    handlelength=1.2,\n",
    "    borderpad=0.3,\n",
    "    fontsize=10\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1493018,
     "sourceId": 2468162,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2859031,
     "sourceId": 4930249,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7020230,
     "sourceId": 11237252,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 292809,
     "modelInstanceId": 271817,
     "sourceId": 322552,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
