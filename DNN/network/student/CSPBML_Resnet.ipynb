{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compute α，β\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scanning: 100%|███████████████████████| 20/20 [00:00<00:00, 25.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  mean=4295.14  std=14429.3  α=6.93034e-05  β=-0.297668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scanning: 100%|█████████████████████████| 4/4 [00:00<00:00, 26.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL  ]  mean=4258.27  std=14284.5  α=7.00059e-05  β=-0.298104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scanning: 100%|█████████████████████████| 4/4 [00:00<00:00, 26.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST ]  mean=4280.03  std=14425.2  α=6.93232e-05  β=-0.296705\n",
      "file,   mean,      std,        alpha,       beta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "per-file α/β:   0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_batch_01.pt, 4093.09, 14121.9, 7.08121e-05, -0.289841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "per-file α/β:  11%|█         | 3/28 [00:00<00:00, 29.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_batch_02.pt, 4677.42, 15457.6, 6.46929e-05, -0.302596\n",
      "2022_batch_03.pt, 4209.98, 14110.1, 7.08714e-05, -0.298367\n",
      "2022_batch_04.pt, 4099.58, 13298.3, 7.51978e-05, -0.308279\n",
      "2022_batch_05.pt, 4160.33, 14470.6, 6.91056e-05, -0.287502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "per-file α/β:  25%|██▌       | 7/28 [00:00<00:00, 29.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_batch_06.pt, 4662.43, 15452.6, 6.47141e-05, -0.301725\n",
      "2022_batch_07.pt, 4245.91, 14162.9, 7.06069e-05, -0.29979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "per-file α/β:  36%|███▌      | 10/28 [00:00<00:00, 29.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_batch_08.pt, 4036.47, 13126.1, 7.61838e-05, -0.307513\n",
      "2022_batch_09.pt, 4190.58, 14581.5, 6.85803e-05, -0.287391\n",
      "2022_batch_10.pt, 4632.37, 15371.2, 6.50567e-05, -0.301367\n",
      "2022_batch_11.pt, 4165.24, 13868.5, 7.21059e-05, -0.300338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "per-file α/β:  46%|████▋     | 13/28 [00:00<00:00, 29.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_batch_12.pt, 4072.68, 13271.6, 7.53486e-05, -0.306871\n",
      "2022_batch_13.pt, 4184.38, 14623, 6.83853e-05, -0.286151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "per-file α/β:  57%|█████▋    | 16/28 [00:00<00:00, 29.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_batch_14.pt, 4610.79, 15287.6, 6.54123e-05, -0.301602\n",
      "2022_batch_15.pt, 4151.6, 13854.3, 7.21796e-05, -0.299661\n",
      "2022_batch_16.pt, 4180.08, 13622.2, 7.34093e-05, -0.306857\n",
      "2022_batch_17.pt, 4265.58, 15061.6, 6.63939e-05, -0.283209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "per-file α/β:  68%|██████▊   | 19/28 [00:00<00:00, 29.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_batch_18.pt, 4522.87, 15096.3, 6.62413e-05, -0.299601\n",
      "2022_batch_19.pt, 4192.76, 14169.8, 7.05728e-05, -0.295894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "per-file α/β:  79%|███████▊  | 22/28 [00:00<00:00, 29.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_batch_20.pt, 4244.66, 13765.1, 7.26477e-05, -0.308365\n",
      "2022_batch_21.pt, 4234.7, 15009.7, 6.66235e-05, -0.282131\n",
      "2022_batch_22.pt, 4562.91, 15208.6, 6.57524e-05, -0.300022\n",
      "2022_batch_23.pt, 4215.54, 14297.5, 6.99423e-05, -0.294845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "per-file α/β:  89%|████████▉ | 25/28 [00:00<00:00, 29.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_batch_24.pt, 4231.87, 13633.1, 7.33508e-05, -0.310411\n",
      "2022_batch_25.pt, 4235.05, 14939.4, 6.69372e-05, -0.283483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "per-file α/β: 100%|██████████| 28/28 [00:00<00:00, 29.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_batch_26.pt, 4583.8, 15249.2, 6.55771e-05, -0.300592\n",
      "2022_batch_27.pt, 4221.13, 14273.7, 7.00588e-05, -0.295727\n",
      "2022_batch_28.pt, 4172.21, 13507.8, 7.40314e-05, -0.308875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scanning: 100%|███████████████████████| 20/20 [00:00<00:00, 26.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  mean=4295.14  std=14429.3  α=6.93232e-05  β=-0.296705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch, math, os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPS = 1e-5\n",
    "\n",
    "def calc_mean_std(pt_list):\n",
    "    total_sum, total_sq, total_n = 0.0, 0.0, 0\n",
    "    for f in tqdm(pt_list, desc=\"scanning\", ncols=70):\n",
    "        X, _ = torch.load(f, map_location='cpu', weights_only=True)  # X: (N,128,128)\n",
    "        X = X.float()\n",
    "        total_sum += X.sum().item()\n",
    "        total_sq  += (X ** 2).sum().item()\n",
    "        total_n   += X.numel()\n",
    "    mu  = total_sum / total_n\n",
    "    var = total_sq / total_n - mu ** 2\n",
    "    std = math.sqrt(max(var, 0.0))\n",
    "    return mu, std\n",
    "\n",
    "\n",
    "root = \"../../cached_scd_tim/2022_c_64_512sample_hamming\"   \n",
    "pt_files = sorted([f\"{root}/2022_batch_{i:02d}.pt\" for i in range(1, 29)])\n",
    "\n",
    "train_files = pt_files[:10] + pt_files[-10:]   # 01–10 & 19–28\n",
    "val_files   = pt_files[10:14]                  # 11–14\n",
    "test_files  = pt_files[14:18]                  # 15–18\n",
    "\n",
    "\n",
    "for tag, flist in [(\"TRAIN\", train_files),\n",
    "                   (\"VAL  \", val_files),\n",
    "                   (\"TEST \", test_files)]:\n",
    "    mu, std = calc_mean_std(flist)\n",
    "    alpha   = 1.0 / (std + EPS)\n",
    "    beta    = -mu * alpha\n",
    "    print(f\"[{tag}]  mean={mu:.6g}  std={std:.6g}  \"\n",
    "          f\"α={alpha:.6g}  β={beta:.6g}\")\n",
    "\n",
    "import torch, math, os, csv\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPS = 1e-5\n",
    "\n",
    "def alpha_beta_for_file(pt_path: str):\n",
    "    \n",
    "    X, _ = torch.load(pt_path, map_location=\"cpu\", weights_only=True)  # X:(N,128,128)\n",
    "    X = X.float()\n",
    "    mu  = X.mean().item()\n",
    "    std = X.std(unbiased=False).item()      # population std\n",
    "    alpha = 1.0 / (std + EPS)\n",
    "    beta  = -mu * alpha\n",
    "    return mu, std, alpha, beta\n",
    "\n",
    "root = \"../../cached_scd_tim/2022_c_64_512sample_hamming\"\n",
    "pt_files = sorted([f\"{root}/2022_batch_{i:02d}.pt\" for i in range(1, 29)])\n",
    "\n",
    "print(\"file,   mean,      std,        alpha,       beta\")\n",
    "for f in tqdm(pt_files, desc=\"per-file α/β\"):\n",
    "    mu, std, a, b = alpha_beta_for_file(f)\n",
    "    print(f\"{Path(f).name}, {mu:.6g}, {std:.6g}, {a:.6g}, {b:.6g}\")\n",
    "\n",
    "for tag, flist in [(\"TRAIN\", train_files)]:\n",
    "    mu, std = calc_mean_std(flist)\n",
    "    alpha_train   = 1.0 / (std + EPS)\n",
    "    beta_train    = -mu * alpha\n",
    "    print(f\"[{tag}]  mean={mu:.6g}  std={std:.6g}  \"\n",
    "          f\"α={alpha:.6g}  β={beta:.6g}\")\n",
    "\n",
    "\n",
    "ALPHA = alpha_train      # 1 / (std_train + 1e-5)\n",
    "BETA  = beta_train        # -mean_train * ALPHA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n",
      "Total params: 6.40 M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_597497/2431584195.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  n = torch.load(f, map_location=\"cpu\")[0].shape[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled split → train 80,000 | val 16,000 | test 16,000\n",
      "E01  train 16.50% | loss 2.0668   val 17.67% | loss 2.0540   test 17.44% | loss 2.0572\n",
      "current lr = 0.028000000000000004\n",
      "E02  train 17.36% | loss 2.0351   val 18.02% | loss 2.0132   test 17.88% | loss 2.0139\n",
      "current lr = 0.046000000000000006\n"
     ]
    }
   ],
   "source": [
    "import os, random, numpy as np, math, gc, time, torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import os, random, numpy as np, torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(0);  np.random.seed(0);  random.seed(0)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device =\", device)\n",
    "\n",
    "ALPHA = alpha_train      # 1 / (std_train + 1e-5)\n",
    "BETA  = beta_train        # -mean_train * ALPHA\n",
    "\n",
    "\n",
    "class SCDTensorDataset(Dataset):\n",
    "\n",
    "    def __init__(self, pt_files):\n",
    "        self.meta, self.cache, off = [], {}, 0\n",
    "        for f in pt_files:\n",
    "            n = torch.load(f, map_location=\"cpu\")[0].shape[0]\n",
    "            self.meta.append((f, off, off+n));  off += n\n",
    "        self.N = off\n",
    "\n",
    "    def __len__(self): return self.N\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        for f, beg, end in self.meta:\n",
    "            if beg <= idx < end:\n",
    "                if f not in self.cache:\n",
    "                    self.cache[f] = torch.load(f, map_location=\"cpu\",weights_only=True)\n",
    "                X, y = self.cache[f]\n",
    "                img = X[idx - beg].float()          \n",
    "                img = img * ALPHA + BETA       \n",
    "                img = img.unsqueeze(0)          \n",
    "                return img, y[idx - beg].long().squeeze()\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, stride, 1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_c)\n",
    "\n",
    "        self.down = nn.Identity()\n",
    "        if stride != 1 or in_c != out_c:\n",
    "            self.down = nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_c))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        return F.relu(y + self.down(x))\n",
    "\n",
    "def make_layer(in_c, out_c, blocks, stride):\n",
    "    layers = [BasicBlock(in_c, out_c, stride)]\n",
    "    layers += [BasicBlock(out_c, out_c) for _ in range(blocks-1)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class SCDResNet64(nn.Module):\n",
    "    def __init__(self, n_cls=8):\n",
    "        super().__init__()\n",
    "        # Stem：1×64×64 → 64×32×32\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=1, padding=3, bias=False),  \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)                   \n",
    "        )      \n",
    "        self.layer1 = make_layer( 64,  64, blocks=3, stride=1)  # 64×32×32\n",
    "        self.layer2 = make_layer( 64, 128, blocks=3, stride=2)  # 128×16×16\n",
    "        self.layer3 = make_layer(128, 192, blocks=3, stride=2)  # 192×8×8\n",
    "        self.layer4 = make_layer(192, 256, blocks=3, stride=2)  # 256×4×4\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, n_cls)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x);  x = self.layer2(x)\n",
    "        x = self.layer3(x);  x = self.layer4(x)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "net = SCDResNet64().to(device)\n",
    "print(f\"Total params: {sum(p.numel() for p in net.parameters())/1e6:.2f} M\")\n",
    "\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "pt_files = sorted([f\"../../cached_scd_tim/2022_c_64_512sample_hamming/2022_batch_{i:02d}.pt\" for i in range(1, 29)])  # 01-28\n",
    "train_files = pt_files[:10] + pt_files[-10:]   \n",
    "val_files   = pt_files[10:14]                  \n",
    "test_files  = pt_files[14:18]                  \n",
    "\n",
    "train_ds = SCDTensorDataset(train_files)\n",
    "val_ds   = SCDTensorDataset(val_files)\n",
    "test_ds  = SCDTensorDataset(test_files)\n",
    "\n",
    "print(f\"Shuffled split → train {len(train_ds):,} | val {len(val_ds):,} | test {len(test_ds):,}\")\n",
    "\n",
    "\n",
    "num_workers = 4\n",
    "batch_size  = 32\n",
    "train_ld = DataLoader(train_ds, batch_size, shuffle=True,\n",
    "                      num_workers=num_workers, pin_memory=(device == \"cuda\"))\n",
    "val_ld   = DataLoader(val_ds,   batch_size, shuffle=False,\n",
    "                      num_workers=num_workers, pin_memory=(device == \"cuda\"))\n",
    "test_ld  = DataLoader(test_ds,  batch_size, shuffle=False,\n",
    "                      num_workers=num_workers, pin_memory=(device == \"cuda\"))\n",
    "\n",
    "\n",
    "net = SCDResNet64(n_cls=8).to(device)\n",
    "for m in net.modules():                           \n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        m.momentum = 0.1          \n",
    "\n",
    "optim = torch.optim.SGD(net.parameters(), lr=0.1,\n",
    "                        momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "warm_epochs   = 5          \n",
    "total_epochs  = 500        \n",
    "eta_min       = 1e-8      \n",
    "\n",
    "warmup = torch.optim.lr_scheduler.LinearLR(\n",
    "            optim, start_factor=0.1, end_factor=1.0, total_iters=warm_epochs)\n",
    "\n",
    "cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optim, T_max=total_epochs - warm_epochs, eta_min=eta_min)\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "               optim, schedulers=[warmup, cosine],\n",
    "               milestones=[warm_epochs])        \n",
    "\n",
    "crit = nn.CrossEntropyLoss(label_smoothing=0.05)   \n",
    "def run_epoch(loader, training=True):\n",
    "    net.train(training)\n",
    "    tot=loss_sum=acc=0\n",
    "    for xb, yb in loader:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        with torch.set_grad_enabled(training):\n",
    "            out  = net(xb)\n",
    "            loss = crit(out,yb)\n",
    "            if training:\n",
    "                optim.zero_grad(); loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)  \n",
    "                optim.step()\n",
    "        pred = out.argmax(1)\n",
    "        bsz  = yb.size(0)\n",
    "        tot  += bsz\n",
    "        loss_sum += loss.item()*bsz\n",
    "        acc  += (pred==yb).sum().item()\n",
    "    return loss_sum/tot, acc/tot\n",
    "\n",
    "\n",
    "best = 0\n",
    "epochs=total_epochs\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    tr_l, tr_a = run_epoch(train_ld, True)\n",
    "    vl_l, vl_a = run_epoch(val_ld, False)\n",
    "    ts_l, ts_a = run_epoch(test_ld, False)  \n",
    "\n",
    "    print(f\"E{ep:02d}  train {tr_a*100:5.2f}% | loss {tr_l:.4f}   \"\n",
    "          f\"val {vl_a*100:5.2f}% | loss {vl_l:.4f}   \"\n",
    "          f\"test {ts_a*100:5.2f}% | loss {ts_l:.4f}\")\n",
    "\n",
    "    scheduler.step()       \n",
    "\n",
    "    if vl_a > best:\n",
    "        best = vl_a\n",
    "        torch.save(net.state_dict(), \"best_scd_resnet.pth\")\n",
    "\n",
    "    print(\"current lr =\", optim.param_groups[0]['lr'])\n",
    "\n",
    "print(\"Best val acc:\", best)\n",
    "\n",
    "# evaluate best on test\n",
    "net.load_state_dict(torch.load(\"best_scd_resnet.pth\"))\n",
    "net.eval()\n",
    "test_loss, test_acc = run_epoch(test_ld, training=False)\n",
    "print(f\"Final Test Accuracy: {test_acc*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, random, time, gc, math, numpy as np, torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device =\", device)\n",
    "\n",
    "ALPHA = alpha_train      # 1 / (std_train + 1e-5)\n",
    "BETA  = beta_train        # -mean_train * ALPHA\n",
    "\n",
    "class SCDTensorDataset(Dataset):\n",
    "    def __init__(self, pt_files):\n",
    "        self.meta, self.cache, off = [], {}, 0\n",
    "        for f in pt_files:\n",
    "            n = torch.load(f, map_location=\"cpu\")[0].shape[0]\n",
    "            self.meta.append((f, off, off+n)); off += n\n",
    "        self.N = off\n",
    "\n",
    "    def __len__(self): return self.N\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        for f, beg, end in self.meta:\n",
    "            if beg <= idx < end:\n",
    "                if f not in self.cache:\n",
    "                    self.cache[f] = torch.load(f, map_location=\"cpu\", weights_only=True)\n",
    "                X, y = self.cache[f]\n",
    "                img  = X[idx - beg].float()\n",
    "                \n",
    "                img = img * ALPHA + BETA        \n",
    "                img = img.unsqueeze(0)          \n",
    "                \n",
    "                return img, y[idx - beg].long().squeeze()\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, stride, 1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_c)\n",
    "        self.down  = nn.Identity()\n",
    "        if stride != 1 or in_c != out_c:\n",
    "            self.down = nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_c))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        return F.relu(y + self.down(x))\n",
    "\n",
    "def make_layer(in_c, out_c, blocks, stride):\n",
    "    layers = [BasicBlock(in_c, out_c, stride)]\n",
    "    layers += [BasicBlock(out_c, out_c) for _ in range(blocks-1)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class SCDResNet64(nn.Module):\n",
    "    def __init__(self, n_cls=8):\n",
    "        super().__init__()\n",
    "        self.stem   = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 7, 1, 3, bias=False),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, 2, 1))\n",
    "        self.layer1 = make_layer( 64,  64, 3, 1)\n",
    "        self.layer2 = make_layer( 64, 128, 3, 2)\n",
    "        self.layer3 = make_layer(128, 192, 3, 2)\n",
    "        self.layer4 = make_layer(192, 256, 3, 2)\n",
    "        self.head   = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, n_cls))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x); x = self.layer2(x)\n",
    "        x = self.layer3(x); x = self.layer4(x)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class SCDResNet64_Student(nn.Module):\n",
    "    def __init__(self, n_cls=8):\n",
    "        super().__init__()\n",
    "        ch = [24, 24, 48, 72, 96]          \n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(1, ch[0], 7, 1, 3, bias=False),\n",
    "            nn.BatchNorm2d(ch[0]), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, 2, 1))\n",
    "        self.layer1 = make_layer(ch[0], ch[1], 2, 1)\n",
    "        self.layer2 = make_layer(ch[1], ch[2], 2, 2)\n",
    "        self.layer3 = make_layer(ch[2], ch[3], 2, 2)\n",
    "        self.layer4 = make_layer(ch[3], ch[4], 2, 2)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(ch[4], n_cls))\n",
    "    def forward(self,x):\n",
    "        x=self.stem(x)\n",
    "        x=self.layer1(x); x=self.layer2(x)\n",
    "        x=self.layer3(x); x=self.layer4(x)\n",
    "        return self.head(x)\n",
    "\n",
    "pt_files = sorted([f\"../../cached_scd_tim/2022_c_64_512sample_hamming/2022_batch_{i:02d}.pt\"\n",
    "                   for i in range(1, 29)])       # 01-28\n",
    "train_files = pt_files[:10] + pt_files[-10:]     # 01-10 & 19-28\n",
    "val_files   = pt_files[10:14]                    # 11-14\n",
    "test_files  = pt_files[14:18]                    # 15-18\n",
    "\n",
    "train_ds = SCDTensorDataset(train_files)\n",
    "val_ds   = SCDTensorDataset(val_files)\n",
    "test_ds  = SCDTensorDataset(test_files)\n",
    "print(f\"train {len(train_ds):,} | val {len(val_ds):,} | test {len(test_ds):,}\")\n",
    "\n",
    "loader_cfg = dict(batch_size=32, num_workers=4,\n",
    "                  pin_memory=(device=='cuda'))\n",
    "train_ld = DataLoader(train_ds, shuffle=True,  **loader_cfg)\n",
    "val_ld   = DataLoader(val_ds,   shuffle=False, **loader_cfg)\n",
    "test_ld  = DataLoader(test_ds,  shuffle=False, **loader_cfg)\n",
    "\n",
    "def kd_loss(student_logits, teacher_logits, labels,\n",
    "            T=4.0, alpha=0.7):\n",
    "    kd = F.kl_div(\n",
    "        F.log_softmax(student_logits/T, dim=1),\n",
    "        F.softmax(teacher_logits/T, dim=1),\n",
    "        reduction='batchmean') * (T*T)\n",
    "    ce = F.cross_entropy(student_logits, labels)\n",
    "    return alpha*kd + (1-alpha)*ce\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, loader):\n",
    "    model.eval()\n",
    "    tot = loss_sum = acc = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        out = model(xb)\n",
    "        loss = F.cross_entropy(out, yb)\n",
    "        pred = out.argmax(1)\n",
    "        bsz  = yb.size(0)\n",
    "        tot += bsz\n",
    "        loss_sum += loss.item()*bsz\n",
    "        acc += (pred==yb).sum().item()\n",
    "    return loss_sum/tot, acc/tot\n",
    "\n",
    "teacher = SCDResNet64(); teacher.load_state_dict(\n",
    "    torch.load(\"best_scd_resnet.pth\", map_location=device))\n",
    "teacher.eval().to(device)\n",
    "\n",
    "student = SCDResNet64_Student().to(device)\n",
    "print(f\"Student params: {sum(p.numel() for p in student.parameters())/1e6:.2f} M\")\n",
    "\n",
    "opt = torch.optim.SGD(student.parameters(), lr=0.05,\n",
    "                      momentum=0.9, weight_decay=2e-4)\n",
    "warm_epochs   = 5\n",
    "total_epochs  = 500\n",
    "eta_min       = 1e-10                      \n",
    "warmup = torch.optim.lr_scheduler.LinearLR(\n",
    "            opt, start_factor=0.1, end_factor=1.0, total_iters=warm_epochs)\n",
    "cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            opt, T_max=total_epochs - warm_epochs, eta_min=eta_min)\n",
    "\n",
    "sched = torch.optim.lr_scheduler.SequentialLR(\n",
    "            opt, schedulers=[warmup, cosine], milestones=[warm_epochs])\n",
    "best_val = 0.0\n",
    "for epoch in range(1, total_epochs + 1):\n",
    "    student.train()\n",
    "    tr_tot = tr_loss_sum = tr_correct = 0\n",
    "    for xb, yb in train_ld:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        with torch.no_grad():\n",
    "            t_out = teacher(xb)\n",
    "        s_out  = student(xb)\n",
    "        loss   = kd_loss(s_out, t_out, yb, T=4, alpha=0.7)\n",
    "\n",
    "        opt.zero_grad(); loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        bsz = yb.size(0)\n",
    "        tr_tot       += bsz\n",
    "        tr_loss_sum  += loss.item() * bsz\n",
    "        tr_correct   += (s_out.argmax(1) == yb).sum().item()\n",
    "\n",
    "    sched.step()           \n",
    "\n",
    "    train_loss = tr_loss_sum / tr_tot\n",
    "    train_acc  = tr_correct  / tr_tot\n",
    "\n",
    "    val_loss,  val_acc  = eval_model(student, val_ld)\n",
    "    test_loss, test_acc = eval_model(student, test_ld)\n",
    "\n",
    "    print(f\"E{epoch:03d} | \"\n",
    "          f\"train {train_acc*100:5.2f}% / {train_loss:.4f}   \"\n",
    "          f\"val {val_acc*100:5.2f}% / {val_loss:.4f}   \"\n",
    "          f\"test {test_acc*100:5.2f}% / {test_loss:.4f}   \"\n",
    "          f\"lr {sched.get_last_lr()[0]:.3e}\")\n",
    "\n",
    "    if val_acc > best_val:\n",
    "        best_val = val_acc\n",
    "        torch.save(student.state_dict(), \"best_student.pth\")\n",
    "\n",
    "print(\"KD finished. Best val acc:\", best_val*100)\n",
    "\n",
    "\n",
    "student.load_state_dict(torch.load(\"best_student.pth\"))\n",
    "_, test_acc = eval_model(student, test_ld)\n",
    "print(f\"Student Test Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "xb, _ = next(iter(test_ld))\n",
    "img = xb[0:1].to(device)\n",
    "torch.cuda.synchronize() if device=='cuda' else None\n",
    "t0 = time.time()\n",
    "_ = student(img)\n",
    "torch.cuda.synchronize() if device=='cuda' else None\n",
    "t1 = time.time()\n",
    "print(f\"⏱️ Student inference (1 img): {(t1-t0)*1000:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, seaborn as sns, matplotlib.pyplot as plt, matplotlib as mpl\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.nn as nn, torch.nn.functional as F\n",
    "import matplotlib as mpl\n",
    "from matplotlib import font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.lines as mlines\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "# =============================================================\n",
    "# 0. 参数 & 主题\n",
    "# =============================================================\n",
    "device  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "classes = ['BPSK','QPSK','8PSK','DQPSK','MSK','16QAM','64QAM','256QAM']\n",
    "n_cls   = len(classes)\n",
    "\n",
    "# 1⃣ 先设 seaborn 风格（若你不用 seaborn，可省略）\n",
    "sns.set_style('white')\n",
    "\n",
    "# 2⃣ 手动把 Times New Roman 的 TTF 注册进 Matplotlib\n",
    "tnr_path = '/usr/share/fonts/truetype/msttcorefonts/Times_New_Roman.ttf'\n",
    "font_manager.fontManager.addfont(tnr_path)\n",
    "\n",
    "# 3⃣ 取出“字体内部名称”，确保写对\n",
    "tnr_name = font_manager.FontProperties(fname=tnr_path).get_name()\n",
    "print(\"内部家族名：\", tnr_name)          # 通常就是 'Times New Roman'\n",
    "\n",
    "# 4⃣ 设置全局 serif 字体优先为 Times New Roman\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams['font.serif']  = [tnr_name]          # 若想回退可追加其它 serif\n",
    "mpl.rcParams.update({\n",
    "    'font.size'      : 14,        # 统一字号\n",
    "    'axes.titlesize' : 16,\n",
    "    'axes.labelsize' : 15,\n",
    "})\n",
    "\n",
    "cmap_blue = sns.color_palette('Blues', as_cmap=True)\n",
    "\n",
    "# =============================================================\n",
    "# 1. 定义网络（与训练保持一致，此处略去重复实现）\n",
    "#    —— 若已 import，可直接跳到 §2 加载权重\n",
    "# =============================================================\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, stride, 1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_c)\n",
    "        self.down  = nn.Identity()\n",
    "        if stride != 1 or in_c != out_c:\n",
    "            self.down = nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_c))\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        return F.relu(y + self.down(x))\n",
    "\n",
    "def make_layer(in_c, out_c, blocks, stride):\n",
    "    layers = [BasicBlock(in_c, out_c, stride)]\n",
    "    layers += [BasicBlock(out_c, out_c) for _ in range(blocks-1)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class SCDResNet64(nn.Module):\n",
    "    def __init__(self, n_cls=8):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 7, 1, 3, bias=False),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, 2, 1))\n",
    "        self.layer1 = make_layer( 64,  64, 3, 1)\n",
    "        self.layer2 = make_layer( 64, 128, 3, 2)\n",
    "        self.layer3 = make_layer(128, 192, 3, 2)\n",
    "        self.layer4 = make_layer(192, 256, 3, 2)\n",
    "        self.head   = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "            nn.Dropout(0.5), nn.Linear(256, n_cls))\n",
    "    def forward(self,x):\n",
    "        x=self.stem(x)\n",
    "        x=self.layer1(x);x=self.layer2(x)\n",
    "        x=self.layer3(x);x=self.layer4(x)\n",
    "        return self.head(x)\n",
    "\n",
    "class SCDResNet64_Student(nn.Module):\n",
    "    def __init__(self, n_cls=8):\n",
    "        super().__init__()\n",
    "        ch=[24,24,48,72,96]\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(1,ch[0],3,1,1,bias=False),\n",
    "            nn.BatchNorm2d(ch[0]), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3,2,1))\n",
    "        self.layer1 = make_layer(ch[0],ch[1],2,1)\n",
    "        self.layer2 = make_layer(ch[1],ch[2],2,2)\n",
    "        self.layer3 = make_layer(ch[2],ch[3],2,2)\n",
    "        self.layer4 = make_layer(ch[3],ch[4],2,2)\n",
    "        self.head   = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "            nn.Dropout(0.4), nn.Linear(ch[4],n_cls))\n",
    "    def forward(self,x):\n",
    "        x=self.stem(x)\n",
    "        x=self.layer1(x);x=self.layer2(x)\n",
    "        x=self.layer3(x);x=self.layer4(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# =============================================================\n",
    "# 2. 加载权重\n",
    "# =============================================================\n",
    "student = SCDResNet64_Student(n_cls).to(device)\n",
    "teacher = SCDResNet64(n_cls).to(device)\n",
    "\n",
    "student.load_state_dict(torch.load('best_student.pth',    map_location=device))\n",
    "teacher.load_state_dict(torch.load('best_scd_resnet.pth', map_location=device))\n",
    "\n",
    "# =============================================================\n",
    "# 3. 预测函数\n",
    "# =============================================================\n",
    "@torch.no_grad()\n",
    "def get_preds(model, loader):\n",
    "    model.eval()\n",
    "    p, y = [], []\n",
    "    for xb, yb in loader:               # test_ld 为你的 DataLoader\n",
    "        p.append(model(xb.to(device)).argmax(1).cpu())\n",
    "        y.append(yb.cpu())\n",
    "    return torch.cat(p).numpy(), torch.cat(y).numpy()\n",
    "\n",
    "y_pred_s, y_true = get_preds(student, test_ld)\n",
    "y_pred_t, _      = get_preds(teacher, test_ld)\n",
    "\n",
    "# =============================================================\n",
    "# 4. 计算混淆矩阵百分比 & 注释文本\n",
    "# =============================================================\n",
    "def cm_percent(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(n_cls))\n",
    "    return cm / cm.sum(1, keepdims=True) * 100, cm\n",
    "\n",
    "cm_s_perc, cm_s = cm_percent(y_true, y_pred_s)\n",
    "cm_t_perc, cm_t = cm_percent(y_true, y_pred_t)\n",
    "\n",
    "# —— 构造双行注释：Student 在上，Teacher 在下 ——  \n",
    "annot = np.empty_like(cm_s_perc, dtype=object)\n",
    "for i in range(n_cls):\n",
    "    for j in range(n_cls):\n",
    "        if cm_s[i,j] == cm_t[i,j] == 0:\n",
    "            annot[i,j] = ''\n",
    "        else:\n",
    "            annot[i,j] = f'{cm_s_perc[i,j]:.1f}%\\n{cm_t_perc[i,j]:.1f}%'\n",
    "\n",
    "# =============================================================\n",
    "# 5. 单图绘制\n",
    "# =============================================================\n",
    "# 自定义配色：明亮蓝色用于热力图\n",
    "cmap_student = LinearSegmentedColormap.from_list(\n",
    "    \"custom_blue\", [\"#e0f3f8\", \"#abd9e9\", \"#74add1\", \"#4575b4\"]\n",
    ")\n",
    "\n",
    "# 创建图形与坐标轴\n",
    "fig, ax = plt.subplots(figsize=(7,6))\n",
    "\n",
    "# 绘制热力图底色（使用学生预测百分比 cm_s_perc）\n",
    "sns.heatmap(cm_s_perc, ax=ax, cmap=cmap_student, cbar=False,\n",
    "            annot=False, linewidths=.4, linecolor='grey',\n",
    "            vmin=0, vmax=100, mask=(cm_s==0), square=True)\n",
    "\n",
    "# 添加对角线橙色方块高亮（可选保留）\n",
    "for i in range(n_cls):\n",
    "    rect = plt.Rectangle((i, i), 1, 1,\n",
    "                         facecolor='#FFC34E',\n",
    "                         edgecolor='none',\n",
    "                         alpha=0.9,\n",
    "                         zorder=3)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "# 写入每个单元格的两行百分比：上为学生（红），下为教师（绿）\n",
    "for i in range(n_cls):\n",
    "    for j in range(n_cls):\n",
    "        if cm_s[i, j] == cm_t[i, j] == 0:\n",
    "            continue\n",
    "        ax.text(j + 0.5, i + 0.35, f'{cm_s_perc[i, j]:.1f}%',\n",
    "                ha='center', va='center', color='#d62728', fontsize=13)\n",
    "        ax.text(j + 0.5, i + 0.65, f'{cm_t_perc[i, j]:.1f}%',\n",
    "                ha='center', va='center', color='#2ca02c', fontsize=13)\n",
    "\n",
    "# 设置坐标轴标签与标题\n",
    "ax.set_xticklabels(classes, rotation=45, ha='right', fontsize=13)\n",
    "ax.set_yticklabels(classes, rotation=0,  fontsize=13)\n",
    "ax.set_xlabel('Predicted Class', fontsize=14)\n",
    "ax.set_ylabel('True Class',      fontsize=14)\n",
    "plt.title('Confusion Matrix (Student ↑  /  Teacher ↓)  on CSPB.ML.2022', fontsize=16)\n",
    "\n",
    "# 设置图例（Legend）\n",
    "student_legend = mlines.Line2D([], [], color='#d62728', marker='s', linestyle='None',\n",
    "                               markersize=5, label='Student')\n",
    "teacher_legend = mlines.Line2D([], [], color='#2ca02c', marker='s', linestyle='None',\n",
    "                               markersize=5, label='Teacher')\n",
    "ax.legend(\n",
    "    handles=[student_legend, teacher_legend],\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1.02, 1),\n",
    "    borderaxespad=0.2,\n",
    "    handletextpad=0.4,\n",
    "    labelspacing=0.2,\n",
    "    handlelength=1.2,\n",
    "    borderpad=0.3,\n",
    "    fontsize=10\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1493018,
     "sourceId": 2468162,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2859031,
     "sourceId": 4930249,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7020230,
     "sourceId": 11237252,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 292809,
     "modelInstanceId": 271817,
     "sourceId": 322552,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
